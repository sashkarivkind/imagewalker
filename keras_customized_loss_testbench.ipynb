{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5, 38)\n",
      "debu return sequence True\n",
      "WARNING:tensorflow:From /home/bnapp/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "debug __________________time_distributed_accuracy____________________  5\n",
      "debug_shapes: (?, 5, 10)\n",
      "debug_shapes: (?, 5)\n",
      "debug _________________time_distributed_accuracy_last_step_____________________  5\n",
      "debug_shapes: (?, 5, 10)\n",
      "debug_shapes: (?, 5)\n",
      "debug ___________________time_distributed_xentropy_loss___________________  5\n",
      "WARNING:tensorflow:From /home/bnapp/arivkindNet/rlnet1/keras_networks.py:23: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Are we random? 8\n",
      "debu0 [[36 36]\n",
      " [31 40]\n",
      " [34 39]\n",
      " [32 36]\n",
      " [36 32]]\n",
      "Fit model on training data\n",
      "WARNING:tensorflow:From /home/bnapp/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "55000/55000 [==============================] - 7s 126us/sample - loss: 2.0362 - time_distributed_accuracy_: 0.2792 - time_distributed_accuracy_last_step_: 0.3006 - val_loss: 1.5304 - val_time_distributed_accuracy_: 0.4903 - val_time_distributed_accuracy_last_step_: 0.5632\n",
      "Epoch 2/3\n",
      "34155/55000 [=================>............] - ETA: 2s - loss: 1.4946 - time_distributed_accuracy_: 0.4939 - time_distributed_accuracy_last_step_: 0.5802"
     ]
    }
   ],
   "source": [
    "from misc import HP\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "import SYCLOP_env as syc\n",
    "from misc import *\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras_networks import rnn_model_102e\n",
    "from curriculum_utils import create_mnist_dataset, bad_res102\n",
    "\n",
    "# ArgumentParser()\n",
    "def split_dataset_xy(dataset):\n",
    "    dataset_x1 = [uu[0] for uu in dataset]\n",
    "    dataset_x2 = [uu[1] for uu in dataset]\n",
    "    dataset_y = [uu[-1] for uu in dataset]\n",
    "    return (np.array(dataset_x1)[...,np.newaxis],np.array(dataset_x2)[:,:n_timesteps,:]), np.repeat(np.reshape(dataset_y,[-1,1]),hp.steps_per_episode,axis=1)\n",
    "\n",
    "#parse hyperparameters\n",
    "\n",
    "lsbjob = os.getenv('LSB_JOBID')\n",
    "lsbjob = '' if lsbjob is None else lsbjob\n",
    "\n",
    "hp = HP()\n",
    "hp.save_path = 'saved_runs'\n",
    "hp.description=''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--tau_int', default=4., type=float, help='Integration timescale for adaaptation')\n",
    "parser.add_argument('--resize', default=1.0, type=float, help='resize of images')\n",
    "parser.add_argument('--run_name_suffix', default='', type=str, help='suffix for runname')\n",
    "parser.add_argument('--eval_dir', default=None, type=str, help='eval dir')\n",
    "\n",
    "parser.add_argument('--dqn_initial_network', default=None, type=str, help='dqn_initial_network')\n",
    "parser.add_argument('--decoder_initial_network', default=None, type=str, help='decoder_initial_network')\n",
    "parser.add_argument('--decoder_learning_rate',  default=1e-3, type=float, help='decoder learning rate')\n",
    "parser.add_argument('--decoder_dropout',  default=0.0, type=float, help='decoder dropout')\n",
    "parser.add_argument('--decoder_rnn_type',  default='gru', type=str, help='gru or rnn')\n",
    "parser.add_argument('--decoder_rnn_units',  default=100, type=int, help='decoder rnn units')\n",
    "parser.add_argument('--decoder_rnn_layers',  default=1, type=int, help='decoder rnn units')\n",
    "\n",
    "\n",
    "parser.add_argument('--decoder_ignore_position', dest='decoder_ignore_position', action='store_true')\n",
    "parser.add_argument('--no-decoder_ignore_position', dest='decoder_ignore_position', action='store_false')\n",
    "\n",
    "parser.add_argument('--syclop_learning_rate',  default=2.5e-3, type=float, help='syclop (RL) learning rate')\n",
    "\n",
    "parser.add_argument('--color', default='grayscale', type=str, help='grayscale/rgb')\n",
    "parser.add_argument('--speed_reward',  default=0.0, type=float, help='speed reward, typically negative')\n",
    "parser.add_argument('--intensity_reward',  default=0.0, type=float, help='speed penalty reward')\n",
    "parser.add_argument('--loss_reward',  default=-1.0, type=float, help='reward for loss, typically negative')\n",
    "parser.add_argument('--resolution',  default=6, type=int, help='resolution')\n",
    "parser.add_argument('--max_eval_episodes',  default=10000, type=int, help='episodes for evaluation mode')\n",
    "parser.add_argument('--steps_per_episode',  default=5, type=int, help='time steps in each episode in ')\n",
    "parser.add_argument('--fit_verbose',  default=1, type=int, help='verbose level for model.fit                        ')\n",
    "parser.add_argument('--steps_between_learnings',  default=100, type=int, help='steps_between_learnings')\n",
    "parser.add_argument('--num_epochs',  default=100, type=int, help='steps_between_learnings')\n",
    "\n",
    "parser.add_argument('--alpha_increment',  default=0.01, type=float, help='reward for loss, typically negative')\n",
    "\n",
    "\n",
    "parser.add_argument('--beta_t1',  default=400000, type=int, help='time rising bete')\n",
    "parser.add_argument('--beta_t2',  default=700000, type=int, help='end rising beta')\n",
    "parser.add_argument('--beta_b1',  default=0.1, type=float, help='beta initial value')\n",
    "parser.add_argument('--beta_b2',  default=1.0, type=float, help='beta final value')\n",
    "\n",
    "parser.add_argument('--curriculum_enable', dest='curriculum_enable', action='store_true')\n",
    "parser.add_argument('--no-curriculum_enable', dest='curriculum_enable', action='store_false')\n",
    "\n",
    "parser.add_argument('--conv_fe', dest='conv_fe', action='store_true')\n",
    "parser.add_argument('--no-conv_fe', dest='conv_fe', action='store_false')\n",
    "\n",
    "parser.add_argument('--acceleration_mode', dest='acceleration_mode', action='store_true')\n",
    "parser.add_argument('--no-acceleration_mode', dest='acceleration_mode', action='store_false')\n",
    "\n",
    "\n",
    "parser.set_defaults(eval_mode=False, decode_from_dvs=False,test_mode=False,rising_beta_schedule=True,decoder_ignore_position=False, curriculum_enable=True, conv_fe=False,\n",
    "                    acceleration_mode=False)\n",
    "\n",
    "config = parser.parse_args('')\n",
    "config = vars(config)\n",
    "hp.upadte_from_dict(config)\n",
    "hp.this_run_name = sys.argv[0] + '_noname_' + hp.run_name_suffix + '_' + lsbjob + '_' + str(int(time.time()))\n",
    "\n",
    "#define model\n",
    "n_timesteps = hp.steps_per_episode\n",
    "\n",
    "##\n",
    "# deploy_logs()\n",
    "##\n",
    "decoder = rnn_model_102e(n_timesteps=hp.steps_per_episode, lr=hp.decoder_learning_rate,ignore_input_B=hp.decoder_ignore_position,dropout=hp.decoder_dropout,rnn_type=hp.decoder_rnn_type,\n",
    "                                input_size=(hp.resolution,hp.resolution, 1),rnn_layers=hp.decoder_rnn_layers,conv_fe=hp.conv_fe)\n",
    "# decoder = keras.models.load_model(hp.decoder_initial_network)\n",
    "#define dataset\n",
    "(images, labels), (images_test, labels_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "\n",
    "#fit one epoch in a  time\n",
    "# scheduler = Scheduler(hp.lambda_schedule)\n",
    "# for epoch in range(hp.num_epochs):\n",
    "#     lambda_epoch = scheduler.step(epoch)\n",
    "alpha=0\n",
    "for epoch in range(hp.num_epochs):\n",
    "    if hp.curriculum_enable:\n",
    "        if epoch == 0:\n",
    "            train_dataset, test_dataset = create_mnist_dataset(images, labels, 6, bad_res_func=bad_res102, return_datasets=True, q_0=0, alpha=1.0,random_trajectories=False,acceleration_mode=hp.acceleration_mode)\n",
    "            train_dataset_x, train_dataset_y = split_dataset_xy(train_dataset)\n",
    "            test_dataset_x, test_dataset_y = split_dataset_xy(test_dataset)\n",
    "            q_0=train_dataset_x[1][0]\n",
    "            print('debu0',q_0)\n",
    "        else:\n",
    "            alpha += hp.alpha_increment\n",
    "            alpha = np.minimum(alpha,1.0)\n",
    "\n",
    "            train_dataset, test_dataset = create_mnist_dataset(images, labels, 6, bad_res_func=bad_res102, return_datasets=True, q_0=q_0, alpha=alpha,random_trajectories=True,acceleration_mode=hp.acceleration_mode)\n",
    "            train_dataset_x, train_dataset_y = split_dataset_xy(train_dataset)\n",
    "            test_dataset_x, test_dataset_y = split_dataset_xy(test_dataset)\n",
    "            q_prime=train_dataset_x[1][0]\n",
    "            print('epoch',epoch,' alpha',alpha,'first q --', q_prime.reshape([-1]))\n",
    "    else:\n",
    "        train_dataset, test_dataset = create_mnist_dataset(images, labels, 6, bad_res_func=bad_res102,\n",
    "                                                           return_datasets=True, q_0=0, alpha=1.0,\n",
    "                                                           random_trajectories=True,acceleration_mode=hp.acceleration_mode)\n",
    "        train_dataset_x, train_dataset_y = split_dataset_xy(train_dataset)\n",
    "        test_dataset_x, test_dataset_y = split_dataset_xy(test_dataset)\n",
    "        q_prime = train_dataset_x[1][0]\n",
    "        print('epoch', epoch, '  CONTROL!!!',' first q --', q_prime.reshape([-1]))\n",
    "        print('epoch', epoch, '  CONTROL!!!',' first im --', train_dataset_x[0][0])\n",
    "\n",
    "    print(\"Fit model on training data\")\n",
    "    history = decoder.fit(\n",
    "        train_dataset_x,\n",
    "        train_dataset_y,\n",
    "        batch_size=135,\n",
    "        epochs=3,\n",
    "        verbose=hp.fit_verbose,\n",
    "        # We pass some validation for\n",
    "        # monitoring validation loss and metrics\n",
    "        # at the end of each epoch\n",
    "        validation_data=(test_dataset_x, test_dataset_y)  # (validation_images, validation_labels)\n",
    "        )\n",
    "\n",
    "#save the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argparse.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = parser.parse_args('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            train_dataset, test_dataset = create_mnist_dataset(images, labels, 6, bad_res_func=bad_res102, return_datasets=True, q_0=0, alpha=1.0,random_trajectories=False,acceleration_mode=hp.acceleration_mode)\n",
    "            train_dataset_x, train_dataset_y = split_dataset_xy(train_dataset)\n",
    "            test_dataset_x, test_dataset_y = split_dataset_xy(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.shape(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered=True\n",
    "imm=np.reshape(images,[-1,28**2])\n",
    "imm=imm.astype(np.float)\n",
    "if centered:\n",
    "    imm=imm - imm.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=imm.transpose()@imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee,vv=np.linalg.eig(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(ee2)/ee2.sum())\n",
    "plt.plot(np.cumsum(ee)/ee.sum())\n",
    "plt.grid()\n",
    "plt.legend(['not centered','centered'])\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Cumulative_explained_variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in decoder.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = lambda k: k+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nana(x):\n",
    "    return x+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
