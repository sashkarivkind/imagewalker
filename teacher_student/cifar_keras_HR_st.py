#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

Comparing student teacher methods -  using Knowledge Disillation (KD) and Feature Learning (FL)

Using Keras and Cifar 10 with syclop. 

A pervious comperision was made to distill knowledge with pytorch on cifar HR
images - there we found that the KD outperformed FL.
Another comparision was made with MNIST and pytorch with syclop on LR images - 
there we found that FL outperformed KD - the opposite!

lr = 5e-4 alpha = 0.9 beta = 0.8 temp = 10 - out.
lr = 1e-4 alpha = 0.9 beta = 0.9 temp = 10 - out.299259 pre-train start = 1 (mistake) Batch norm
KD test accuracy =  [0.38359999656677246, 0.4424000084400177, 0.503000020980835, 0.5307999849319458, 0.550599992275238, 0.5595999956130981, 0.5555999875068665, 0.5889000296592712, 0.5857999920845032, 0.6021999716758728, 0.6140999794006348, 0.6348000168800354, 0.6157000064849854, 0.637499988079071, 0.6445000171661377, 0.6340000033378601, 0.6309999823570251, 0.6347000002861023, 0.6434999704360962, 0.6614999771118164, 0.636900007724762, 0.652400016784668, 0.6646999716758728, 0.6478000283241272, 0.6377000212669373, 0.6610999703407288, 0.6563000082969666, 0.6481000185012817, 0.6621000170707703, 0.6675000190734863, 0.6732000112533569, 0.6582000255584717, 0.6676999926567078, 0.6585000157356262, 0.6664000153541565, 0.6693999767303467, 0.666100025177002, 0.6586999893188477, 0.6751999855041504, 0.675000011920929, 0.6607999801635742, 0.6448000073432922, 0.6689000129699707, 0.6642000079154968, 0.6776999831199646, 0.6704000234603882, 0.6786999702453613, 0.6700999736785889, 0.6769000291824341, 0.67330002784729, 0.6672999858856201, 0.6717000007629395, 0.6694999933242798, 0.6743999719619751, 0.6622999906539917, 0.6733999848365784, 0.6693000197410583, 0.6754000186920166, 0.6449000239372253, 0.6833000183105469, 0.6685000061988831, 0.6626999974250793, 0.6710000038146973, 0.6789000034332275, 0.6622999906539917, 0.6714000105857849, 0.6769000291824341, 0.6797000169754028, 0.6776000261306763, 0.65829998254776, 0.6639999747276306, 0.6747999787330627, 0.6567999720573425, 0.6682000160217285, 0.6603000164031982, 0.6699000000953674, 0.6675000190734863, 0.6679999828338623, 0.667900025844574, 0.6736999750137329, 0.66839998960495, 0.6743999719619751, 0.6640999913215637, 0.6693000197410583, 0.6776000261306763, 0.6714000105857849, 0.669700026512146, 0.6744999885559082, 0.666100025177002, 0.6590999960899353, 0.6784999966621399, 0.6643999814987183, 0.6771000027656555, 0.6764000058174133, 0.6697999835014343, 0.6700999736785889, 0.661899983882904, 0.6674000024795532, 0.6657999753952026, 0.670199990272522, 0.6682000160217285, 0.6704000234603882, 0.6603999733924866, 0.6718000173568726, 0.66839998960495, 0.6516000032424927, 0.6651999950408936, 0.6678000092506409, 0.6700000166893005, 0.6686000227928162, 0.6729000210762024, 0.6747000217437744, 0.6801000237464905, 0.6520000100135803, 0.6672999858856201, 0.6718000173568726, 0.6480000019073486, 0.6747999787330627, 0.6696000099182129, 0.6735000014305115, 0.6620000004768372, 0.6690000295639038, 0.6700999736785889, 0.6621999740600586, 0.6744999885559082, 0.6614000201225281, 0.6581000089645386, 0.6547999978065491, 0.6639999747276306, 0.6601999998092651, 0.6689000129699707, 0.676800012588501, 0.6739000082015991, 0.6725999712944031, 0.6735000014305115, 0.666100025177002, 0.6639000177383423, 0.6699000000953674, 0.670799970626831, 0.6717000007629395, 0.6495000123977661, 0.6711000204086304, 0.6593999862670898, 0.6692000031471252, 0.66839998960495, 0.67540001869201
KD2 test accuracy =  [0.3903999924659729, 0.4415999948978424, 0.5016999840736389, 0.5378999710083008, 0.5444999933242798, 0.5802000164985657, 0.5964000225067139, 0.5879999995231628, 0.6068999767303467, 0.6144999861717224, 0.6115000247955322, 0.6259999871253967, 0.6320000290870667, 0.6388999819755554, 0.6535999774932861, 0.6373000144958496, 0.633899986743927, 0.6557999849319458, 0.6507999897003174, 0.652899980545044, 0.654699981212616, 0.6542999744415283, 0.6624000072479248, 0.6575000286102295, 0.656000018119812, 0.6657000184059143, 0.6589000225067139, 0.6714000105857849, 0.6679999828338623, 0.676800012588501, 0.6639999747276306, 0.6754999756813049, 0.6686000227928162, 0.6740999817848206, 0.6814000010490417, 0.6712999939918518, 0.6794999837875366, 0.6819999814033508, 0.6819000244140625, 0.6662999987602234, 0.6438999772071838, 0.6700999736785889, 0.6845999956130981, 0.6812000274658203, 0.6664999723434448, 0.6887000203132629, 0.6789000034332275, 0.6940000057220459, 0.6840000152587891, 0.6780999898910522, 0.6830999851226807, 0.692300021648407, 0.6761999726295471, 0.6876000165939331, 0.6832000017166138, 0.6776999831199646, 0.6888999938964844, 0.685699999332428, 0.6807000041007996, 0.6833999752998352, 0.6883000135421753, 0.6848000288009644, 0.6779000163078308, 0.6876000165939331, 0.6818000078201294, 0.6787999868392944, 0.6887999773025513, 0.6858000159263611, 0.6906999945640564, 0.675599992275238, 0.6935999989509583, 0.6879000067710876, 0.6843000054359436, 0.6872000098228455, 0.6934999823570251, 0.6916999816894531, 0.6780999898910522, 0.6938999891281128, 0.6779999732971191, 0.682699978351593, 0.6955000162124634, 0.6929000020027161, 0.6830999851226807, 0.6948999762535095, 0.683899998664856, 0.6887000203132629, 0.6854000091552734, 0.6898999810218811, 0.6931999921798706, 0.6832000017166138, 0.6901000142097473, 0.6876000165939331, 0.6930000185966492, 0.6881999969482422, 0.6855999827384949, 0.6865000128746033, 0.6941999793052673, 0.6836000084877014, 0.6945000290870667, 0.6868000030517578, 0.6868000030517578, 0.695900022983551, 0.6818000078201294, 0.6868000030517578, 0.694100022315979, 0.6797999739646912, 0.6934999823570251, 0.6977999806404114, 0.7016000151634216, 0.6869999766349792, 0.6933000087738037, 0.6988000273704529, 0.6883999705314636, 0.7002999782562256, 0.697700023651123, 0.6995999813079834, 0.6894000172615051, 0.6942999958992004, 0.6840999722480774, 0.6898999810218811, 0.6938999891281128, 0.6958000063896179, 0.684499979019165, 0.6797999739646912, 0.6829000115394592, 0.6998000144958496, 0.6798999905586243, 0.7002999782562256, 0.689300000667572, 0.6965000033378601, 0.6866000294685364, 0.6887000203132629, 0.6952999830245972, 0.6920999884605408, 0.6899999976158142, 0.6991999745368958, 0.6935999989509583, 0.7008000016212463, 0.7019000053405762, 0.7009000182151794, 0.6869999766349792, 0.7017999887466431, 0.6956999897956848, 0.6987000107765198, 0.6962000131607056, 0.6916000247001648, 0.684499979019165, 0.6852999925613403, 0.6710000038146973, 0.7045999765396118]
Baseline test accuracy =  [0.3813999891281128, 0.44440001249313354, 0.4896000027656555, 0.5184000134468079, 0.5554999709129333, 0.5534999966621399, 0.5835999846458435, 0.5953999757766724, 0.5985999703407288, 0.6121000051498413, 0.6229000091552734, 0.6019999980926514, 0.6290000081062317, 0.6295999884605408, 0.6342999935150146, 0.6355000138282776, 0.6305000185966492, 0.6514000296592712, 0.6403999924659729, 0.656000018119812, 0.6549000144004822, 0.6552000045776367, 0.6575000286102295, 0.6589999794960022, 0.6590999960899353, 0.6550999879837036, 0.6682999730110168, 0.656000018119812, 0.6532999873161316, 0.6682999730110168, 0.6736999750137329, 0.6607999801635742, 0.6642000079154968, 0.663100004196167, 0.6704999804496765, 0.6703000068664551, 0.6751000285148621, 0.6794999837875366, 0.6730999946594238, 0.6761999726295471, 0.6754999756813049, 0.6761999726295471, 0.6776000261306763, 0.6765000224113464, 0.6700000166893005, 0.6718000173568726, 0.6625999808311462, 0.6697999835014343, 0.6581000089645386, 0.6692000031471252, 0.6740000247955322, 0.6804999709129333, 0.669700026512146, 0.6816999912261963, 0.6776000261306763, 0.6765000224113464, 0.6740000247955322, 0.6772000193595886, 0.6775000095367432, 0.676800012588501, 0.682699978351593, 0.6830999851226807, 0.6765000224113464, 0.6805999875068665, 0.6712999939918518, 0.6746000051498413, 0.6832000017166138, 0.677299976348877, 0.6708999872207642, 0.6765000224113464, 0.6829000115394592, 0.6832000017166138, 0.6796000003814697, 0.6549000144004822, 0.6696000099182129, 0.6747999787330627, 0.6759999990463257, 0.6593999862670898, 0.6732000112533569, 0.6751999855041504, 0.6714000105857849, 0.6740999817848206, 0.678600013256073, 0.6714000105857849, 0.6747999787330627, 0.6751000285148621, 0.6801999807357788, 0.6818000078201294, 0.6739000082015991, 0.676800012588501, 0.6725999712944031, 0.6722000241279602, 0.6804999709129333, 0.6827999949455261, 0.6820999979972839, 0.6711999773979187, 0.6661999821662903, 0.6628999710083008, 0.666700005531311, 0.6753000020980835, 0.6775000095367432, 0.6694999933242798, 0.6730999946594238, 0.680899977684021, 0.6694999933242798, 0.6639999747276306, 0.6636000275611877, 0.6710000038146973, 0.6740000247955322, 0.6685000061988831, 0.6722999811172485, 0.6696000099182129, 0.6696000099182129, 0.6722999811172485, 0.6747000217437744, 0.6722000241279602, 0.6739000082015991, 0.667900025844574, 0.6593999862670898, 0.6690000295639038, 0.6692000031471252, 0.6699000000953674, 0.670799970626831, 0.6707000136375427, 0.6711000204086304, 0.6692000031471252, 0.670199990272522, 0.6646000146865845, 0.6674000024795532, 0.6723999977111816, 0.6722999811172485, 0.6639000177383423, 0.6689000129699707, 0.6718000173568726, 0.647599995136261, 0.6787999868392944, 0.6632999777793884, 0.6668000221252441, 0.6675999760627747, 0.6528000235557556, 0.6735000014305115, 0.663100004196167, 0.6728000044822693, 0.6740999817848206, 0.661899983882904, 0.6662999987602234, 0.6699000000953674, 0.6496000289916992, 0.6586999893188477, 0.6699000000953674]
KD2 pt test accuracy =  [0.474700003862381, 0.5041999816894531, 0.5203999876976013, 0.5543000102043152, 0.5726000070571899, 0.5859000086784363, 0.5940999984741211, 0.6140999794006348, 0.6184999942779541, 0.6116999983787537, 0.6240000128746033, 0.6326000094413757, 0.6348999738693237, 0.6276000142097473, 0.6403999924659729, 0.6556000113487244, 0.6431999802589417, 0.6498000025749207, 0.6438999772071838, 0.661899983882904, 0.666700005531311, 0.6579999923706055, 0.6675999760627747, 0.6705999970436096, 0.6577000021934509, 0.6690999865531921, 0.6650000214576721, 0.6723999977111816, 0.6654000282287598, 0.6743000149726868, 0.6748999953269958, 0.6686000227928162, 0.6660000085830688, 0.6764000058174133, 0.6811000108718872, 0.6791999936103821, 0.6696000099182129, 0.6764000058174133, 0.6854000091552734, 0.6823999881744385, 0.6870999932289124, 0.6775000095367432, 0.6881999969482422, 0.6780999898910522, 0.690500020980835, 0.6861000061035156, 0.6886000037193298, 0.6805999875068665, 0.6741999983787537, 0.6765000224113464, 0.678600013256073, 0.6840000152587891, 0.6818000078201294, 0.6650000214576721, 0.6665999889373779, 0.677299976348877, 0.6888999938964844, 0.6746000051498413, 0.6801000237464905, 0.6895999908447266, 0.6880999803543091, 0.6891999840736389, 0.6854000091552734, 0.6804999709129333, 0.6901999711990356, 0.6916000247001648, 0.6833999752998352, 0.6915000081062317, 0.6895999908447266, 0.6919999718666077, 0.6947000026702881, 0.6924999952316284, 0.6852999925613403, 0.6876000165939331, 0.6802999973297119, 0.6880999803543091, 0.6825000047683716, 0.6894000172615051, 0.6916000247001648, 0.6905999779701233, 0.6930999755859375, 0.6920999884605408, 0.6880999803543091, 0.6919000148773193, 0.6919000148773193, 0.6980999708175659, 0.6913999915122986, 0.692799985408783, 0.6876999735832214, 0.6953999996185303, 0.6953999996185303, 0.6942999958992004, 0.6929000020027161, 0.6843000054359436, 0.6883000135421753, 0.6610999703407288, 0.6945000290870667, 0.6915000081062317, 0.6966999769210815, 0.689300000667572, 0.6855000257492065, 0.6901999711990356, 0.6919000148773193, 0.6697999835014343, 0.6854000091552734, 0.6814000010490417, 0.6868000030517578, 0.6973999738693237, 0.6966000199317932, 0.680899977684021, 0.6922000050544739, 0.6962000131607056, 0.6848999857902527, 0.6895999908447266, 0.652999997138977, 0.6848000288009644, 0.6894000172615051, 0.7039999961853027, 0.6974999904632568, 0.699999988079071, 0.6898999810218811, 0.7045000195503235, 0.7005000114440918, 0.6919000148773193, 0.6931999921798706, 0.6690999865531921, 0.6945000290870667, 0.6947000026702881, 0.6988000273704529, 0.692300021648407, 0.6955999732017517, 0.6920999884605408, 0.689300000667572, 0.6952999830245972, 0.6769999861717224, 0.6995999813079834, 0.6902999877929688, 0.6758000254631042, 0.6941999793052673, 0.6937999725341797, 0.6851000189781189, 0.6940000057220459, 0.6814000010490417, 0.6926000118255615, 0.6916000247001648, 0.690500020980835, 0.6901000142097473, 0.6927000284194946, 0.6898999810218811]

lr = 1e-4 alpha = 0.95 beta = 0.9 temp = 10 - out.309004 pre-train start = 50 Batchnorm

lr = 1e-4 alpha = 0.95 beta = 0.9 temp = 10 - out.309171 pre-train start = 50 Dropout

lr = 1e-4 alpha = 0.5 beta = 0.9 temp = 3 - out.641962 pre-train start = 50 Dropout
KD2 test accuracy =  [0.20149999856948853, 0.2671999931335449, 0.31839999556541443, 0.35749998688697815, 0.38100001215934753, 0.4077000021934509, 0.4194999933242798, 0.4399000108242035, 0.45019999146461487, 0.4691999852657318, 0.4799000024795532, 0.491100013256073, 0.5044000148773193, 0.5192000269889832, 0.5245000123977661, 0.5374000072479248, 0.5435000061988831, 0.546500027179718, 0.5486000180244446, 0.5619000196456909, 0.5666000247001648, 0.5726000070571899, 0.5853999853134155, 0.5918999910354614, 0.5952000021934509, 0.5949000120162964, 0.6037999987602234, 0.5990999937057495, 0.6100999712944031, 0.6144999861717224, 0.6169999837875366, 0.6284000277519226, 0.6291000247001648, 0.635200023651123, 0.6373999714851379, 0.6409000158309937, 0.6412000060081482, 0.6439999938011169, 0.6470000147819519, 0.6527000069618225, 0.6448000073432922, 0.6516000032424927, 0.6565999984741211, 0.6639999747276306, 0.6564000248908997, 0.6643000245094299, 0.6589000225067139, 0.664900004863739, 0.6751000285148621, 0.6765000224113464, 0.6758999824523926, 0.6757000088691711, 0.6759999990463257, 0.6862000226974487, 0.6848000288009644, 0.6869999766349792, 0.6840000152587891, 0.685699999332428, 0.6834999918937683, 0.6852999925613403, 0.6862000226974487, 0.6819999814033508, 0.6841999888420105, 0.6924999952316284, 0.7013000249862671, 0.6922000050544739, 0.7005000114440918, 0.6958000063896179, 0.6937000155448914, 0.6980000138282776, 0.6973999738693237, 0.703499972820282, 0.7013999819755554, 0.7050999999046326, 0.6995999813079834, 0.7087000012397766, 0.704200029373169, 0.7092000246047974, 0.7074999809265137, 0.6959999799728394, 0.7096999883651733, 0.7081000208854675, 0.7134000062942505, 0.7121000289916992, 0.7034000158309937, 0.7121999859809875, 0.7069000005722046, 0.7103999853134155, 0.7057999968528748, 0.7105000019073486, 0.7046999931335449, 0.7106999754905701, 0.7142000198364258, 0.7174999713897705, 0.7161999940872192, 0.7106999754905701, 0.7188000082969666, 0.7217000126838684, 0.7246000170707703, 0.7188000082969666, 0.7200000286102295, 0.7200000286102295, 0.7250000238418579, 0.7128999829292297, 0.7215999960899353, 0.7196000218391418, 0.713100016117096, 0.72079998254776, 0.7283999919891357, 0.732699990272522, 0.7245000004768372, 0.7225000262260437, 0.7264999747276306, 0.7293000221252441, 0.7271000146865845, 0.7235999703407288, 0.7286999821662903, 0.7263000011444092, 0.7310000061988831, 0.7287999987602234, 0.7321000099182129, 0.7311999797821045, 0.7319999933242798, 0.7271000146865845, 0.7254999876022339, 0.7263000011444092, 0.732200026512146, 0.7329000234603882, 0.727400004863739, 0.7379000186920166, 0.739300012588501, 0.7307999730110168, 0.7329999804496765, 0.7336999773979187, 0.7383000254631042, 0.7347999811172485, 0.7373999953269958, 0.7330999970436096, 0.7339000105857849, 0.7348999977111816, 0.7376000285148621, 0.7390000224113464, 0.7386000156402588, 0.7425000071525574, 0.7335000038146973, 0.737500011920929, 0.7408000230789185, 0.7401999831199646, 0.7303000092506409, 0.7376999855041504, 0.7408999800682068, 0.7437000274658203, 0.7353000044822693, 0.7405999898910522, 0.7369999885559082, 0.7425000071525574, 0.7402999997138977, 0.7443000078201294, 0.7498000264167786, 0.7348999977111816, 0.7407000064849854, 0.7430999875068665, 0.7470999956130981, 0.739799976348877, 0.741599977016449, 0.7429999709129333, 0.7386999726295471, 0.7426999807357788, 0.7325999736785889, 0.7447999715805054, 0.7479000091552734, 0.7461000084877014, 0.7466999888420105, 0.7454000115394592, 0.746399998664856, 0.7505000233650208, 0.7366999983787537, 0.746999979019165, 0.7390999794006348, 0.7416999936103821, 0.7480000257492065, 0.7516999840736389, 0.7459999918937683, 0.7511000037193298, 0.7426000237464905, 0.7457000017166138, 0.7433000206947327, 0.7473999857902527, 0.744700014591217, 0.745199978351593, 0.748199999332428, 0.7411999702453613, 0.753000020980835, 0.7531999945640564, 0.7487999796867371, 0.7450000047683716, 0.7519999742507935, 0.743399977684021, 0.7501999735832214, 0.741100013256073]
KD2 distillation loss =  [0.7871592044830322, 1.3410167694091797, 0.9986200928688049, 1.1051454544067383, 1.1391096115112305, 0.7870993614196777, 0.9411205649375916, 0.9337542057037354, 0.607284665107727, 0.7138881087303162, 0.9100112915039062, 0.6739363670349121, 0.6693611145019531, 0.6461464166641235, 0.6322426199913025, 0.7030384540557861, 0.6516678929328918, 0.5896331071853638, 0.7282107472419739, 0.6747434139251709, 0.9624555706977844, 0.45461782813072205, 0.671148419380188, 0.5002896785736084, 0.5572657585144043, 0.4115712642669678, 0.43409764766693115, 0.42333361506462097, 0.4922516345977783, 0.47700050473213196, 0.6025142669677734, 0.5729089975357056, 0.3617939352989197, 0.43188703060150146, 0.3681584894657135, 0.7008503675460815, 0.47401750087738037, 0.49973803758621216, 0.4544600248336792, 0.43761229515075684, 0.40676313638687134, 0.5174828767776489, 0.4499775171279907, 0.3414403200149536, 0.43066495656967163, 0.495077908039093, 0.6833703517913818, 0.3758752942085266, 0.5567318201065063, 0.42913973331451416, 0.3377401828765869, 0.5975868105888367, 0.5763370990753174, 0.42565232515335083, 0.3283512592315674, 0.3453218638896942, 0.28224825859069824, 0.517924964427948, 0.5146214962005615, 0.32705941796302795, 0.4444997310638428, 0.3712458610534668, 0.32729828357696533, 0.3369118571281433, 0.31173765659332275, 0.5346062183380127, 0.5038756132125854, 0.3704315721988678, 0.5838367938995361, 0.44317835569381714, 0.36941003799438477, 0.3592440187931061, 0.21184927225112915, 0.23453190922737122, 0.4382873475551605, 0.3578474521636963, 0.393565833568573, 0.2909661531448364, 0.31750017404556274, 0.4938052296638489, 0.3781093955039978, 0.3972201347351074, 0.4014999270439148, 0.4065401256084442, 0.3599832057952881, 0.22862640023231506, 0.3607178330421448, 0.3311280608177185, 0.3039836287498474, 0.3870564103126526, 0.2731861472129822, 0.3019981384277344, 0.3370743691921234, 0.31642091274261475, 0.3049963116645813, 0.4950338304042816, 0.27123719453811646, 0.2513456344604492, 0.3190571665763855, 0.42288559675216675, 0.30229368805885315, 0.31597083806991577, 0.2870437502861023, 0.37004774808883667, 0.6793122887611389, 0.2426028996706009, 0.2795112431049347, 0.3517705202102661, 0.41937747597694397, 0.2541689872741699, 0.46522119641304016, 0.4742282032966614, 0.3297872245311737, 0.48456674814224243, 0.4404357373714447, 0.2614678144454956, 0.3686622977256775, 0.46017202734947205, 0.45290884375572205, 0.30654439330101013, 0.2686707675457001, 0.2924491763114929, 0.3342996835708618, 0.22874575853347778, 0.30676060914993286, 0.34744712710380554, 0.45991984009742737, 0.3923477530479431, 0.38361355662345886, 0.32375335693359375, 0.24538132548332214, 0.3245515823364258, 0.2736915647983551, 0.45325395464897156, 0.4769876301288605, 0.4795405864715576, 0.2718495726585388, 0.29134607315063477, 0.3288007974624634, 0.5386894941329956, 0.3366650342941284, 0.2493840456008911, 0.29990196228027344, 0.2903265953063965, 0.330153226852417, 0.23685914278030396, 0.4198257327079773, 0.3435520529747009, 0.20144079625606537, 0.4115573763847351, 0.5082359313964844, 0.3044372797012329, 0.2689543068408966, 0.25492149591445923, 0.3476405143737793, 0.35114413499832153, 0.26559367775917053, 0.38287997245788574, 0.29348286986351013, 0.36494484543800354, 0.4099682569503784, 0.4526335597038269, 0.33783066272735596, 0.42233550548553467, 0.34918302297592163, 0.4494056701660156, 0.3599148094654083, 0.4326822757720947, 0.27869170904159546, 0.3136177659034729, 0.3042903542518616, 0.36332613229751587, 0.3317531645298004, 0.27019599080085754, 0.37893232703208923, 0.3673546612262726, 0.3020401895046234, 0.235656276345253, 0.3407430648803711, 0.23219163715839386, 0.39010924100875854, 0.271454393863678, 0.22335365414619446, 0.3668723702430725, 0.3711305558681488, 0.22547313570976257, 0.4244823455810547, 0.4526672959327698, 0.3478700518608093, 0.38377687335014343, 0.26179972290992737, 0.22184991836547852, 0.328212171792984, 0.32567769289016724, 0.36189666390419006, 0.30837851762771606, 0.31055447459220886, 0.33158445358276367, 0.329925537109375, 0.23712992668151855]
with 500 epocs out.700421

lr = 1e-4 alpha = 0.8 beta = 0.9 temp = 3 - out.657782 pre-train start = 50 Dropout
KD2 test accuracy =  [0.19009999930858612, 0.23090000450611115, 0.2948000133037567, 0.3287999927997589, 0.362199991941452, 0.38670000433921814, 0.4032000005245209, 0.42179998755455017, 0.43939998745918274, 0.4562999904155731, 0.4648999869823456, 0.47350001335144043, 0.49970000982284546, 0.49970000982284546, 0.5216000080108643, 0.5382999777793884, 0.5378000140190125, 0.5533000230789185, 0.5526999831199646, 0.5685999989509583, 0.5748000144958496, 0.5570999979972839, 0.5889000296592712, 0.5838000178337097, 0.5953999757766724, 0.6047999858856201, 0.612500011920929, 0.6184999942779541, 0.6189000010490417, 0.6126000285148621, 0.6322000026702881, 0.6349999904632568, 0.6391000151634216, 0.6401000022888184, 0.638700008392334, 0.6481000185012817, 0.6549000144004822, 0.6517999768257141, 0.6330999732017517, 0.6625000238418579, 0.6722999811172485, 0.6700999736785889, 0.6686999797821045, 0.6660000085830688, 0.6723999977111816, 0.6746000051498413, 0.6779000163078308, 0.6761999726295471, 0.6744999885559082, 0.6772000193595886, 0.6837000250816345, 0.6875, 0.6890000104904175, 0.6797000169754028, 0.6926000118255615, 0.6894000172615051, 0.6966000199317932, 0.6969000101089478, 0.6949999928474426, 0.692799985408783, 0.6869000196456909, 0.6972000002861023, 0.6990000009536743, 0.6983000040054321, 0.6990000009536743, 0.7016000151634216, 0.707099974155426, 0.7006000280380249, 0.6992999911308289, 0.703499972820282, 0.7008000016212463, 0.7121999859809875, 0.7057999968528748, 0.7164999842643738, 0.7062000036239624, 0.713100016117096, 0.7105000019073486, 0.7113999724388123, 0.7027999758720398, 0.7059999704360962, 0.7156999707221985, 0.7184000015258789, 0.7181000113487244, 0.7204999923706055, 0.7239999771118164, 0.722599983215332, 0.725600004196167, 0.7210000157356262, 0.7199000120162964, 0.7282999753952026, 0.7221999764442444, 0.7167999744415283, 0.7192999720573425, 0.7222999930381775, 0.7239999771118164, 0.7229999899864197, 0.7231000065803528, 0.7249000072479248, 0.7275999784469604, 0.7265999913215637, 0.7221999764442444, 0.7272999882698059, 0.7258999943733215, 0.7311999797821045, 0.7257999777793884, 0.7335000038146973, 0.7319999933242798, 0.7333999872207642, 0.732200026512146, 0.7299000024795532, 0.7350000143051147, 0.7282999753952026, 0.7333999872207642, 0.730400025844574, 0.7303000092506409, 0.7261999845504761, 0.733299970626831, 0.7254999876022339, 0.7285000085830688, 0.7372000217437744, 0.7317000031471252, 0.7379000186920166, 0.7342000007629395, 0.7364000082015991, 0.7364000082015991, 0.7390999794006348, 0.7330999970436096, 0.7372999787330627, 0.7261999845504761, 0.7390999794006348, 0.7383999824523926, 0.7400000095367432, 0.7315000295639038, 0.7353000044822693, 0.738099992275238, 0.7357000112533569, 0.7401000261306763, 0.7318999767303467, 0.7384999990463257, 0.7383000254631042, 0.7350999712944031, 0.7386000156402588, 0.7328000068664551, 0.7383000254631042, 0.7350999712944031, 0.7390999794006348, 0.7347000241279602, 0.7396000027656555, 0.7397000193595886, 0.7396000027656555, 0.7351999878883362, 0.7409999966621399, 0.7427999973297119, 0.7419000267982483, 0.7384999990463257, 0.7412999868392944, 0.7400000095367432, 0.7458000183105469, 0.739799976348877, 0.7425000071525574, 0.7394999861717224, 0.7412999868392944, 0.7390000224113464, 0.7394999861717224, 0.7443000078201294, 0.7350000143051147, 0.7427999973297119, 0.7458000183105469, 0.7486000061035156, 0.7432000041007996, 0.7411999702453613, 0.744700014591217, 0.748199999332428, 0.7476999759674072, 0.7404000163078308, 0.73580002784729, 0.7426999807357788, 0.7433000206947327, 0.7476999759674072, 0.7476000189781189, 0.7441999912261963, 0.7480999827384949, 0.746999979019165, 0.7457000017166138, 0.7495999932289124, 0.7415000200271606, 0.7476999759674072, 0.7436000108718872, 0.7486000061035156, 0.7433000206947327, 0.7448999881744385, 0.7444000244140625, 0.7495999932289124, 0.7437000274658203, 0.741100013256073, 0.7402999997138977, 0.7502999901771545, 0.7468000054359436, 0.7486000061035156, 0.7487000226974487]
KD2 distillation loss =  [1.4070411920547485, 1.0167059898376465, 1.0260555744171143, 0.570500910282135, 0.6004698872566223, 0.6474026441574097, 0.5268874168395996, 0.6886751651763916, 0.8738529086112976, 0.7330048680305481, 0.7107346057891846, 0.5043946504592896, 0.6115565299987793, 0.7234911918640137, 0.7732465267181396, 0.4925687313079834, 0.5846859216690063, 0.5207072496414185, 0.3449341952800751, 0.6146495938301086, 0.4831247627735138, 0.4585416913032532, 0.3797729015350342, 0.33466288447380066, 0.5462702512741089, 0.5335054397583008, 0.4179249703884125, 0.5940957069396973, 0.6135232448577881, 0.4762148857116699, 0.4318919777870178, 0.4794664680957794, 0.21381670236587524, 0.36463505029678345, 0.2978760004043579, 0.5291873216629028, 0.3745766580104828, 0.35303694009780884, 0.42254921793937683, 0.28666967153549194, 0.3567342758178711, 0.33262899518013, 0.3600232005119324, 0.3108353614807129, 0.27956992387771606, 0.37602490186691284, 0.2587240934371948, 0.3288584351539612, 0.4648122489452362, 0.26672834157943726, 0.3751830458641052, 0.30630093812942505, 0.2521124482154846, 0.33393394947052, 0.3029782772064209, 0.2633516192436218, 0.374378502368927, 0.36455875635147095, 0.27064356207847595, 0.382268488407135, 0.28631657361984253, 0.2884501814842224, 0.25342071056365967, 0.46408945322036743, 0.2378244400024414, 0.36393094062805176, 0.15688103437423706, 0.25414425134658813, 0.16878794133663177, 0.3195737302303314, 0.30988404154777527, 0.30316251516342163, 0.2594297528266907, 0.30077672004699707, 0.30346304178237915, 0.3492909073829651, 0.20816300809383392, 0.1715359389781952, 0.18509197235107422, 0.28499266505241394, 0.27811115980148315, 0.3070839047431946, 0.31455692648887634, 0.3553048074245453, 0.380800724029541, 0.28667765855789185, 0.3762407898902893, 0.2530364990234375, 0.3755604326725006, 0.3575592041015625, 0.41093748807907104, 0.25917890667915344, 0.3002254366874695, 0.20286035537719727, 0.23165616393089294, 0.3612518608570099, 0.29706764221191406, 0.23150977492332458, 0.294582724571228, 0.20049838721752167, 0.1867644488811493, 0.1939626932144165, 0.19638018310070038, 0.2678152918815613, 0.3967595100402832, 0.5827844738960266, 0.2288583368062973, 0.29962605237960815, 0.25883591175079346, 0.2977904677391052, 0.34059834480285645, 0.31139734387397766, 0.24135810136795044, 0.20398738980293274, 0.2963547706604004, 0.2676991820335388, 0.203810453414917, 0.28650951385498047, 0.3762422204017639, 0.3123202919960022, 0.27609914541244507, 0.263092041015625, 0.371988981962204, 0.19024722278118134, 0.20139533281326294, 0.2640831172466278, 0.23552049696445465, 0.24954578280448914, 0.19109046459197998, 0.21976149082183838, 0.19276537001132965, 0.28094208240509033, 0.16607023775577545, 0.19009150564670563, 0.23625344038009644, 0.21797722578048706, 0.2953683137893677, 0.21408803761005402, 0.2121707797050476, 0.24885451793670654, 0.2733827233314514, 0.29107165336608887, 0.2647361159324646, 0.2257513403892517, 0.22654151916503906, 0.2601872980594635, 0.22936803102493286, 0.34031665325164795, 0.33970415592193604, 0.29631373286247253, 0.18186217546463013, 0.288857638835907, 0.25877654552459717, 0.2870103120803833, 0.19916266202926636, 0.20846715569496155, 0.2834467589855194, 0.2626083493232727, 0.30644965171813965, 0.4079762101173401, 0.23953379690647125, 0.29452913999557495, 0.2749243378639221, 0.22041034698486328, 0.1651623249053955, 0.15729588270187378, 0.2643410563468933, 0.32795771956443787, 0.20912694931030273, 0.267126202583313, 0.22851254045963287, 0.32239532470703125, 0.1638675034046173, 0.17051297426223755, 0.30014216899871826, 0.33793896436691284, 0.18850964307785034, 0.2732478678226471, 0.25173047184944153, 0.20474272966384888, 0.20037150382995605, 0.17964321374893188, 0.22363054752349854, 0.21975189447402954, 0.18893788754940033, 0.2926955223083496, 0.32126474380493164, 0.17778630554676056, 0.1900191307067871, 0.18526071310043335, 0.2585638761520386, 0.17460669577121735, 0.3418349623680115, 0.2614235579967499, 0.20438823103904724, 0.30305665731430054, 0.28721126914024353, 0.16835904121398926, 0.15715278685092926, 0.19609332084655762]

lr = 1e-4 alpha = 0.5 beta = 0.9 temp = 1 - out.700395 pre-train start = 50,
epochs = 300

lr = 1e-4 alpha = 0.9 beta = 0.9 temp = 1 - out.700430 pre-train start = 50,
epochs = 300

correct mistakes in feature function

"""
import sys
sys.path.insert(1, '/home/orram/Documents/GitHub/imagewalker')
sys.path.insert(1, '/home/labs/ahissarlab/orra/imagewalker')
import tensorflow as tf
from tensorflow import keras

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
  tf.config.experimental.set_memory_growth(gpu, True)
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.Session(config=config)

import numpy as np

import pandas as pd
import matplotlib.pyplot as plt

import tensorflow.keras as keras
import tensorflow as tf

from tensorflow.keras.datasets import cifar10

# load dataset
(trainX, trainy), (testX, testy) = cifar10.load_data()
images, labels = trainX, trainy
#%%
# import importlib
# importlib.reload(misc)
# from misc import Logger
import os 
import sys

######################## Network Parameters ##################################

if len(sys.argv) == 1:
    st_parameters = {
    'lr' : 1,#float(sys.argv[1]),
    'epochs' : 5,#int(sys.argv[2]),
    "student_fst_learning" : 1,#int(sys.argv[3]), #The first learning stage of the student - number of epochs
    'alpha': 0.5,#float(sys.argv[4]), #KD weights
    'temp' : 12,#int(sys.argv[5]),   #KD weights
    'beta' : 0.9#float(sys.argv[6]), #features st weights
    
    }
else:
    st_parameters = {
    'lr' : float(sys.argv[1]),
    'epochs' : int(sys.argv[2]),
    "student_fst_learning" : int(sys.argv[3]), #The first learning stage of the student - number of epochs
    'alpha': float(sys.argv[4]), #KD weights
    'temp' : int(sys.argv[5]),   #KD weights
    'beta' : float(sys.argv[6]), #features st weights
    
    }

print('Run Parameters:', st_parameters)
class teacher_training(keras.Model):
    def __init__(self,teacher):
        super(teacher_training, self).__init__()
        self.teacher = teacher


    def compile(
        self,
        optimizer,
        metrics,
        loss_fn,

    ):
        """ Configure the distiller.

        Args:
            optimizer: Keras optimizer for the student weights
            metrics: Keras metrics for evaluation
            student_loss_fn: Loss function of difference between student
                predictions and ground-truth
            distillation_loss_fn: Loss function of difference between soft
                student predictions and soft teacher predictions
            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn
            temperature: Temperature for softening probability distributions.
                Larger temperature gives softer distributions.
        """
        super(teacher_training, self).compile(optimizer=optimizer, metrics=metrics)
        self.loss_fn = loss_fn


    def train_step(self, data):
        # Unpack data
        HR_data , y = data

        with tf.GradientTape() as tape:
            # Forward pass of student
            features, predictions = self.teacher(HR_data, training=True)

            # Compute losses
            loss = self.loss_fn(y, predictions)

        # Compute gradients
        trainable_vars = self.teacher.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Update the metrics configured in `compile()`.
        self.compiled_metrics.update_state(y, predictions)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}

        return results

    def test_step(self, data):
        # Unpack the data
        x, y = data

        # Compute predictions
        features, y_prediction = self.teacher(x, training=False)

        # Calculate the loss
        loss = self.loss_fn(y, y_prediction)

        # Update the metrics.
        self.compiled_metrics.update_state(y, y_prediction)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}

        return results
    
    def call(self, data, training = False):
        x = data
        features, prediction = self.teacher(x, training = training)
        return features, prediction 


class Distiller(keras.Model):
    def __init__(self, student, teacher):
        super(Distiller, self).__init__()
        self.teacher = teacher
        self.student = student

    def compile(
        self,
        optimizer,
        metrics,
        student_loss_fn,
        distillation_loss_fn,
        alpha=0.1,
        temperature=3,
    ):
        """ Configure the distiller.

        Args:
            optimizer: Keras optimizer for the student weights
            metrics: Keras metrics for evaluation
            student_loss_fn: Loss function of difference between student
                predictions and ground-truth
            distillation_loss_fn: Loss function of difference between soft
                student predictions and soft teacher predictions
            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn
            temperature: Temperature for softening probability distributions.
                Larger temperature gives softer distributions.
        """
        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)
        self.student_loss_fn = student_loss_fn
        self.distillation_loss_fn = distillation_loss_fn
        self.alpha = alpha
        self.temperature = temperature

    def train_step(self, data):
        # Unpack data
        HR_data , y = data
        # Forward pass of teacher
        
        teacher_features, teacher_predictions = self.teacher.call(HR_data, training=False)
        
        with tf.GradientTape() as tape:
            # Forward pass of student
            student_features, student_predictions = self.student(HR_data, training=True)

            # Compute losses
            student_loss = self.student_loss_fn(y, student_predictions)
            distillation_loss = self.distillation_loss_fn(
                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),
                tf.nn.log_softmax(student_predictions / self.temperature, axis=1),
            )
            loss = (1 - self.alpha) * student_loss + (self.alpha * self.temperature * self.temperature) * distillation_loss

        # Compute gradients
        trainable_vars = self.student.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Update the metrics configured in `compile()`.
        self.compiled_metrics.update_state(y, student_predictions)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update(
            {"student_loss": student_loss, "distillation_loss": distillation_loss}
        )
        return results

    def test_step(self, data):
        # Unpack the data
        x, y = data

        # Compute predictions
        student_features, y_prediction = self.student(x, training=False)

        # Calculate the loss
        student_loss = self.student_loss_fn(y, y_prediction)

        # Update the metrics.
        self.compiled_metrics.update_state(y, y_prediction)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update({"student_loss": student_loss})
        return results
    
class Distiller2(keras.Model):
    def __init__(self, student, teacher):
        super(Distiller2, self).__init__()
        self.teacher = teacher
        self.student = student

    def compile(
        self,
        optimizer,
        metrics,
        student_loss_fn,
        distillation_loss_fn,
        alpha=0.1,
        temperature=3,
    ):
        """ Configure the distiller.

        Args:
            optimizer: Keras optimizer for the student weights
            metrics: Keras metrics for evaluation
            student_loss_fn: Loss function of difference between student
                predictions and ground-truth
            distillation_loss_fn: Loss function of difference between soft
                student predictions and soft teacher predictions
            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn
            temperature: Temperature for softening probability distributions.
                Larger temperature gives softer distributions.
        """
        super(Distiller2, self).compile(optimizer=optimizer, metrics=metrics)
        self.student_loss_fn = student_loss_fn
        self.distillation_loss_fn = distillation_loss_fn
        self.alpha = alpha
        self.temperature = temperature

    def train_step(self, data):
        # Unpack data
        HR_data , y = data
        # Forward pass of teacher
        
        teacher_features, teacher_predictions = self.teacher.call(HR_data, training=False)
        
        with tf.GradientTape() as tape:
            # Forward pass of student
            student_features, student_predictions = self.student(HR_data, training=True)

            # Compute losses
            student_loss = self.student_loss_fn(y, student_predictions)
            distillation_loss = self.distillation_loss_fn(
                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),
                tf.nn.softmax(student_predictions / self.temperature, axis=1),
            )
            loss = (1-self.alpha) * student_loss + (self.alpha) * distillation_loss

        # Compute gradients
        trainable_vars = self.student.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Update the metrics configured in `compile()`.
        self.compiled_metrics.update_state(y, student_predictions)

        # Return a dict of performancekeras.layers.TimeDistributed(
        results = {m.name: m.result() for m in self.metrics}
        results.update(
            {"student_loss": student_loss, "distillation_loss": distillation_loss}
        )
        return results

    def test_step(self, data):
        # Unpack the data
        x, y = data

        # Compute predictions
        student_features, y_prediction = self.student(x, training=False)

        # Calculate the loss
        student_loss = self.student_loss_fn(y, y_prediction)

        # Update the metrics.
        self.compiled_metrics.update_state(y, y_prediction)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update({"student_loss": student_loss})
        return results
    
class feature_st(keras.Model):
    def __init__(self, student, teacher):
        super(feature_st, self).__init__()
        self.teacher = teacher
        self.student = student

    def compile(
        self,
        optimizer,
        metrics,
        student_loss_fn,
        features_loss_fn,
        beta=0.1,
        temperature=3,
    ):
        """ Configure the distiller.

        Args:
            optimizer: Keras optimizer for the student weights
            metrics: Keras metrics for evaluation
            student_loss_fn: Loss function of difference between student
                predictions and ground-truth
            distillation_loss_fn: Loss function of difference between soft
                student predictions and soft teacher predictions
            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn
            temperature: Temperature for softening probability distributions.
                Larger temperature gives softer distributions.
        """
        super(feature_st, self).compile(optimizer=optimizer, metrics=metrics)
        self.student_loss_fn = student_loss_fn
        self.features_loss_fn = features_loss_fn
        self.beta = beta
        self.temperature = temperature

    def train_step(self, data):
        # Unpack data
        HR_data , y = data
        # Forward pass of teacher
        teacher_features, teacher_predictions = self.teacher(HR_data, training=False)
        # layer_name = 'teacher_features'
        # intermediate_layer_model = keras.Model(inputs=model.input,
        #                                outputs=model.get_layer(layer_name).output)
        # intermediate_output = intermediate_layer_model(data)
        with tf.GradientTape() as tape:
            # Forward pass of student
            student_features, student_predictions = self.student(HR_data, training=True)

            # Compute losses
            student_loss = self.student_loss_fn(y, student_predictions)
            features_loss = self.features_loss_fn(
                teacher_features,
                student_features,
            )
            loss = (1-self.beta) * student_loss + (self.beta) * features_loss

        # Compute gradients
        trainable_vars = self.student.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Update the metrics configured in `compile()`.
        self.compiled_metrics.update_state(y, student_predictions)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update(
            {"student_loss": student_loss, "features_loss": features_loss}
        )
        return results

    def test_step(self, data):
        # Unpack the data
        x, y = data

        # Compute predictions
        _, y_prediction = self.student(x, training=False)
        print(y_prediction.shape)
        # Calculate the loss
        student_loss = self.student_loss_fn(y, y_prediction)

        # Update the metrics.
        self.compiled_metrics.update_state(y, y_prediction)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update({"student_loss": student_loss})
        return results


def teacher(input_size = 32 ,dropout = 0.2):
    '''
    Takes only the first image from the burst and pass it trough a net that 
    aceives ~80% accuracy on full res cifar. 
    '''
    inputA = keras.layers.Input(shape=(input_size,input_size,3))

    
    # define CNN model
    x1=keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same')(inputA)
    print(x1.shape)
    x1=keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)
    x1=keras.layers.Dropout(dropout)(x1)

    x1=keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)
    x1=keras.layers.Dropout(dropout)(x1)

    x1=keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)
    x1=keras.layers.Dropout(dropout)(x1)
    print(x1.shape)

    # x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    # print(x1.shape)st_parameters['epochs']

    x1 = keras.layers.Flatten(name = 'teacher_features')(x1)
    x1 = keras.layers.Dense(512, activation = 'relu')(x1)
    print(x1.shape)
    x2 = keras.layers.Dense(10)(x1)
    print(x2.shape)
    model = keras.models.Model(inputs=inputA,outputs=[x1,x2], name = 'teacher')
    opt=tf.keras.optimizers.Adam(lr=1e-3)
    model = teacher_training(model)
    model.compile(
        optimizer=opt,
        loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"],
    )
    return model

def student(hidden_size = 128,input_size = 32, 
            concat = True, cnn_dropout = 0.2, rnn_dropout = 0.2):
    '''
    
    CNN RNN combination that extends the CNN to a network that achieves 
    ~80% accuracy on full res cifar.
    This student network outputs teacher = extended_cnn_one_img(n_timesteps = 1, input_size = st_parameters['epochs']32, dropout = 0.2)
    Parameters
    ----------
    n_timesteps : TYPE, optional
        DESCRIPTION. The default is 5.
    img_dim : TYPE, optional
        DESCRIPTION. The default is 32.
    hidden_size : TYPE, optional
        DESCRIPTION. The default is 128.teacher_network = teacher(input_size = 32, dropout = 0.2)
    input_size : TYPE, optional
        DESCRIPTION. The default is 32.epochshidden_size

    Returns
    -------
    model : TYPE
        DESCRIPTION.

    '''
    inputA = keras.layers.Input(shape=(input_size,input_size,3))
    

    # define CNN model

    x1=keras.layers.Conv2D(16,(3,3),activation='relu', padding = 'same')(inputA)
    x1=keras.layers.Dropout(cnn_dropout)(x1)
    #x1=keras.layers.BatchNormalization(momentum=0.1, epsilon = 1e-5)(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)
    
    x1=keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.Dropout(cnn_dropout)(x1)
    #x1=keras.layers.BatchNormalization(momentum=0.1, epsilon = 1e-5)(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)

    x1=keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.Dropout(cnn_dropout)(x1)
    #x1=keras.layers.BatchNormalization(momentum=0.1, epsilon = 1e-5)(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)
    print(x1.shape)


    x1=keras.layers.Flatten(name = 'cnn_student_features')(x1)
    print(x1.shape)

    x = keras.layers.Dense(64, activation = 'relu', name = "student_features")(x1)
    print(x1.shape)
    
    x = keras.layers.Dense(10)(x1) #activation will be in the distiller
    model = keras.models.Model(inputs=inputA,outputs=[x1,x], name = 'student{}'.format(concat))

    return model

def length_regulerize(desir_len, vec):
    new_vec = np.ones(desir_len) * vec[-1]
    new_vec[:len(vec)] = vec
    return new_vec
#%%
accur_dataset = pd.DataFrame()
loss_dataset = pd.DataFrame()
#%%
teacher_network = teacher(input_size = 32, dropout = 0.2)

#%%
print('######################### TRAIN TEACHER ##############################')
callback = tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=10)

teacher_history = teacher_network.fit(
                           trainX,
                           trainy,
                           batch_size = 64,
                           epochs = 5,#st_parameters['epochs'],
                           validation_data = (testX,testy),
                           verbose = 1,
                           callbacks=[callback]
                           )
print('teacher test accuracy = ',teacher_history.history['val_sparse_categorical_accuracy'])
teacher_test = length_regulerize(st_parameters['epochs'], teacher_history.history['val_sparse_categorical_accuracy'])
teacher_train = length_regulerize(st_parameters['epochs'], teacher_history.history['sparse_categorical_accuracy'])

accur_dataset['teacher_test'] = teacher_test
accur_dataset['teacher_train'] = teacher_train

#%%
print('######################### TRAIN STUDENT ##############################')

student_network = student(hidden_size = 128,input_size = 32, 
            concat = True, cnn_dropout = 0.2 , rnn_dropout = 0.2)


#keras.utils.plot_model(student_network, expand_nested=True)
#%%
# print('################## Train Student - No pre-training ####################')
# print('####################### Knowledge Distillation ########################')
# KD_student = keras.models.clone_model(student_network)
# KD_student.set_weights(student_network.get_weights()) 
# KD = Distiller(KD_student, teacher_network)

# KD.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
#            metrics   = ["sparse_categorical_accuracy"],
#            student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
#            distillation_loss_fn = keras.losses.KLDivergence(),
#            alpha = st_parameters['alpha'],
#            temperature = st_parameters['temp'])

# KD_history = KD.fit(trainX,
#         trainy,
#         batch_size = 64,
#         epochs = st_parameters['epochs'],
#         validation_data = (testX,testy),
#         verbose = 0,
#         )

# print('KD test accuracy = ',KD_history.history['val_sparse_categorical_accuracy'])
# print('KD distillation loss = ',KD_history.history['distillation_loss'])
# loss_dataset['KD'] = KD_history.history['distillation_loss']
# accur_dataset['KD_test'] = KD_history.history['val_sparse_categorical_accuracy']
# accur_dataset['KD_train'] = KD_history.history['sparse_categorical_accuracy']

#%%
print('################## Train Student - No pre-training ####################')
print('####################### Knowledge Distillation 2 ########################')
KD_student2 = keras.models.clone_model(student_network)
KD_student2.set_weights(student_network.get_weights()) 
KD2 = Distiller2(KD_student2, teacher_network)

KD2.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           distillation_loss_fn = keras.losses.KLDivergence(),
           alpha = st_parameters['alpha'],
           temperature = st_parameters['temp'])

KD2_history = KD2.fit(trainX,
        trainy,
        batch_size = 64,
        epochs = st_parameters['epochs'],
        validation_data = (testX,testy),
        verbose = 0,
        )

print('KD2 test accuracy = ',KD2_history.history['val_sparse_categorical_accuracy'])
print('KD2 distillation loss = ',KD2_history.history['distillation_loss'])
loss_dataset['KD2'] = KD2_history.history['distillation_loss']
accur_dataset['KD2_test'] = KD2_history.history['val_sparse_categorical_accuracy']
accur_dataset['KD2_train'] = KD2_history.history['sparse_categorical_accuracy']
#%%
print('########################## Feature Learing ############################')
FL_student = keras.models.clone_model(student_network)
FL_student.set_weights(student_network.get_weights()) 
FL = feature_st(FL_student, teacher_network)

FL.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           features_loss_fn = keras.losses.MeanSquaredError(),
           beta = st_parameters['beta'],
           temperature = st_parameters['temp'])

FL_history = FL.fit(trainX,
        trainy,
        batch_size = 64,
        epochs = st_parameters['epochs'],
        validation_data = (testX,testy),
        verbose = 0,
        )

print('FL test accuracy = ',FL_history.history['val_sparse_categorical_accuracy'])
print('FL feature loss = ', FL_history.history['features_loss'])
accur_dataset['FL_test'] = FL_history.history['val_sparse_categorical_accuracy']
accur_dataset['FL_train'] = FL_history.history['sparse_categorical_accuracy']
#%%
print('############################## Baseline ###############################')
baseline_student = keras.models.clone_model(student_network)
baseline_student.set_weights(student_network.get_weights()) 
model = teacher_training(baseline_student)
model.compile(
        optimizer=keras.optimizers.Adam(lr = st_parameters['lr']),
        loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"],
    )
baseline_history = model.fit(trainX,
        trainy,
        batch_size = 64,
        epochs = st_parameters['epochs'],
        validation_data = (testX,testy),
        verbose = 0,
        )

print('Baseline test accuracy = ',baseline_history.history['val_sparse_categorical_accuracy'])
accur_dataset['base_test'] = baseline_history.history['val_sparse_categorical_accuracy']
accur_dataset['base_train'] = baseline_history.history['sparse_categorical_accuracy']

#%%
plt.figure()
#plt.plot(teacher_history.history['val_sparse_categorical_accuracy'], label = 'teacher')
#plt.plot(KD_history.history['val_sparse_categorical_accuracy'], label = 'KD')
plt.plot(KD2_history.history['val_sparse_categorical_accuracy'], label = 'KD2')
plt.plot(FL_history.history['val_sparse_categorical_accuracy'], label = 'FL')
plt.plot(baseline_history.history['val_sparse_categorical_accuracy'], label = 'baseline')
plt.legend()
plt.grid()
plt.ylim(0.68,0.78)
plt.title('Comparing KD and FL teacher student models on cifar')
plt.savefig('KD_FL_cifar_syclop_{:.0e}_{}.png'.format(st_parameters['alpha'],st_parameters['temp']))

#%%
print('################ Train Student - with pre-training ####################')
base_student = keras.models.clone_model(student_network)
base_student.set_weights(student_network.get_weights()) 
base_model = teacher_training(base_student)
base_model.compile(
        optimizer=keras.optimizers.Adam(lr = st_parameters['lr']),
        loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"],
    )
base_history = base_model.fit(trainX,
        trainy,
        batch_size = 64,
        epochs = st_parameters['student_fst_learning'],
        validation_data = (testX,testy),
        verbose = 0,
        )

print('Base test accuracy = ',base_history.history['val_sparse_categorical_accuracy'])

#%%
new_epochs = st_parameters['epochs'] - st_parameters['student_fst_learning']
if new_epochs < 1:
    new_epochs = 1
    print("ERROR - less than 1 epochs left after base learning!!!!!!!!")
print('############## Knowledge Distillation wt Pre-Trainiong ################')
KD_student_pt = keras.models.clone_model(base_student)
KD_student_pt.set_weights(base_student.get_weights()) 
KD_pt = Distiller(KD_student_pt, teacher_network)

KD_pt.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           distillation_loss_fn = keras.losses.KLDivergence(),
           alpha = st_parameters['alpha'],
           temperature = st_parameters['temp'])

#KD_pt.evaluate(testX,testy)
KD_pt_history = KD_pt.fit(trainX,
        trainy,
        batch_size = 64,
        epochs = new_epochs,
        validation_data = (testX,testy),
        verbose = 0,
        )

print('KD pt test accuracy = ',KD_pt_history.history['val_sparse_categorical_accuracy'])
print('KD pt distillation loss = ',KD_pt_history.history['distillation_loss'])
KD_pt_dist_loss = np.ones(len(KD2_history.history['distillation_loss']))*KD_pt_history.history['distillation_loss'][-1]
KD_pt_dist_loss[:len(KD_pt_history.history['distillation_loss'])] = KD_pt_history.history['distillation_loss']
loss_dataset['KD_pt'] = KD_pt_dist_loss
accur_dataset['KD_pt_test'] = base_history.history['val_sparse_categorical_accuracy'] + \
                                KD_pt_history.history['val_sparse_categorical_accuracy']
accur_dataset['KD_pt_train'] = base_history.history['sparse_categorical_accuracy'] + \
                                KD_pt_history.history['sparse_categorical_accuracy']
#%%

print('############## Knowledge Distillation 2 wt Pre-Trainiong ################')
KD2_student_pt = keras.models.clone_model(base_student)
KD2_student_pt.set_weights(base_student.get_weights()) 
KD2_pt = Distiller2(KD2_student_pt, teacher_network)

KD2_pt.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           distillation_loss_fn = keras.losses.KLDivergence(),
           alpha = st_parameters['alpha'],
           temperature = st_parameters['temp'])

#KD2_pt.evaluate(testX,testy)
KD2_pt_history = KD2_pt.fit(trainX,
        trainy,
        batch_size = 64,
        epochs = new_epochs,
        validation_data = (testX,testy),
        verbose = 0,
        )

print('KD2 pt test accuracy = ',KD2_pt_history.history['val_sparse_categorical_accuracy'])
print('KD2 pt distillation loss = ',KD2_pt_history.history['distillation_loss'])
KD2_pt_dist_loss = np.ones(len(KD2_history.history['distillation_loss']))*KD_pt_history.history['distillation_loss'][-1]
KD2_pt_dist_loss[:len(KD2_pt_history.history['distillation_loss'])] = KD2_pt_history.history['distillation_loss']
loss_dataset['KD2_pt'] = KD2_pt_dist_loss
accur_dataset['KD2_pt_test'] = base_history.history['val_sparse_categorical_accuracy'] + \
                                KD2_pt_history.history['val_sparse_categorical_accuracy']
accur_dataset['KD2_pt_train'] = base_history.history['sparse_categorical_accuracy'] + \
                                KD2_pt_history.history['sparse_categorical_accuracy']
#%%
print('########################## Feature Learing ############################')
FL_pt_student = keras.models.clone_model(base_student)
FL_pt_student.set_weights(base_student.get_weights()) 
FL_pt = feature_st(FL_pt_student, teacher_network)

FL_pt.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           features_loss_fn = keras.losses.MeanSquaredError(),
           beta = st_parameters['beta'],
           temperature = st_parameters['temp'])
#FL_pt.evaluate(testX,testy)
FL_pt_history = FL_pt.fit(trainX,
        trainy,
        batch_size = 64,
        epochs = new_epochs,
        validation_data = (testX,testy),
        verbose = 0,
        )


print('FL pt test accuracy = ',FL_pt_history.history['val_sparse_categorical_accuracy'])
print('FL pt feature loss = ', FL_pt_history.history['features_loss'])
accur_dataset['FL_pt_test'] = base_history.history['val_sparse_categorical_accuracy'] + \
                                FL_pt_history.history['val_sparse_categorical_accuracy']
accur_dataset['FL_pt_train'] = base_history.history['val_sparse_categorical_accuracy'] + \
                                FL_pt_history.history['sparse_categorical_accuracy']

accur_dataset.to_pickle('HR_KD_FL_pt_cifar_syclop_{:.0e}_{}_{:.0e}_{}.pkl'.format(st_parameters['alpha'],st_parameters['temp'], st_parameters['beta'],accur_dataset['KD2_test'][-1]))
#%%
plt.figure()
#plt.plot(teacher_history.history['val_sparse_categorical_accuracy'], label = 'teacher')
#plt.plot(KD_history.history['val_sparse_categorical_accuracy'], label = 'KD')
plt.plot(KD2_history.history['val_sparse_categorical_accuracy'], label = 'KD2')
plt.plot(FL_history.history['val_sparse_categorical_accuracy'], label = 'FL')
plt.plot(baseline_history.history['val_sparse_categorical_accuracy'], label = 'baseline')
plt.plot(accur_dataset['KD_pt_test'], label = 'KD_pt')
plt.plot(accur_dataset['KD2_pt_test'], label = 'KD2_pt')
plt.plot(accur_dataset['FL_pt_test'], label = 'FL_pt')
plt.legend()
plt.grid()
plt.ylim(0.68,0.78)
plt.title('Comparing KD and FL teacher student models on cifar')
plt.savefig('HR_KD_FL_pt_cifar_syclop_{:.0e}_{}_{:.0e}_{:.0e}.png'.format(st_parameters['alpha'],st_parameters['temp'], st_parameters['beta'],accur_dataset['KD2_test'][-1]))





