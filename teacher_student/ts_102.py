#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""

Comparing student teacher methods -  using Knowledge Disillation (KD) and Feature Learning (FL)

Using Keras and Cifar 10 with syclop. 

A pervious comperision was made to distill knowledge with pytorch on cifar HR
images - there we found that the KD outperformed FL.
Another comparision was made with MNIST and pytorch with syclop on LR images - 
there we found that FL outperformed KD - the opposite!


out.642289

lr = 1e-4, epochs = 500, pt = 50, alpha = 0.9, temp = 3 , beta = 0.95 
put.986226

lr = 1e-4, epochs = 500, pt = 300, alpha = 0.9, temp = 3 , beta = 0.95, out.5953
KD test accuracy =  [0.20659999549388885, 0.27000001072883606, 0.34200000762939453, 0.3718000054359436, 0.38420000672340393, 0.3822000026702881, 0.40400001406669617, 0.39719998836517334, 0.4251999855041504, 0.4352000057697296, 0.4343999922275543, 0.45879998803138733, 0.4339999854564667, 0.4496000111103058, 0.4684000015258789, 0.47620001435279846, 0.46779999136924744, 0.4708000123500824, 0.48019999265670776, 0.4708000123500824, 0.47859999537467957, 0.4805999994277954, 0.4607999920845032, 0.4973999857902527, 0.49939998984336853, 0.4936000108718872, 0.49939998984336853, 0.5088000297546387, 0.49480000138282776, 0.5138000249862671, 0.5174000263214111, 0.5130000114440918, 0.5202000141143799, 0.5266000032424927, 0.5297999978065491, 0.5248000025749207, 0.5317999720573425, 0.5270000100135803, 0.5375999808311462, 0.5351999998092651, 0.5343999862670898, 0.5162000060081482, 0.5389999747276306, 0.548799991607666, 0.5443999767303467, 0.5472000241279602, 0.5401999950408936, 0.5460000038146973, 0.521399974822998, 0.5483999848365784, 0.5386000275611877, 0.5253999829292297, 0.5529999732971191, 0.546999990940094, 0.5514000058174133, 0.5464000105857849, 0.5472000241279602, 0.5397999882698059, 0.5627999901771545, 0.5568000078201294, 0.5616000294685364, 0.5447999835014343, 0.5648000240325928, 0.553600013256073, 0.5479999780654907, 0.5645999908447266, 0.5523999929428101, 0.5633999705314636, 0.5672000050544739, 0.5550000071525574, 0.5594000220298767, 0.5598000288009644, 0.5608000159263611, 0.5622000098228455, 0.5666000247001648, 0.5663999915122986, 0.574400007724762, 0.5684000253677368, 0.550599992275238, 0.5663999915122986, 0.5551999807357788, 0.5763999819755554, 0.5785999894142151, 0.5703999996185303, 0.5612000226974487, 0.5687999725341797, 0.5666000247001648, 0.5672000050544739, 0.5752000212669373, 0.5752000212669373, 0.571399986743927, 0.5708000063896179, 0.5659999847412109, 0.5651999711990356, 0.5681999921798706, 0.5752000212669373, 0.5691999793052673, 0.5631999969482422, 0.576200008392334, 0.5756000280380249, 0.5587999820709229, 0.5781999826431274, 0.5802000164985657, 0.5663999915122986, 0.5723999738693237, 0.5788000226020813, 0.5735999941825867, 0.5708000063896179, 0.578000009059906, 0.5777999758720398, 0.5771999955177307, 0.5788000226020813, 0.5777999758720398, 0.5727999806404114, 0.574400007724762, 0.5781999826431274, 0.5771999955177307, 0.5766000151634216, 0.5708000063896179, 0.576200008392334, 0.5856000185012817, 0.5831999778747559, 0.5756000280380249, 0.569599986076355, 0.5843999981880188, 0.5717999935150146, 0.5807999968528748, 0.5702000260353088, 0.579200029373169, 0.5717999935150146, 0.5842000246047974, 0.58160001039505, 0.5942000150680542, 0.574400007724762, 0.5770000219345093, 0.5669999718666077, 0.5867999792098999, 0.5807999968528748, 0.5863999724388123, 0.5932000279426575, 0.5964000225067139, 0.5863999724388123, 0.5759999752044678, 0.5694000124931335, 0.5874000191688538, 0.5892000198364258, 0.5870000123977661, 0.5821999907493591, 0.578000009059906, 0.5888000130653381, 0.5932000279426575, 0.5820000171661377, 0.5861999988555908, 0.5789999961853027, 0.5838000178337097, 0.574400007724762, 0.5875999927520752, 0.5825999975204468, 0.5827999711036682, 0.5789999961853027, 0.569599986076355, 0.5861999988555908, 0.5884000062942505, 0.5830000042915344, 0.5907999873161316, 0.5745999813079834, 0.5892000198364258, 0.5795999765396118, 0.5856000185012817, 0.5753999948501587, 0.5848000049591064, 0.5861999988555908, 0.5889999866485596, 0.5863999724388123, 0.5839999914169312, 0.5820000171661377, 0.5875999927520752, 0.5813999772071838, 0.5893999934196472, 0.5741999745368958, 0.5788000226020813, 0.5928000211715698, 0.5830000042915344, 0.5928000211715698, 0.5812000036239624, 0.5928000211715698, 0.5825999975204468, 0.592199981212616, 0.5831999778747559, 0.5820000171661377, 0.5857999920845032, 0.5799999833106995, 0.6021999716758728, 0.5960000157356262, 0.58160001039505, 0.5835999846458435, 0.5910000205039978, 0.5803999900817871, 0.5878000259399414, 0.5812000036239624, 0.590399980545044, 0.5867999792098999, 0.5879999995231628, 0.5956000089645386, 0.5917999744415283, 0.5867999792098999, 0.5925999879837036, 0.576200008392334, 0.5884000062942505, 0.5893999934196472, 0.5830000042915344, 0.5812000036239624, 0.5997999906539917, 0.5885999798774719, 0.5839999914169312, 0.5898000001907349, 0.5866000056266785, 0.5916000008583069, 0.5902000069618225, 0.5932000279426575, 0.6001999974250793, 0.5842000246047974, 0.5910000205039978, 0.5983999967575073, 0.5902000069618225, 0.5979999899864197, 0.5938000082969666, 0.5917999744415283, 0.5928000211715698, 0.5924000144004822, 0.5874000191688538, 0.5965999960899353, 0.5917999744415283, 0.5952000021934509, 0.5953999757766724, 0.5924000144004822, 0.5916000008583069, 0.5938000082969666, 0.5774000287055969, 0.5925999879837036, 0.5992000102996826, 0.6015999913215637, 0.590399980545044, 0.6000000238418579, 0.604200005531311, 0.603600025177002, 0.5992000102996826, 0.6033999919891357, 0.5938000082969666, 0.5842000246047974, 0.6047999858856201, 0.6003999710083008, 0.592199981212616, 0.5911999940872192, 0.605400025844574, 0.5867999792098999, 0.603600025177002, 0.5942000150680542, 0.593999981880188, 0.5899999737739563, 0.5917999744415283, 0.5852000117301941, 0.5834000110626221, 0.5834000110626221, 0.5978000164031982, 0.5916000008583069, 0.5947999954223633, 0.5956000089645386, 0.5935999751091003, 0.600600004196167, 0.5928000211715698, 0.5898000001907349, 0.5878000259399414, 0.5888000130653381, 0.5916000008583069, 0.593999981880188, 0.5992000102996826, 0.5839999914169312, 0.6014000177383423, 0.5964000225067139, 0.5978000164031982, 0.593999981880188, 0.6000000238418579, 0.5982000231742859, 0.5974000096321106, 0.599399983882904, 0.5964000225067139, 0.5956000089645386, 0.6007999777793884, 0.5920000076293945, 0.5964000225067139, 0.6046000123023987, 0.6011999845504761, 0.5985999703407288, 0.5947999954223633, 0.6043999791145325, 0.5943999886512756, 0.5920000076293945, 0.6015999913215637, 0.6069999933242798, 0.6086000204086304, 0.6043999791145325, 0.6060000061988831, 0.5982000231742859, 0.5983999967575073, 0.5974000096321106, 0.6018000245094299, 0.6025999784469604, 0.5952000021934509, 0.6015999913215637, 0.600600004196167, 0.6069999933242798, 0.604200005531311, 0.6001999974250793, 0.5953999757766724, 0.5983999967575073, 0.5996000170707703, 0.5902000069618225, 0.5997999906539917, 0.6001999974250793, 0.6028000116348267, 0.5961999893188477, 0.6078000068664551, 0.6037999987602234, 0.6046000123023987, 0.5911999940872192, 0.5974000096321106, 0.5979999899864197, 0.5971999764442444, 0.5938000082969666, 0.5982000231742859, 0.5989999771118164, 0.598800003528595, 0.6018000245094299, 0.6065999865531921, 0.5942000150680542, 0.6014000177383423, 0.6025999784469604, 0.5978000164031982, 0.5997999906539917, 0.6018000245094299, 0.5992000102996826, 0.599399983882904, 0.5892000198364258, 0.5875999927520752, 0.604200005531311, 0.597000002861023, 0.5965999960899353, 0.5960000157356262, 0.6079999804496765, 0.5968000292778015, 0.5889999866485596, 0.593999981880188, 0.6003999710083008, 0.600600004196167, 0.6039999723434448, 0.5956000089645386, 0.5996000170707703, 0.579800009727478, 0.604200005531311, 0.5917999744415283, 0.6001999974250793, 0.6003999710083008, 0.5983999967575073, 0.6007999777793884, 0.6051999926567078, 0.600600004196167, 0.6014000177383423, 0.6078000068664551, 0.6019999980926514, 0.6046000123023987, 0.5964000225067139, 0.6007999777793884, 0.5968000292778015, 0.6032000184059143, 0.6007999777793884, 0.6037999987602234, 0.6033999919891357, 0.6069999933242798, 0.6001999974250793, 0.6033999919891357, 0.6018000245094299, 0.5920000076293945, 0.5960000157356262, 0.5964000225067139, 0.6061999797821045, 0.5997999906539917, 0.599399983882904, 0.6037999987602234, 0.6015999913215637, 0.6061999797821045, 0.598800003528595, 0.5974000096321106, 0.6021999716758728, 0.5929999947547913, 0.593999981880188, 0.6101999878883362, 0.6019999980926514, 0.6060000061988831, 0.6060000061988831, 0.5932000279426575, 0.5982000231742859, 0.5935999751091003, 0.5928000211715698, 0.6075999736785889, 0.6092000007629395, 0.6069999933242798, 0.6019999980926514, 0.5997999906539917, 0.598800003528595, 0.5982000231742859, 0.6074000000953674, 0.5924000144004822, 0.5910000205039978, 0.5960000157356262, 0.5989999771118164, 0.5997999906539917, 0.600600004196167, 0.5985999703407288, 0.5947999954223633, 0.6047999858856201, 0.6050000190734863, 0.5979999899864197, 0.6029999852180481, 0.5983999967575073, 0.6010000109672546, 0.597000002861023, 0.5942000150680542, 0.5928000211715698, 0.6014000177383423, 0.6047999858856201, 0.6037999987602234, 0.59579998254776, 0.6047999858856201, 0.6032000184059143, 0.600600004196167, 0.6082000136375427, 0.603600025177002, 0.5965999960899353, 0.6110000014305115, 0.5989999771118164, 0.6019999980926514, 0.6105999946594238, 0.6011999845504761, 0.5982000231742859, 0.5961999893188477, 0.5996000170707703, 0.6032000184059143, 0.5985999703407288, 0.6028000116348267, 0.597599983215332, 0.598800003528595, 0.609000027179718, 0.5983999967575073, 0.6015999913215637, 0.6007999777793884, 0.6078000068664551, 0.599399983882904, 0.6032000184059143, 0.6021999716758728, 0.5942000150680542, 0.6032000184059143, 0.5983999967575073, 0.603600025177002, 0.602400004863739, 0.6007999777793884, 0.6001999974250793, 0.6074000000953674, 0.6101999878883362, 0.5982000231742859, 0.5978000164031982, 0.6003999710083008, 0.6000000238418579, 0.5978000164031982, 0.6010000109672546, 0.6087999939918518, 0.6029999852180481, 0.5985999703407288, 0.6028000116348267, 0.6018000245094299, 0.6078000068664551, 0.6039999723434448, 0.6021999716758728, 0.5997999906539917, 0.6075999736785889, 0.5950000286102295, 0.602400004863739, 0.6029999852180481, 0.6028000116348267, 0.605400025844574, 0.6060000061988831, 0.5924000144004822, 0.6047999858856201, 0.6010000109672546, 0.597000002861023, 0.6051999926567078, 0.603600025177002, 0.6015999913215637, 0.6083999872207642, 0.5979999899864197]
FL test accuracy =  [0.24199999868869781, 0.29660001397132874, 0.3630000054836273, 0.38359999656677246, 0.39500001072883606, 0.4059999883174896, 0.4214000105857849, 0.4339999854564667, 0.43779999017715454, 0.4390000104904175, 0.44839999079704285, 0.46299999952316284, 0.45840001106262207, 0.4726000130176544, 0.4731999933719635, 0.48500001430511475, 0.47519999742507935, 0.47360000014305115, 0.49540001153945923, 0.4828000068664551, 0.48339998722076416, 0.49219998717308044, 0.5099999904632568, 0.5037999749183655, 0.49140000343322754, 0.5156000256538391, 0.5144000053405762, 0.5031999945640564, 0.5077999830245972, 0.5252000093460083, 0.5249999761581421, 0.5217999815940857, 0.5288000106811523, 0.5284000039100647, 0.5357999801635742, 0.5404000282287598, 0.5335999727249146, 0.5333999991416931, 0.5188000202178955, 0.5386000275611877, 0.5393999814987183, 0.5081999897956848, 0.5307999849319458, 0.5415999889373779, 0.5414000153541565, 0.5325999855995178, 0.5482000112533569, 0.5432000160217285, 0.5559999942779541, 0.5415999889373779, 0.5491999983787537, 0.5428000092506409, 0.5497999787330627, 0.545799970626831, 0.5544000267982483, 0.5491999983787537, 0.5537999868392944, 0.5475999712944031, 0.557200014591217, 0.551800012588501, 0.5526000261306763, 0.5522000193595886, 0.5604000091552734, 0.5633999705314636, 0.5649999976158142, 0.5562000274658203, 0.5558000206947327, 0.5658000111579895, 0.5669999718666077, 0.5587999820709229, 0.5630000233650208, 0.5532000064849854, 0.555400013923645, 0.5601999759674072, 0.5698000192642212, 0.5723999738693237, 0.5631999969482422, 0.5645999908447266, 0.579200029373169, 0.5735999941825867, 0.5690000057220459, 0.5709999799728394, 0.5619999766349792, 0.5720000267028809, 0.5655999779701233, 0.5655999779701233, 0.5407999753952026, 0.579200029373169, 0.5673999786376953, 0.5730000138282776, 0.5794000029563904, 0.5758000016212463, 0.5708000063896179, 0.578000009059906, 0.5889999866485596, 0.5753999948501587, 0.574999988079071, 0.5730000138282776, 0.5820000171661377, 0.5756000280380249, 0.5893999934196472, 0.5825999975204468, 0.5848000049591064, 0.5687999725341797, 0.5803999900817871, 0.5857999920845032, 0.5867999792098999, 0.5834000110626221, 0.5830000042915344, 0.5849999785423279, 0.5785999894142151, 0.5884000062942505, 0.5914000272750854, 0.5812000036239624, 0.5867999792098999, 0.5906000137329102, 0.5849999785423279, 0.5802000164985657, 0.599399983882904, 0.5899999737739563, 0.5907999873161316, 0.5866000056266785, 0.5911999940872192, 0.5856000185012817, 0.5935999751091003, 0.5881999731063843, 0.5946000218391418, 0.5839999914169312, 0.5968000292778015, 0.576200008392334, 0.5911999940872192, 0.5906000137329102, 0.5888000130653381, 0.5914000272750854, 0.5845999717712402, 0.5871999859809875, 0.5934000015258789, 0.5888000130653381, 0.5916000008583069, 0.5834000110626221, 0.58160001039505, 0.5965999960899353, 0.5839999914169312, 0.5870000123977661, 0.5916000008583069, 0.5953999757766724, 0.5867999792098999, 0.6003999710083008, 0.5985999703407288, 0.5974000096321106, 0.5964000225067139, 0.597000002861023, 0.6003999710083008, 0.5935999751091003, 0.59579998254776, 0.5843999981880188, 0.5806000232696533, 0.5881999731063843, 0.6069999933242798, 0.59579998254776, 0.5964000225067139, 0.5947999954223633, 0.5929999947547913, 0.5928000211715698, 0.600600004196167, 0.599399983882904, 0.5917999744415283, 0.5946000218391418, 0.6015999913215637, 0.5947999954223633, 0.5974000096321106, 0.603600025177002, 0.5892000198364258, 0.599399983882904, 0.6057999730110168, 0.5947999954223633, 0.5916000008583069, 0.5956000089645386, 0.6014000177383423, 0.5983999967575073, 0.5893999934196472, 0.5979999899864197, 0.5952000021934509, 0.59579998254776, 0.5961999893188477, 0.58160001039505, 0.597000002861023, 0.5916000008583069, 0.5925999879837036, 0.5971999764442444, 0.5953999757766724, 0.5961999893188477, 0.5934000015258789, 0.5946000218391418, 0.5960000157356262, 0.5982000231742859, 0.5961999893188477, 0.5928000211715698, 0.5881999731063843, 0.602400004863739, 0.5848000049591064, 0.5899999737739563, 0.602400004863739, 0.5956000089645386, 0.5866000056266785, 0.5925999879837036, 0.5942000150680542, 0.5892000198364258, 0.5916000008583069, 0.6001999974250793, 0.5968000292778015, 0.5952000021934509, 0.5985999703407288, 0.5914000272750854, 0.598800003528595, 0.5929999947547913, 0.5952000021934509, 0.5961999893188477, 0.5983999967575073, 0.5952000021934509, 0.6033999919891357, 0.5979999899864197, 0.5992000102996826, 0.6043999791145325, 0.5960000157356262, 0.5978000164031982, 0.6000000238418579, 0.6000000238418579, 0.6029999852180481, 0.5929999947547913, 0.590399980545044, 0.599399983882904, 0.5914000272750854, 0.5853999853134155, 0.5896000266075134, 0.5928000211715698, 0.5943999886512756, 0.5946000218391418, 0.6003999710083008, 0.6057999730110168, 0.5899999737739563, 0.6019999980926514, 0.5942000150680542, 0.599399983882904, 0.5971999764442444, 0.5968000292778015, 0.5992000102996826, 0.6007999777793884, 0.5950000286102295, 0.5943999886512756, 0.6014000177383423, 0.5978000164031982, 0.5985999703407288, 0.599399983882904, 0.6010000109672546, 0.5914000272750854, 0.598800003528595, 0.5916000008583069, 0.5932000279426575, 0.5982000231742859, 0.5943999886512756, 0.5979999899864197, 0.5989999771118164, 0.597000002861023, 0.5932000279426575, 0.600600004196167, 0.597000002861023, 0.6043999791145325, 0.593999981880188, 0.6018000245094299, 0.604200005531311, 0.5928000211715698, 0.5902000069618225, 0.592199981212616, 0.5910000205039978, 0.5982000231742859, 0.5902000069618225, 0.5920000076293945, 0.6043999791145325, 0.5950000286102295, 0.5893999934196472, 0.5920000076293945, 0.5932000279426575, 0.5924000144004822, 0.600600004196167, 0.5916000008583069, 0.5920000076293945, 0.5914000272750854, 0.5899999737739563, 0.5896000266075134, 0.5983999967575073, 0.5929999947547913, 0.5907999873161316, 0.5971999764442444, 0.5839999914169312, 0.5885999798774719, 0.604200005531311, 0.5842000246047974, 0.5907999873161316, 0.5827999711036682, 0.5929999947547913, 0.5879999995231628, 0.5968000292778015, 0.5983999967575073, 0.5875999927520752, 0.5889999866485596, 0.5857999920845032, 0.597000002861023, 0.5911999940872192, 0.5888000130653381, 0.5838000178337097, 0.5848000049591064, 0.5857999920845032, 0.5938000082969666, 0.5898000001907349, 0.5842000246047974, 0.5889999866485596, 0.5960000157356262, 0.5960000157356262, 0.6011999845504761, 0.59579998254776, 0.5892000198364258, 0.5947999954223633, 0.5952000021934509, 0.5934000015258789, 0.6064000129699707, 0.5802000164985657, 0.5920000076293945, 0.5979999899864197, 0.5853999853134155, 0.592199981212616, 0.6014000177383423, 0.5867999792098999, 0.59579998254776, 0.5938000082969666, 0.597599983215332, 0.5879999995231628, 0.590399980545044, 0.5898000001907349, 0.5878000259399414, 0.59579998254776, 0.5935999751091003, 0.5875999927520752, 0.5938000082969666, 0.5906000137329102, 0.5892000198364258, 0.6007999777793884, 0.5871999859809875, 0.5952000021934509, 0.5956000089645386, 0.5849999785423279, 0.5878000259399414, 0.5985999703407288, 0.5929999947547913, 0.5848000049591064, 0.5835999846458435, 0.5861999988555908, 0.592199981212616, 0.5956000089645386, 0.5935999751091003, 0.5867999792098999, 0.5898000001907349, 0.5867999792098999, 0.5849999785423279, 0.5917999744415283, 0.5863999724388123, 0.5910000205039978, 0.5874000191688538, 0.5863999724388123, 0.5835999846458435, 0.5928000211715698, 0.5911999940872192, 0.5968000292778015, 0.5835999846458435, 0.5870000123977661, 0.5889999866485596, 0.5807999968528748, 0.5953999757766724, 0.5856000185012817, 0.5896000266075134, 0.5910000205039978, 0.5899999737739563, 0.5914000272750854, 0.5950000286102295, 0.5857999920845032, 0.5867999792098999, 0.5752000212669373, 0.5896000266075134, 0.5928000211715698, 0.5896000266075134, 0.5899999737739563, 0.5827999711036682, 0.5835999846458435, 0.5867999792098999, 0.5834000110626221, 0.5902000069618225, 0.5870000123977661, 0.5875999927520752, 0.5898000001907349, 0.5907999873161316, 0.5857999920845032, 0.5781999826431274, 0.5884000062942505, 0.5774000287055969, 0.5785999894142151, 0.5807999968528748, 0.5820000171661377, 0.5795999765396118, 0.5884000062942505, 0.5845999717712402, 0.5785999894142151, 0.5794000029563904, 0.5842000246047974, 0.5860000252723694, 0.5821999907493591, 0.58160001039505, 0.5827999711036682, 0.5766000151634216, 0.5827999711036682, 0.5857999920845032, 0.5843999981880188, 0.5867999792098999, 0.5920000076293945, 0.5834000110626221, 0.5771999955177307, 0.5843999981880188, 0.576200008392334, 0.5821999907493591, 0.5820000171661377, 0.5843999981880188, 0.5842000246047974, 0.5842000246047974, 0.58160001039505, 0.5852000117301941, 0.578000009059906, 0.5812000036239624, 0.58160001039505, 0.5809999704360962, 0.5834000110626221, 0.5842000246047974, 0.5817999839782715, 0.5784000158309937, 0.5789999961853027, 0.5874000191688538, 0.5879999995231628, 0.5907999873161316, 0.5776000022888184, 0.5825999975204468, 0.5821999907493591, 0.5830000042915344, 0.5781999826431274, 0.5813999772071838, 0.5759999752044678, 0.5888000130653381, 0.574400007724762, 0.5884000062942505, 0.5838000178337097, 0.5881999731063843, 0.5799999833106995, 0.5813999772071838, 0.5838000178337097, 0.5785999894142151, 0.5722000002861023, 0.579200029373169, 0.5866000056266785, 0.5766000151634216, 0.5910000205039978, 0.5759999752044678, 0.5784000158309937, 0.5834000110626221, 0.5720000267028809, 0.579800009727478, 0.573199987411499, 0.5861999988555908, 0.5774000287055969, 0.5830000042915344, 0.5860000252723694, 0.5788000226020813, 0.5720000267028809, 0.5708000063896179, 0.5794000029563904, 0.5723999738693237, 0.5740000009536743, 0.5777999758720398, 0.5759999752044678, 0.5771999955177307, 0.5827999711036682, 0.578000009059906, 0.5803999900817871, 0.5717999935150146, 0.5834000110626221, 0.5807999968528748, 0.5799999833106995, 0.5820000171661377, 0.5809999704360962, 0.5763999819755554, 0.5763999819755554, 0.5785999894142151, 0.5709999799728394, 0.5795999765396118]
Baseline test accuracy =  [0.25760000944137573, 0.31940001249313354, 0.3490000069141388, 0.3790000081062317, 0.39419999718666077, 0.4092000126838684, 0.4065999984741211, 0.42899999022483826, 0.4397999942302704, 0.4246000051498413, 0.4447999894618988, 0.44440001249313354, 0.460999995470047, 0.4569999873638153, 0.47519999742507935, 0.453000009059906, 0.46239998936653137, 0.48179998993873596, 0.477400004863739, 0.4740000069141388, 0.4846000075340271, 0.4797999858856201, 0.4659999907016754, 0.4715999960899353, 0.4880000054836273, 0.4830000102519989, 0.5004000067710876, 0.5145999789237976, 0.5059999823570251, 0.5031999945640564, 0.5004000067710876, 0.5055999755859375, 0.49939998984336853, 0.5067999958992004, 0.5188000202178955, 0.5054000020027161, 0.5170000195503235, 0.5325999855995178, 0.5217999815940857, 0.5332000255584717, 0.5333999991416931, 0.5238000154495239, 0.5281999707221985, 0.49540001153945923, 0.5181999802589417, 0.5450000166893005, 0.5220000147819519, 0.5443999767303467, 0.5464000105857849, 0.5447999835014343, 0.5306000113487244, 0.5293999910354614, 0.5450000166893005, 0.5347999930381775, 0.5370000004768372, 0.5425999760627747, 0.5329999923706055, 0.5368000268936157, 0.5546000003814697, 0.5428000092506409, 0.5378000140190125, 0.5587999820709229, 0.5527999997138977, 0.54339998960495, 0.5475999712944031, 0.5559999942779541, 0.5541999936103821, 0.5436000227928162, 0.5473999977111816, 0.5655999779701233, 0.5347999930381775, 0.5522000193595886, 0.5699999928474426, 0.5303999781608582, 0.5564000010490417, 0.5425999760627747, 0.5540000200271606, 0.5514000058174133, 0.5564000010490417, 0.5641999840736389, 0.5659999847412109, 0.5598000288009644, 0.5321999788284302, 0.5681999921798706, 0.551800012588501, 0.5730000138282776, 0.5591999888420105, 0.5645999908447266, 0.5716000199317932, 0.5680000185966492, 0.5386000275611877, 0.5637999773025513, 0.5533999800682068, 0.5648000240325928, 0.5698000192642212, 0.5698000192642212, 0.5576000213623047, 0.5519999861717224, 0.557200014591217, 0.5699999928474426, 0.5622000098228455, 0.5627999901771545, 0.5753999948501587, 0.5738000273704529, 0.574999988079071, 0.5784000158309937, 0.5616000294685364, 0.5802000164985657, 0.5651999711990356, 0.5821999907493591, 0.5591999888420105, 0.5842000246047974, 0.5703999996185303, 0.5691999793052673, 0.5623999834060669, 0.5756000280380249, 0.5694000124931335, 0.5659999847412109, 0.5727999806404114, 0.5866000056266785, 0.5759999752044678, 0.5681999921798706, 0.5799999833106995, 0.5784000158309937, 0.5888000130653381, 0.571399986743927, 0.5705999732017517, 0.5813999772071838, 0.5781999826431274, 0.5726000070571899, 0.5845999717712402, 0.5827999711036682, 0.576200008392334, 0.5684000253677368, 0.5723999738693237, 0.574400007724762, 0.5776000022888184, 0.5785999894142151, 0.5794000029563904, 0.5845999717712402, 0.5702000260353088, 0.5785999894142151, 0.5827999711036682, 0.5809999704360962, 0.5806000232696533, 0.5735999941825867, 0.5770000219345093, 0.5794000029563904, 0.5771999955177307, 0.5691999793052673, 0.592199981212616, 0.5766000151634216, 0.5759999752044678, 0.5885999798774719, 0.5813999772071838, 0.5906000137329102, 0.5889999866485596, 0.5777999758720398, 0.5910000205039978, 0.5942000150680542, 0.5771999955177307, 0.5827999711036682, 0.5843999981880188, 0.5802000164985657, 0.5853999853134155, 0.5831999778747559, 0.5830000042915344, 0.5825999975204468, 0.5781999826431274, 0.5852000117301941, 0.5861999988555908, 0.5839999914169312, 0.5839999914169312, 0.5799999833106995, 0.5907999873161316, 0.5771999955177307, 0.5839999914169312, 0.579800009727478, 0.5748000144958496, 0.5867999792098999, 0.5879999995231628, 0.5889999866485596, 0.5648000240325928, 0.5907999873161316, 0.5924000144004822, 0.5809999704360962, 0.5911999940872192, 0.5763999819755554, 0.5856000185012817, 0.5812000036239624, 0.5916000008583069, 0.597000002861023, 0.5878000259399414, 0.5799999833106995, 0.5943999886512756, 0.5979999899864197, 0.58160001039505, 0.5920000076293945, 0.5827999711036682, 0.5845999717712402, 0.5820000171661377, 0.5889999866485596, 0.5835999846458435, 0.5803999900817871, 0.5821999907493591, 0.5767999887466431, 0.5752000212669373, 0.5893999934196472, 0.5838000178337097, 0.5861999988555908, 0.5893999934196472, 0.5781999826431274, 0.5888000130653381, 0.5914000272750854, 0.5863999724388123, 0.5920000076293945, 0.5906000137329102, 0.5968000292778015, 0.5845999717712402, 0.5892000198364258, 0.5802000164985657, 0.5860000252723694, 0.5902000069618225, 0.5879999995231628, 0.5860000252723694, 0.5705999732017517, 0.5920000076293945, 0.5834000110626221, 0.5950000286102295, 0.5831999778747559, 0.5861999988555908, 0.5863999724388123, 0.5709999799728394, 0.5759999752044678, 0.5763999819755554, 0.5867999792098999, 0.5996000170707703, 0.5896000266075134, 0.58160001039505, 0.5867999792098999, 0.5870000123977661, 0.5947999954223633, 0.5866000056266785, 0.5784000158309937, 0.5730000138282776, 0.5860000252723694, 0.5838000178337097, 0.5898000001907349, 0.5824000239372253, 0.590399980545044, 0.5842000246047974, 0.5881999731063843, 0.5726000070571899, 0.5807999968528748, 0.5738000273704529, 0.5806000232696533, 0.5896000266075134, 0.5881999731063843, 0.5842000246047974, 0.5907999873161316, 0.5879999995231628, 0.5867999792098999, 0.5860000252723694, 0.579200029373169, 0.5770000219345093, 0.5856000185012817, 0.590399980545044, 0.5785999894142151, 0.5813999772071838, 0.5950000286102295, 0.5863999724388123, 0.5857999920845032, 0.5852000117301941, 0.5866000056266785, 0.5902000069618225, 0.5960000157356262, 0.576200008392334, 0.5878000259399414, 0.5770000219345093, 0.5690000057220459, 0.5871999859809875, 0.5727999806404114, 0.5856000185012817, 0.5867999792098999, 0.592199981212616, 0.5916000008583069, 0.5838000178337097, 0.5848000049591064, 0.5893999934196472, 0.5878000259399414, 0.5938000082969666, 0.5799999833106995, 0.5825999975204468, 0.5845999717712402, 0.5843999981880188, 0.5788000226020813, 0.5906000137329102, 0.5899999737739563, 0.5856000185012817, 0.5892000198364258, 0.5758000016212463, 0.5838000178337097, 0.5821999907493591, 0.5848000049591064, 0.5825999975204468, 0.579800009727478, 0.5824000239372253, 0.5893999934196472, 0.5874000191688538, 0.5916000008583069, 0.5892000198364258, 0.5834000110626221, 0.5906000137329102, 0.579200029373169, 0.5866000056266785, 0.5712000131607056, 0.5853999853134155, 0.5871999859809875, 0.5756000280380249, 0.5848000049591064, 0.5835999846458435, 0.5817999839782715, 0.5821999907493591, 0.5843999981880188, 0.5870000123977661, 0.5722000002861023, 0.5788000226020813, 0.5748000144958496, 0.579800009727478, 0.5871999859809875, 0.5896000266075134, 0.5738000273704529, 0.5861999988555908, 0.5860000252723694, 0.5845999717712402, 0.5838000178337097, 0.5871999859809875, 0.5907999873161316, 0.5809999704360962, 0.5705999732017517, 0.5888000130653381, 0.5920000076293945, 0.579800009727478, 0.5843999981880188, 0.5838000178337097, 0.5888000130653381, 0.58160001039505, 0.5929999947547913, 0.5834000110626221, 0.5889999866485596, 0.5799999833106995, 0.5831999778747559, 0.5892000198364258, 0.5848000049591064, 0.5928000211715698, 0.5866000056266785, 0.5870000123977661, 0.5914000272750854, 0.5860000252723694, 0.5835999846458435, 0.5795999765396118, 0.569599986076355, 0.5813999772071838, 0.5794000029563904, 0.5879999995231628, 0.5899999737739563, 0.5878000259399414, 0.5770000219345093, 0.5853999853134155, 0.5803999900817871, 0.5774000287055969, 0.5824000239372253, 0.5848000049591064, 0.5875999927520752, 0.5852000117301941, 0.5845999717712402, 0.5788000226020813, 0.5893999934196472, 0.5874000191688538, 0.5785999894142151, 0.5740000009536743, 0.5875999927520752, 0.5881999731063843, 0.5834000110626221, 0.5759999752044678, 0.5723999738693237, 0.5848000049591064, 0.579800009727478, 0.5884000062942505, 0.5852000117301941, 0.5849999785423279, 0.5817999839782715, 0.5824000239372253, 0.5853999853134155, 0.5759999752044678, 0.579800009727478, 0.5896000266075134, 0.5843999981880188, 0.5838000178337097, 0.5807999968528748, 0.5856000185012817, 0.5789999961853027, 0.5861999988555908, 0.5803999900817871, 0.5803999900817871, 0.5802000164985657, 0.5809999704360962, 0.5892000198364258, 0.5741999745368958, 0.5824000239372253, 0.5821999907493591, 0.5717999935150146, 0.5812000036239624, 0.5860000252723694, 0.5788000226020813, 0.5789999961853027, 0.5803999900817871, 0.5830000042915344, 0.5740000009536743, 0.5741999745368958, 0.5849999785423279, 0.5771999955177307, 0.5774000287055969, 0.5821999907493591, 0.5834000110626221, 0.5843999981880188, 0.5879999995231628, 0.5777999758720398, 0.5871999859809875, 0.579800009727478, 0.5820000171661377, 0.5712000131607056, 0.5813999772071838, 0.5807999968528748, 0.5820000171661377, 0.5784000158309937, 0.5824000239372253, 0.5838000178337097, 0.5824000239372253, 0.5807999968528748, 0.5821999907493591, 0.5753999948501587, 0.5843999981880188, 0.579200029373169, 0.5812000036239624, 0.5856000185012817, 0.5745999813079834, 0.5809999704360962, 0.5843999981880188, 0.5803999900817871, 0.5759999752044678, 0.5756000280380249, 0.5839999914169312, 0.5807999968528748, 0.5789999961853027, 0.5699999928474426, 0.574999988079071, 0.5727999806404114, 0.5735999941825867, 0.573199987411499, 0.5795999765396118, 0.5766000151634216, 0.5722000002861023, 0.5758000016212463, 0.5794000029563904, 0.5875999927520752, 0.579200029373169, 0.5788000226020813, 0.5759999752044678, 0.5753999948501587, 0.579200029373169, 0.5785999894142151, 0.5759999752044678, 0.5789999961853027, 0.5763999819755554, 0.5691999793052673, 0.5812000036239624, 0.5820000171661377, 0.574999988079071, 0.5745999813079834, 0.5788000226020813, 0.5843999981880188, 0.5767999887466431, 0.5807999968528748, 0.5741999745368958, 0.5788000226020813, 0.578000009059906, 0.5777999758720398, 0.5759999752044678, 0.5806000232696533, 0.5753999948501587, 0.5812000036239624, 0.5672000050544739, 0.5734000205993652, 0.5730000138282776, 0.5767999887466431, 0.5720000267028809, 0.579800009727478, 0.5771999955177307, 0.5784000158309937]
Base test accuracy =  [0.22939999401569366, 0.3041999936103821, 0.3668000102043152, 0.3935999870300293, 0.39419999718666077, 0.40799999237060547, 0.43959999084472656, 0.42579999566078186, 0.42179998755455017, 0.45019999146461487, 0.45719999074935913, 0.46160000562667847, 0.4668000042438507, 0.45579999685287476, 0.4569999873638153, 0.4797999858856201, 0.4796000123023987, 0.498199999332428, 0.49399998784065247, 0.49720001220703125, 0.5, 0.48899999260902405, 0.503600001335144, 0.4909999966621399, 0.5135999917984009, 0.5148000121116638, 0.5131999850273132, 0.5139999985694885, 0.5103999972343445, 0.5067999958992004, 0.527999997138977, 0.5175999999046326, 0.5303999781608582, 0.5248000025749207, 0.5351999998092651, 0.5135999917984009, 0.5440000295639038, 0.5410000085830688, 0.5329999923706055, 0.534600019454956, 0.5364000201225281, 0.5321999788284302, 0.545799970626831, 0.5306000113487244, 0.5342000126838684, 0.531000018119812, 0.5443999767303467, 0.5478000044822693, 0.5580000281333923, 0.5562000274658203, 0.5562000274658203, 0.5662000179290771, 0.5347999930381775, 0.5577999949455261, 0.5604000091552734, 0.5598000288009644, 0.5501999855041504, 0.5651999711990356, 0.5565999746322632, 0.5659999847412109, 0.5594000220298767, 0.5630000233650208, 0.5680000185966492, 0.569599986076355, 0.557200014591217, 0.5613999962806702, 0.5541999936103821, 0.5708000063896179, 0.5727999806404114, 0.5698000192642212, 0.5667999982833862, 0.5748000144958496, 0.5789999961853027, 0.5752000212669373, 0.5821999907493591, 0.5730000138282776, 0.5777999758720398, 0.5598000288009644, 0.579800009727478, 0.5690000057220459, 0.5680000185966492, 0.5817999839782715, 0.5777999758720398, 0.5852000117301941, 0.5812000036239624, 0.5758000016212463, 0.5771999955177307, 0.5825999975204468, 0.5802000164985657, 0.579200029373169, 0.5795999765396118, 0.5917999744415283, 0.5767999887466431, 0.5781999826431274, 0.5857999920845032, 0.5748000144958496, 0.5785999894142151, 0.5781999826431274, 0.5795999765396118, 0.5852000117301941, 0.5825999975204468, 0.5835999846458435, 0.5848000049591064, 0.5794000029563904, 0.5795999765396118, 0.5857999920845032, 0.5950000286102295, 0.5860000252723694, 0.593999981880188, 0.5867999792098999, 0.5866000056266785, 0.5893999934196472, 0.5874000191688538, 0.5906000137329102, 0.5867999792098999, 0.5898000001907349, 0.5932000279426575, 0.5867999792098999, 0.5884000062942505, 0.5835999846458435, 0.592199981212616, 0.5763999819755554, 0.597000002861023, 0.5852000117301941, 0.5910000205039978, 0.5861999988555908, 0.5929999947547913, 0.5879999995231628, 0.5934000015258789, 0.5899999737739563, 0.5960000157356262, 0.5899999737739563, 0.5896000266075134, 0.5860000252723694, 0.5884000062942505, 0.6032000184059143, 0.5989999771118164, 0.5898000001907349, 0.5916000008583069, 0.5916000008583069, 0.597599983215332, 0.5821999907493591, 0.5964000225067139, 0.599399983882904, 0.597599983215332, 0.5924000144004822, 0.5925999879837036, 0.6003999710083008, 0.5935999751091003, 0.5884000062942505, 0.5985999703407288, 0.5888000130653381, 0.5838000178337097, 0.59579998254776, 0.5911999940872192, 0.6051999926567078, 0.5953999757766724, 0.5947999954223633, 0.5953999757766724, 0.5978000164031982, 0.6001999974250793, 0.6033999919891357, 0.5885999798774719, 0.6037999987602234, 0.5956000089645386, 0.5952000021934509, 0.6055999994277954, 0.5985999703407288, 0.5996000170707703, 0.6032000184059143, 0.5983999967575073, 0.5985999703407288, 0.5989999771118164, 0.5813999772071838, 0.5950000286102295, 0.5938000082969666, 0.5935999751091003, 0.6061999797821045, 0.5953999757766724, 0.597000002861023, 0.5974000096321106, 0.5989999771118164, 0.6010000109672546, 0.6014000177383423, 0.6007999777793884, 0.6050000190734863, 0.6003999710083008, 0.5830000042915344, 0.5950000286102295, 0.597599983215332, 0.5892000198364258, 0.5952000021934509, 0.6021999716758728, 0.6037999987602234, 0.5979999899864197, 0.5953999757766724, 0.5947999954223633, 0.6000000238418579, 0.6021999716758728, 0.6032000184059143, 0.6033999919891357, 0.6007999777793884, 0.5964000225067139, 0.603600025177002, 0.5968000292778015, 0.593999981880188, 0.599399983882904, 0.5989999771118164, 0.6047999858856201, 0.6032000184059143, 0.590399980545044, 0.6046000123023987, 0.602400004863739, 0.6010000109672546, 0.599399983882904, 0.5971999764442444, 0.6029999852180481, 0.5985999703407288, 0.5956000089645386, 0.6011999845504761, 0.6075999736785889, 0.6001999974250793, 0.5953999757766724, 0.5952000021934509, 0.603600025177002, 0.599399983882904, 0.5992000102996826, 0.6060000061988831, 0.602400004863739, 0.5965999960899353, 0.5953999757766724, 0.6014000177383423, 0.6043999791145325, 0.5983999967575073, 0.5932000279426575, 0.5971999764442444, 0.6078000068664551, 0.6029999852180481, 0.6047999858856201, 0.5965999960899353, 0.6025999784469604, 0.6075999736785889, 0.6043999791145325, 0.5982000231742859, 0.6050000190734863, 0.6021999716758728, 0.6051999926567078, 0.6061999797821045, 0.6014000177383423, 0.6000000238418579, 0.6055999994277954, 0.597599983215332, 0.6028000116348267, 0.5863999724388123, 0.59579998254776, 0.5893999934196472, 0.5974000096321106, 0.599399983882904, 0.6018000245094299, 0.5910000205039978, 0.5953999757766724, 0.599399983882904, 0.5964000225067139, 0.5899999737739563, 0.6025999784469604, 0.593999981880188, 0.5950000286102295, 0.5935999751091003, 0.5968000292778015, 0.5925999879837036, 0.6033999919891357, 0.5982000231742859, 0.6015999913215637, 0.5914000272750854, 0.6043999791145325, 0.5916000008583069, 0.5916000008583069, 0.5989999771118164, 0.5929999947547913, 0.5920000076293945, 0.5996000170707703, 0.5961999893188477, 0.6001999974250793, 0.600600004196167, 0.5953999757766724, 0.5920000076293945, 0.592199981212616, 0.5947999954223633, 0.6001999974250793, 0.593999981880188, 0.5979999899864197, 0.5961999893188477, 0.5956000089645386, 0.597000002861023, 0.597000002861023, 0.5943999886512756, 0.5911999940872192, 0.5871999859809875, 0.5974000096321106, 0.5942000150680542]
KD pt test accuracy =  [0.6065999865531921, 0.5938000082969666, 0.5979999899864197, 0.6078000068664551, 0.6051999926567078, 0.6018000245094299, 0.6021999716758728, 0.5997999906539917, 0.6003999710083008, 0.6079999804496765, 0.6019999980926514, 0.5934000015258789, 0.598800003528595, 0.607200026512146, 0.6075999736785889, 0.6043999791145325, 0.6078000068664551, 0.6092000007629395, 0.6039999723434448, 0.6015999913215637, 0.6079999804496765, 0.605400025844574, 0.6010000109672546, 0.6046000123023987, 0.609000027179718, 0.6043999791145325, 0.6069999933242798, 0.6055999994277954, 0.609000027179718, 0.604200005531311, 0.609000027179718, 0.6126000285148621, 0.5996000170707703, 0.6000000238418579, 0.5961999893188477, 0.600600004196167, 0.6051999926567078, 0.6046000123023987, 0.605400025844574, 0.6100000143051147, 0.6104000210762024, 0.603600025177002, 0.6047999858856201, 0.6100000143051147, 0.6050000190734863, 0.6018000245094299, 0.6068000197410583, 0.6055999994277954, 0.6082000136375427, 0.6083999872207642, 0.6101999878883362, 0.6110000014305115, 0.6075999736785889, 0.6051999926567078, 0.6101999878883362, 0.61080002784729, 0.6055999994277954, 0.6104000210762024, 0.6050000190734863, 0.6093999743461609, 0.6096000075340271, 0.6065999865531921, 0.605400025844574, 0.6118000149726868, 0.6028000116348267, 0.5989999771118164, 0.6019999980926514, 0.6057999730110168, 0.607200026512146, 0.6105999946594238, 0.6101999878883362, 0.6087999939918518, 0.6101999878883362, 0.6083999872207642, 0.6110000014305115, 0.6118000149726868, 0.6104000210762024, 0.5983999967575073, 0.598800003528595, 0.6161999702453613, 0.6075999736785889, 0.6082000136375427, 0.6078000068664551, 0.6119999885559082, 0.6074000000953674, 0.6060000061988831, 0.6032000184059143, 0.6064000129699707, 0.6087999939918518, 0.6075999736785889, 0.6141999959945679, 0.6079999804496765, 0.6010000109672546, 0.6074000000953674, 0.6079999804496765, 0.6033999919891357, 0.6079999804496765, 0.6060000061988831, 0.6007999777793884, 0.6161999702453613, 0.6093999743461609, 0.6033999919891357, 0.609000027179718, 0.6093999743461609, 0.6110000014305115, 0.6065999865531921, 0.6047999858856201, 0.6033999919891357, 0.6046000123023987, 0.604200005531311, 0.6082000136375427, 0.6100000143051147, 0.6078000068664551, 0.6133999824523926, 0.6092000007629395, 0.6068000197410583, 0.6122000217437744, 0.6105999946594238, 0.6069999933242798, 0.6074000000953674, 0.609000027179718, 0.6105999946594238, 0.605400025844574, 0.6114000082015991, 0.6083999872207642, 0.6111999750137329, 0.6010000109672546, 0.6078000068664551, 0.6007999777793884, 0.6057999730110168, 0.609000027179718, 0.6065999865531921, 0.6082000136375427, 0.6057999730110168, 0.6158000230789185, 0.6105999946594238, 0.6146000027656555, 0.6051999926567078, 0.6060000061988831, 0.6029999852180481, 0.6097999811172485, 0.6097999811172485, 0.6043999791145325, 0.6055999994277954, 0.6104000210762024, 0.609000027179718, 0.6069999933242798, 0.6060000061988831, 0.607200026512146, 0.6064000129699707, 0.6069999933242798, 0.6082000136375427, 0.6032000184059143, 0.6087999939918518, 0.6082000136375427, 0.5965999960899353, 0.6068000197410583, 0.607200026512146, 0.6105999946594238, 0.6097999811172485, 0.6046000123023987, 0.6061999797821045, 0.6037999987602234, 0.6065999865531921, 0.6060000061988831, 0.6047999858856201, 0.6075999736785889, 0.6025999784469604, 0.6011999845504761, 0.6057999730110168, 0.6068000197410583, 0.6096000075340271, 0.6100000143051147, 0.6019999980926514, 0.6055999994277954, 0.6065999865531921, 0.6078000068664551, 0.6078000068664551, 0.6074000000953674, 0.6082000136375427, 0.6104000210762024, 0.6065999865531921, 0.605400025844574, 0.6078000068664551, 0.6101999878883362, 0.6029999852180481, 0.6061999797821045, 0.6057999730110168, 0.6046000123023987, 0.6061999797821045, 0.6078000068664551, 0.6065999865531921, 0.6047999858856201, 0.605400025844574, 0.6087999939918518, 0.609000027179718, 0.6097999811172485, 0.6057999730110168, 0.6050000190734863, 0.6025999784469604]
FL pt test accuracy =  [0.5892000198364258, 0.5964000225067139, 0.5985999703407288, 0.5910000205039978, 0.5902000069618225, 0.5914000272750854, 0.592199981212616, 0.5961999893188477, 0.5938000082969666, 0.5911999940872192, 0.5964000225067139, 0.5997999906539917, 0.5956000089645386, 0.5906000137329102, 0.5952000021934509, 0.5934000015258789, 0.5997999906539917, 0.5935999751091003, 0.5910000205039978, 0.5952000021934509, 0.5896000266075134, 0.5929999947547913, 0.5896000266075134, 0.5934000015258789, 0.5932000279426575, 0.5943999886512756, 0.5914000272750854, 0.5953999757766724, 0.5871999859809875, 0.5911999940872192, 0.592199981212616, 0.5856000185012817, 0.5902000069618225, 0.5885999798774719, 0.5879999995231628, 0.5935999751091003, 0.5870000123977661, 0.5929999947547913, 0.5896000266075134, 0.5938000082969666, 0.5884000062942505, 0.5917999744415283, 0.5875999927520752, 0.5935999751091003, 0.5899999737739563, 0.5898000001907349, 0.5929999947547913, 0.5914000272750854, 0.590399980545044, 0.5950000286102295, 0.5893999934196472, 0.5902000069618225, 0.5821999907493591, 0.5938000082969666, 0.5932000279426575, 0.5968000292778015, 0.5889999866485596, 0.5843999981880188, 0.5902000069618225, 0.5953999757766724, 0.5920000076293945, 0.5952000021934509, 0.5817999839782715, 0.5839999914169312, 0.5863999724388123, 0.5848000049591064, 0.5835999846458435, 0.5952000021934509, 0.5885999798774719, 0.5932000279426575, 0.5884000062942505, 0.5867999792098999, 0.5914000272750854, 0.5943999886512756, 0.5893999934196472, 0.5885999798774719, 0.5861999988555908, 0.5932000279426575, 0.5910000205039978, 0.58160001039505, 0.5896000266075134, 0.5839999914169312, 0.5813999772071838, 0.5874000191688538, 0.5884000062942505, 0.5885999798774719, 0.5843999981880188, 0.590399980545044, 0.5914000272750854, 0.5871999859809875, 0.5917999744415283, 0.5910000205039978, 0.5935999751091003, 0.5870000123977661, 0.5813999772071838, 0.5827999711036682, 0.5835999846458435, 0.5838000178337097, 0.5853999853134155, 0.5898000001907349, 0.5874000191688538, 0.5875999927520752, 0.5942000150680542, 0.5889999866485596, 0.5893999934196472, 0.5835999846458435, 0.5867999792098999, 0.5853999853134155, 0.5817999839782715, 0.5888000130653381, 0.5863999724388123, 0.5812000036239624, 0.5802000164985657, 0.5838000178337097, 0.5853999853134155, 0.5856000185012817, 0.5907999873161316, 0.5934000015258789, 0.5911999940872192, 0.5911999940872192, 0.5892000198364258, 0.5834000110626221, 0.5838000178337097, 0.5875999927520752, 0.5853999853134155, 0.5911999940872192, 0.5799999833106995, 0.5874000191688538, 0.5888000130653381, 0.5899999737739563, 0.5881999731063843, 0.5825999975204468, 0.5825999975204468, 0.5770000219345093, 0.5867999792098999, 0.5842000246047974, 0.5856000185012817, 0.5827999711036682, 0.5920000076293945, 0.5893999934196472, 0.5888000130653381, 0.5874000191688538, 0.5824000239372253, 0.5830000042915344, 0.58160001039505, 0.5866000056266785, 0.5896000266075134, 0.5852000117301941, 0.5845999717712402, 0.5845999717712402, 0.5807999968528748, 0.5853999853134155, 0.5794000029563904, 0.5803999900817871, 0.5896000266075134, 0.5838000178337097, 0.5794000029563904, 0.5860000252723694, 0.5857999920845032, 0.5860000252723694, 0.5781999826431274, 0.5853999853134155, 0.5806000232696533, 0.5803999900817871, 0.5845999717712402, 0.5848000049591064, 0.5825999975204468, 0.5852000117301941, 0.5856000185012817, 0.5831999778747559, 0.5845999717712402, 0.5838000178337097, 0.5820000171661377, 0.5756000280380249, 0.5788000226020813, 0.5807999968528748, 0.5824000239372253, 0.5834000110626221, 0.5824000239372253, 0.5853999853134155, 0.5860000252723694, 0.5812000036239624, 0.5789999961853027, 0.5875999927520752, 0.5794000029563904, 0.5857999920845032, 0.5835999846458435, 0.5776000022888184, 0.5842000246047974, 0.5809999704360962, 0.5834000110626221, 0.5807999968528748, 0.5807999968528748, 0.5776000022888184, 0.5820000171661377, 0.574400007724762, 0.5776000022888184, 0.5825999975204468, 0.5794000029563904, 0.5839999914169312]

put.986227
lr = 5e-4, epochs = 500, pt = 300, alpha = 0.9, temp = 3 , beta = 0.95 

###### corrected mistakes in feature function
epochs = 50 out.27425
epochs = 100 out.27421


"""
import sys
sys.path.insert(1, '/home/orram/Documents/GitHub/imagewalker')
sys.path.insert(1, '/home/labs/ahissarlab/orra/imagewalker')
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
# from tensorflow.compat.v1 import ConfigProto
# from tensorflow.compat.v1 import InteractiveSession

# config = ConfigProto()
# config.gpu_options.allow_growth = True
# session = InteractiveSession(config=config)
import numpy as np

import cv2
import misc
import pandas as pd
import matplotlib.pyplot as plt
import pickle

from keras_utils import *
from misc import *

import tensorflow.keras as keras
import tensorflow as tf

from tensorflow.keras.datasets import cifar10

# load dataset
(trainX, trainy), (testX, testy) = cifar10.load_data()
images, labels = trainX, trainy
#%%
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm

trainX, testX = prep_pixels(trainX, testX)
#%%
#Define function for low resolution lens on syclop
def bad_res101(img,res):
    sh=np.shape(img)
    dwnsmp=cv2.resize(img,res, interpolation = cv2.INTER_CUBIC)
    upsmp = cv2.resize(dwnsmp,sh[:2], interpolation = cv2.INTER_CUBIC)
    return upsmp

def bad_res102(img,res):
    sh=np.shape(img)
    dwnsmp=cv2.resize(img,res, interpolation = cv2.INTER_AREA)
    return dwnsmp

# import importlib
# importlib.reload(misc)
# from misc import Logger
import os 
import sys

######################## Network Parameters ##################################
if len(sys.argv) == 1:
    st_parameters = {
    'lr' : 1,#float(sys.argv[1]),
    'epochs' : 5,#int(sys.argv[2]),
    "student_fst_learning" : 1,#int(sys.argv[3]), #The first learning stage of the student - number of epochs
    'alpha': 0.5,#float(sys.argv[4]), #KD weights
    'temp' : 12,#int(sys.argv[5]),   #KD weights
    'beta' : 0.9#float(sys.argv[6]), #features st weights
    
    }
else:
    st_parameters = {
    'lr' : float(sys.argv[1]),
    'epochs' : int(sys.argv[2]),
    "student_fst_learning" : int(sys.argv[3]), #The first learning stage of the student - number of epochs
    'alpha': float(sys.argv[4]), #KD weights
    'temp' : int(sys.argv[5]),   #KD weights
    'beta' : float(sys.argv[6]), #features st weights
    
    }

print('Run Parameters:', st_parameters)
print('Run Parameters:', st_parameters)

class teacher_training(keras.Model):
    def __init__(self,teacher):
        super(teacher_training, self).__init__()
        self.teacher = teacher


    def compile(
        self,
        optimizer,
        metrics,
        loss_fn,

    ):
        """ Configure the distiller.

        Args:
            optimizer: Keras optimizer for the student weights
            metrics: Keras metrics for evaluation
            student_loss_fn: Loss function of difference between student
                predictions and ground-truth
            distillation_loss_fn: Loss function of difference between soft
                student predictions and soft teacher predictions
            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn
            temperature: Temperature for softening probability distributions.
                Larger temperature gives softer distributions.
        """
        super(teacher_training, self).compile(optimizer=optimizer, metrics=metrics)
        self.loss_fn = loss_fn


    def train_step(self, data):
        # Unpack data
        HR_data , y = data

        with tf.GradientTape() as tape:
            # Forward pass of student
            features, predictions = self.teacher(HR_data, training=True)

            # Compute losses
            loss = self.loss_fn(y, predictions)

        # Compute gradients
        trainable_vars = self.teacher.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Update the metrics configured in `compile()`.
        self.compiled_metrics.update_state(y, predictions)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}

        return results

    def test_step(self, data):
        # Unpack the data
        x, y = data

        # Compute predictions
        features, y_prediction = self.teacher(x, training=False)

        # Calculate the loss
        loss = self.loss_fn(y, y_prediction)

        # Update the metrics.
        self.compiled_metrics.update_state(y, y_prediction)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}

        return results
    
    def call(self, data, training = False):
        x = data
        features, prediction = self.teacher(x, training = training)
        return features, prediction 


class Distiller(keras.Model):
    def __init__(self, student, teacher):
        super(Distiller, self).__init__()
        self.teacher = teacher
        self.student = student

    def compile(
        self,
        optimizer,
        metrics,
        student_loss_fn,
        distillation_loss_fn,
        alpha=0.1,
        temperature=3,
    ):
        """ Configure the distiller.

        Args:
            optimizer: Keras optimizer for the student weights
            metrics: Keras metrics for evaluation
            student_loss_fn: Loss function of difference between student
                predictions and ground-truth
            distillation_loss_fn: Loss function of difference between soft
                student predictions and soft teacher predictions
            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn
            temperature: Temperature for softening probability distributions.
                Larger temperature gives softer distributions.
        """
        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)
        self.student_loss_fn = student_loss_fn
        self.distillation_loss_fn = distillation_loss_fn
        self.alpha = alpha
        self.temperature = temperature

    def train_step(self, data):
        # Unpack data
        [syclop_data, HR_data] , y = data
        # Forward pass of teacher
        
        teacher_features, teacher_predictions = self.teacher.call(HR_data, training=False)
        
        with tf.GradientTape() as tape:
            # Forward pass of student
            student_features, student_predictions = self.student(syclop_data, training=True)

            # Compute losses
            student_loss = self.student_loss_fn(y, student_predictions)
            distillation_loss = self.distillation_loss_fn(
                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),
                tf.nn.softmax(student_predictions / self.temperature, axis=1),
            )
            loss = (1 - self.alpha) * student_loss + self.alpha * distillation_loss

        # Compute gradients
        trainable_vars = self.student.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Update the metrics configured in `compile()`.
        self.compiled_metrics.update_state(y, student_predictions)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update(
            {"student_loss": student_loss, "distillation_loss": distillation_loss}
        )
        return results

    def test_step(self, data):
        # Unpack the data
        x, y = data

        # Compute predictions
        student_features, y_prediction = self.student(x, training=False)

        # Calculate the loss
        student_loss = self.student_loss_fn(y, y_prediction)

        # Update the metrics.
        self.compiled_metrics.update_state(y, y_prediction)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update({"student_loss": student_loss})
        return results
    
class feature_st(keras.Model):
    def __init__(self, student, teacher):
        super(feature_st, self).__init__()
        self.teacher = teacher
        self.student = student

    def compile(
        self,
        optimizer,
        metrics,
        student_loss_fn,
        features_loss_fn,
        beta=0.1,
        temperature=3,
    ):
        """ Configure the distiller.

        Args:
            optimizer: Keras optimizer for the student weights
            metrics: Keras metrics for evaluation
            student_loss_fn: Loss function of difference between student
                predictions and ground-truth
            distillation_loss_fn: Loss function of difference between soft
                student predictions and soft teacher predictions
            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn
            temperature: Temperature for softening probability distributions.
                Larger temperature gives softer distributions.
        """
        super(feature_st, self).compile(optimizer=optimizer, metrics=metrics)
        self.student_loss_fn = student_loss_fn
        self.features_loss_fn = features_loss_fn
        self.beta = beta
        self.temperature = temperature

    def train_step(self, data):
        # Unpack data
        [syclop_data, HR_data] , y = data
        # Forward pass of teacher
        teacher_features, teacher_predictions = self.teacher(HR_data, training=False)
        # layer_name = 'teacher_features'
        # intermediate_layer_model = keras.Model(inputs=model.input,
        #                                outputs=model.get_layer(layer_name).output)
        # intermediate_output = intermediate_layer_model(data)
        with tf.GradientTape() as tape:
            # Forward pass of student
            student_features, student_predictions = self.student(syclop_data, training=True)

            # Compute losses
            student_loss = self.student_loss_fn(y, student_predictions)
            features_loss = self.features_loss_fn(
                teacher_features,
                student_features,
            )
            loss = (1-self.beta) * student_loss + (self.beta) * features_loss

        # Compute gradients
        trainable_vars = self.student.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Update the metrics configured in `compile()`.
        self.compiled_metrics.update_state(y, student_predictions)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update(
            {"student_loss": student_loss, "features_loss": features_loss}
        )
        return results

    def test_step(self, data):
        # Unpack the data
        x, y = data

        # Compute predictions
        _, y_prediction = self.student(x, training=False)
        print(y_prediction.shape)
        # Calculate the loss
        student_loss = self.student_loss_fn(y, y_prediction)

        # Update the metrics.
        self.compiled_metrics.update_state(y, y_prediction)

        # Return a dict of performance
        results = {m.name: m.result() for m in self.metrics}
        results.update({"student_loss": student_loss})
        return results


def teacher(input_size = 32 ,dropout = 0.2):
    '''
    Takes only the first image from the burst and pass it trough a net that 
    aceives ~80% accuracy on full res cifar. 
    '''
    inputA = keras.layers.Input(shape=(input_size,input_size,3))

    
    # define CNN model
    x1=keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same')(inputA)
    print(x1.shape)
    x1=keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'valid')(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)
    x1=keras.layers.Dropout(dropout)(x1)

    x1=keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'valid')(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)
    x1=keras.layers.Dropout(dropout)(x1)

    x1=keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same')(x1)
    x1=keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'valid')(x1)
    x1=keras.layers.MaxPooling2D(pool_size=(2, 2))(x1)
    x1=keras.layers.Dropout(dropout)(x1)
    print(x1.shape)

    # x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    # print(x1.shape)st_parameters['epochs']

    x1 = keras.layers.Flatten(name = 'teacher_features')(x1)
    print(x1.shape)
    x2 = keras.layers.Dense(10)(x1)
    print(x2.shape)
    model = keras.models.Model(inputs=inputA,outputs=[x1,x2], name = 'teacher')
    opt=tf.keras.optimizers.Adam(lr=1e-3)
    model = teacher_training(model)
    model.compile(
        optimizer=opt,
        loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"],
    )
    return model

def student(n_timesteps = 5, hidden_size = 256,input_size = 32, 
            concat = True, cnn_dropout = 0.4, rnn_dropout = 0.2):
    '''
    
    CNN RNN combination that extends the CNN to a network that achieves 
    ~80% accuracy on full res cifar.
    This student network outputs teacher = extended_cnn_one_img(n_timesteps = 1, input_size = st_parameters['epochs']32, dropout = 0.2)
    Parameters
    ----------
    n_timesteps : TYPE, optional
        DESCRIPTION. The default is 5.
    img_dim : TYPE, optional
        DESCRIPTION. The default is 32.
    hidden_size : TYPE, optional
        DESCRIPTION. The default is 128.teacher_network = teacher(input_size = 32, dropout = 0.2)
    input_size : TYPE, optional
        DESCRIPTION. The default is 32.epochs

    Returns
    -------
    model : TYPE
        DESCRIPTION.

    '''
    inputA = keras.layers.Input(shape=(n_timesteps,input_size,input_size,3))
    inputB = keras.layers.Input(shape=(n_timesteps,2))

    # define CNN model

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same'))(inputA)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)
    print(x1.shape)


    x1=keras.layers.TimeDistributed(keras.layers.Flatten(),name = 'cnn_student_features')(x1)
    print(x1.shape)
    if concat:
        x1 = keras.layers.Concatenate()([x1,inputB])
    else:
        x1 = x1
    print(x1.shape)

    # define LSTM model
    x1 = keras.layers.LSTM(hidden_size,input_shape=(n_timesteps, None),return_sequences=True,recurrent_dropout=rnn_dropout)(x1)
    print(x1.shape)
    x1 = keras.layers.Flatten()(x1)
    print(x1.shape)
    x1 = keras.layers.Dense(512, activation = 'relu', name = "student_features")(x1)
    print(x1.shape)
    
    x = keras.layers.Dense(10)(x1) #activation will be in the distiller
    model = keras.models.Model(inputs=[inputA,inputB],outputs=[x1,x], name = 'student{}'.format(concat))

    return model

res = 8
sample = 10
n_timesteps = sample

def split_dataset_xy(dataset):
    dataset_x1 = [uu[0] for uu in dataset]
    dataset_x2 = [uu[1] for uu in dataset]
    dataset_y = [uu[-1] for uu in dataset]
    return (np.array(dataset_x1),np.array(dataset_x2)[:,:n_timesteps,:]),np.array(dataset_y)
#%%

accur_dataset = pd.DataFrame()
#%%
teacher_network = teacher(input_size = 32, dropout = 0.2)

#%%
print('######################### TRAIN TEACHER ##############################')
teacher_history = teacher_network.fit(
                           trainX[:45000],
                           trainy[:45000],
                           batch_size = 64,
                           epochs = 15,#st_parameters['epochs'],
                           validation_data = (trainX[45000:],trainy[45000:]),
                           verbose = 0,
                           )
print('teacher test accuracy = ',teacher_history.history['val_sparse_categorical_accuracy'])
# accur_dataset['teacher_test'] = teacher_history.history['val_sparse_categorical_accuracy']
# accur_dataset['teacher_train'] = teacher_history.history['sparse_categorical_accuracy']
#%%
syclop_train_dataset, syclop_test_dataset = create_cifar_dataset(images, labels,res = res,
                                    sample = sample, return_datasets=True, 
                                    mixed_state = False, add_seed = 0,
                                    )
train_dataset_x, train_dataset_y = split_dataset_xy(syclop_train_dataset)
test_dataset_x, test_dataset_y = split_dataset_xy(syclop_test_dataset)
#%%
print('######################### TRAIN STUDENT ##############################')

student_network = student(n_timesteps = sample, hidden_size = 128,input_size = 8, 
            concat = True, cnn_dropout = 0.4, rnn_dropout = 0.2)

#keras.utils.plot_model(student_network, expand_nested=True)
#%%
KD_student = keras.models.clone_model(student_network)
KD = Distiller(KD_student, teacher_network)

KD.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           distillation_loss_fn = keras.losses.KLDivergence(),
           alpha = st_parameters['alpha'],
           temperature = st_parameters['temp'])

KD_history = KD.fit([train_dataset_x,trainX[:45_000]],
        train_dataset_y,
        batch_size = 64,
        epochs = st_parameters['epochs'],
        validation_data = (test_dataset_x, test_dataset_y), 
        verbose = 0,
        )

print('KD test accuracy = ',KD_history.history['val_sparse_categorical_accuracy'])
accur_dataset['KD_test'] = KD_history.history['val_sparse_categorical_accuracy']
accur_dataset['KD_train'] = KD_history.history['sparse_categorical_accuracy']

#%%
FL_student = keras.models.clone_model(student_network)
FL = feature_st(FL_student, teacher_network)

FL.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           features_loss_fn = keras.losses.MeanSquaredError(),
           beta = st_parameters['beta'],
           temperature = st_parameters['temp'])

FL_history = FL.fit([train_dataset_x,trainX[:45_000]],
        train_dataset_y,
        batch_size = 64,
        epochs = st_parameters['epochs'],
        validation_data = (test_dataset_x, test_dataset_y), 
        verbose = 0,
        )

print('FL test accuracy = ',FL_history.history['val_sparse_categorical_accuracy'])
accur_dataset['FL_test'] = FL_history.history['val_sparse_categorical_accuracy']
accur_dataset['FL_train'] = FL_history.history['sparse_categorical_accuracy']
#%%
baseline_student = keras.models.clone_model(student_network)
baseline_model = teacher_training(baseline_student)
baseline_model.compile(
        optimizer=keras.optimizers.Adam(lr = st_parameters['lr']),
        loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"],
    )
baseline_history = baseline_model.fit(train_dataset_x,
        train_dataset_y,
        batch_size = 64,
        epochs = st_parameters['epochs'],
        validation_data = (test_dataset_x, test_dataset_y), 
        verbose = 0,
        )

print('Baseline test accuracy = ',baseline_history.history['val_sparse_categorical_accuracy'])
accur_dataset['baseline_test'] = baseline_history.history['val_sparse_categorical_accuracy']
accur_dataset['baseline_train'] = baseline_history.history['sparse_categorical_accuracy']
#%%
plt.figure()
#plt.plot(teacher_history.history['val_sparse_categorical_accuracy'], label = 'teacher')
plt.plot(KD_history.history['val_sparse_categorical_accuracy'], label = 'KD')
plt.plot(FL_history.history['val_sparse_categorical_accuracy'], label = 'FL')
plt.plot(baseline_history.history['val_sparse_categorical_accuracy'], label = 'baseline')
plt.legend()
plt.grid()
plt.ylim(0.5,0.63)
plt.title('Comparing KD and FL teacher student models on cifar')
plt.savefig('KD_FL_cifar_syclop_{:.0e}_{}_{:.0e}_{}.png'.format(st_parameters['alpha'],st_parameters['temp'], st_parameters['beta'], np.random.randint(5,100)))
#%%
plt.figure()
#plt.plot(teacher_history.history['val_sparse_categorical_accuracy'], label = 'teacher')
#plt.plot(KD_history.history['val_sparse_categorical_accuracy'], label = 'KD')
plt.plot(KD_history.history['sparse_categorical_accuracy'], label = 'KD train')
#plt.plot(FL_history.history['val_sparse_categorical_accuracy'], label = 'FL')
plt.plot(FL_history.history['sparse_categorical_accuracy'], label = 'FL train')
#plt.plot(baseline_history.history['val_sparse_categorical_accuracy'], label = 'baseline')
plt.plot(baseline_history.history['sparse_categorical_accuracy'], label = 'baseline train')
plt.legend()
plt.grid()
plt.ylim(0.5,0.63)
plt.title('Comparing KD and FL teacher student models on cifar train accur')
print(accur_dataset['KD_train'])
plt.savefig('KD_FL_cifar_syclop_train_{:.0e}_{}_{:.0e}_{}.png'.format(st_parameters['alpha'],st_parameters['temp'], st_parameters['beta'], np.random.randint(5,100)))
#%%
print('################ Train Student - with pre-training ####################')
base_student = keras.models.clone_model(student_network)
base_student.set_weights(student_network.get_weights()) 
base_model = teacher_training(base_student)
base_model.compile(
        optimizer=keras.optimizers.Adam(lr = st_parameters['lr']),
        loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"],
    )
base_history = base_model.fit(train_dataset_x,
        train_dataset_y,
        batch_size = 64,
        epochs = st_parameters['student_fst_learning'],
        validation_data = (test_dataset_x,test_dataset_y),
        verbose = 0,
        )

print('Base test accuracy = ',base_history.history['val_sparse_categorical_accuracy'])

#%%
new_epochs = st_parameters['epochs'] - st_parameters['student_fst_learning']
if new_epochs < 0:
    new_epochs = 1
    print("ERROR - less than 1 epochs left after base learning!!!!!!!!")
print('############## Knowledge Distillation wt Pre-Trainiong ################')
KD_student_pt = keras.models.clone_model(base_student)
KD_student_pt.set_weights(base_student.get_weights()) 
KD_pt = Distiller(KD_student_pt, teacher_network)

KD_pt.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           distillation_loss_fn = keras.losses.KLDivergence(),
           alpha = st_parameters['alpha'],
           temperature = st_parameters['temp'])

KD_pt_history = KD_pt.fit([train_dataset_x,trainX[:45_000]],
        train_dataset_y,
        batch_size = 64,
        epochs = new_epochs,
        validation_data = (test_dataset_x, test_dataset_y), 
        verbose = 0,
        )

print('KD pt test accuracy = ',KD_pt_history.history['val_sparse_categorical_accuracy'])
accur_dataset['KD_pt_test'] = base_history.history['val_sparse_categorical_accuracy'] + \
                                KD_pt_history.history['val_sparse_categorical_accuracy']
accur_dataset['KD_pt_train'] = base_history.history['sparse_categorical_accuracy'] + \
                                KD_pt_history.history['sparse_categorical_accuracy']
#%%
print('########################## Feature Learing ############################')
FL_pt_student = keras.models.clone_model(base_student)
FL_pt_student.set_weights(base_student.get_weights()) 
FL_pt = feature_st(FL_pt_student, teacher_network)

FL_pt.compile(optimizer = keras.optimizers.Adam(lr = st_parameters['lr']),
           metrics   = ["sparse_categorical_accuracy"],
           student_loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True),
           features_loss_fn = keras.losses.MeanSquaredError(),
           beta = st_parameters['beta'],
           temperature = st_parameters['temp'])

FL_pt_history = FL_pt.fit([train_dataset_x,trainX[:45_000]],
        train_dataset_y,
        batch_size = 64,
        epochs = new_epochs,
        validation_data = (test_dataset_x, test_dataset_y), 
        verbose = 0,
        )


print('FL pt test accuracy = ',FL_pt_history.history['val_sparse_categorical_accuracy'])
accur_dataset['FL_pt_test'] = base_history.history['val_sparse_categorical_accuracy'] + \
                                FL_pt_history.history['val_sparse_categorical_accuracy']
accur_dataset['FL_pt_train'] = base_history.history['val_sparse_categorical_accuracy'] + \
                                FL_pt_history.history['sparse_categorical_accuracy']



accur_dataset.to_pickle('KD_FL_pt_cifar_syclop_{:.0e}_{}_{:.0e}_{}.pkl'.format(st_parameters['alpha'],st_parameters['temp'], st_parameters['beta'],np.random.randint(5,100)))
#%%
plt.figure()
#plt.plot(teacher_history.history['val_sparse_categorical_accuracy'], label = 'teacher')
plt.plot(KD_history.history['val_sparse_categorical_accuracy'], label = 'KD')
plt.plot(FL_history.history['val_sparse_categorical_accuracy'], label = 'FL')
plt.plot(base_history.history['val_sparse_categorical_accuracy'], label = 'baseline')
plt.plot(accur_dataset['KD_pt_test'], label = 'KD_pt')
plt.plot(accur_dataset['FL_pt_test'], label = 'FL_pt')
plt.legend()
plt.grid()
plt.ylim(0.5,0.63)
plt.title('Comparing KD and FL teacher student models on cifar')
plt.savefig('KD_FL_pt_cifar_syclop_{:.0e}_{}_{:.0e}_{}png'.format(st_parameters['alpha'],st_parameters['temp'], st_parameters['beta'], np.random.randint(5,100)))
