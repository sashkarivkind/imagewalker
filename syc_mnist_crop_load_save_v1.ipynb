{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load recorded events list from .pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set params value for .pkl file name:\n",
    "\n",
    "fr_ev_th=4\n",
    "mnist_sym_start=30000\n",
    "mnist_sym_end=49999\n",
    "frame_size = 16\n",
    "# list_np_fr = [[] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f_path='mnist_dvs/'\n",
    "f_name='mnist_'+str(mnist_sym_start)+'_'+str(mnist_sym_end)+'.pkl'\n",
    "full_path=f_path+f_name\n",
    "with open(full_path, 'rb') as f:\n",
    "    events = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_name)\n",
    "print(len(events))\n",
    "print(len(events[3]))\n",
    "# print(type(events[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list of numpy frames (with crop) on_off/on/off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operational with crop and create a list of 3d numpy arrays = num_of_frames x 2D frame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def to_uint8( data ) :\n",
    "    # maximum pixel\n",
    "    latch = np.zeros_like( data )\n",
    "    latch[:] = 255\n",
    "    # minimum pixel\n",
    "    zeros = np.zeros_like( data )\n",
    "\n",
    "    # unrolled to illustrate steps\n",
    "    d = np.maximum( zeros, data )\n",
    "    d = np.minimum( latch, d )\n",
    "\n",
    "    # cast to uint8\n",
    "    return np.asarray( d, dtype=\"uint8\" )\n",
    "\n",
    "# num_of_mnist_symbols=6000\n",
    "original_frame_size=32\n",
    "\n",
    "# ----------Input parameters:--------\n",
    "max_num_of_frames=100\n",
    "# max_num_of_frames=10\n",
    "frame_size=16\n",
    "# -----------------------------------\n",
    "\n",
    "crop_x_min=(original_frame_size-frame_size)//2+1+(original_frame_size-frame_size)%2\n",
    "crop_x_max=original_frame_size-((original_frame_size-frame_size)//2+1)\n",
    "crop_y_min=crop_x_min\n",
    "crop_y_max=crop_x_max\n",
    "\n",
    "list_np_fr_arr_on_off = []\n",
    "list_np_fr_arr_on = []\n",
    "list_np_fr_arr_off = []\n",
    "# fr_ev_th=8\n",
    "for mnist_sym in events:\n",
    "    np_fr_arr_on_off=np.zeros((max_num_of_frames,frame_size,frame_size))\n",
    "    np_fr_arr_on=np.zeros((max_num_of_frames,frame_size,frame_size))\n",
    "    np_fr_arr_off=np.zeros((max_num_of_frames,frame_size,frame_size))\n",
    "#     ev_cnt=[0,0,0]\n",
    "#     fr_num=[0,0,0]\n",
    "    ev_cnt=0\n",
    "    fr_num=0\n",
    "    for e in mnist_sym:\n",
    "        x=e[1]-crop_x_min\n",
    "        y=e[2]-crop_y_min\n",
    "        if e[1]>=crop_x_min and e[1]<=crop_x_max and e[2]>=crop_y_min and e[2]<=crop_y_max:\n",
    "#             np_fr_arr_on_off[fr_num[0], (frame_size-1)-y,x]=np_fr_arr_on_off[fr_num[0], (frame_size-1)-y,x]+1\n",
    "            np_fr_arr_on_off[fr_num, (frame_size-1)-y,x]=np_fr_arr_on_off[fr_num, (frame_size-1)-y,x]+1\n",
    "            if e[3]==1:\n",
    "#                 if ev_cnt[1]>=fr_ev_th-1 and fr_num[1]<max_num_of_frames-1:\n",
    "#                     fr_num[1]+=1\n",
    "#                     ev_cnt[1]=0\n",
    "#                 else:\n",
    "#                     ev_cnt[1]+=1\n",
    "#                 np_fr_arr_on[fr_num[1], (frame_size-1)-y,x]=np_fr_arr_on[fr_num[1], (frame_size-1)-y,x]+1\n",
    "                np_fr_arr_on[fr_num, (frame_size-1)-y,x]=np_fr_arr_on[fr_num, (frame_size-1)-y,x]+1\n",
    "            else:\n",
    "#                 if ev_cnt[2]>=fr_ev_th-1 and fr_num[2]<max_num_of_frames-1:\n",
    "#                     fr_num[2]+=1\n",
    "#                     ev_cnt[2]=0\n",
    "#                 else:\n",
    "#                     ev_cnt[2]+=1\n",
    "#                 np_fr_arr_off[fr_num[2], (frame_size-1)-y,x]=np_fr_arr_off[fr_num[2], (frame_size-1)-y,x]+1\n",
    "                np_fr_arr_off[fr_num, (frame_size-1)-y,x]=np_fr_arr_off[fr_num, (frame_size-1)-y,x]+1\n",
    "\n",
    "#             if ev_cnt[0]>=fr_ev_th-1 and fr_num[0]<max_num_of_frames-1:\n",
    "#                 fr_num[0]+=1\n",
    "#                 ev_cnt[0]=0\n",
    "#             else:\n",
    "#                 ev_cnt[0]+=1\n",
    "            if ev_cnt>=fr_ev_th-1 and fr_num<max_num_of_frames-1:\n",
    "                fr_num+=1\n",
    "                ev_cnt=0\n",
    "            else:\n",
    "                ev_cnt+=1\n",
    "    \n",
    "#     fr_num[0] += 1\n",
    "#     fr_num[1] += 1\n",
    "#     fr_num[2] += 1\n",
    "    fr_num += 1\n",
    "    \n",
    "#     list_np_fr_arr_on_off.append(np_fr_arr_on_off[:fr_num[0],:,:])\n",
    "#     list_np_fr_arr_on.append(np_fr_arr_on[:fr_num[1],:,:])\n",
    "#     list_np_fr_arr_off.append(np_fr_arr_off[:fr_num[2],:,:])\n",
    "    list_np_fr_arr_on_off.append(np_fr_arr_on_off[:fr_num,:,:]/fr_ev_th)\n",
    "    list_np_fr_arr_on.append(np_fr_arr_on[:fr_num,:,:]/fr_ev_th)\n",
    "    list_np_fr_arr_off.append(np_fr_arr_off[:fr_num,:,:]/fr_ev_th)\n",
    "\n",
    "\n",
    "list_np_fr_6000=[list_np_fr_arr_on_off,list_np_fr_arr_on,list_np_fr_arr_off]\n",
    "\n",
    "# -----Testing parameters:----------    \n",
    "frame_sample = 0\n",
    "mnist_sym_relative_ind=0\n",
    "# ----------------------------------\n",
    "\n",
    "print(len(list_np_fr_arr_on_off))\n",
    "print(y_train[mnist_sym_relative_ind])\n",
    "print(type(list_np_fr_arr_on_off[mnist_sym_relative_ind][frame_sample,:,:]))\n",
    "print(list_np_fr_arr_on_off[mnist_sym_relative_ind][frame_sample,:,:].shape)\n",
    "image = list_np_fr_arr_on_off[mnist_sym_relative_ind][frame_sample,:,:]\n",
    "# plot the sample\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and plot frame samples from the list of 6000 mnist symbol event frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_sym_start=12000\n",
    "frame_sample = 2\n",
    "mnist_sym_relative_ind=2000\n",
    "print(len(list_np_fr_arr_on_off))\n",
    "print(y_train[mnist_sym_start+mnist_sym_relative_ind])\n",
    "print(type(list_np_fr_arr_on[mnist_sym_relative_ind]))\n",
    "print(list_np_fr_arr_on[mnist_sym_relative_ind].shape)\n",
    "for frame_sample in range(list_np_fr_arr_on[mnist_sym_relative_ind].shape[0]):\n",
    "    image = list_np_fr_arr_on[mnist_sym_relative_ind][frame_sample,:,:]\n",
    "    # plot the sample\n",
    "    fig = plt.figure\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "    print(np.sum(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save recorded events numpy frame lists to .pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# list_np_fr=[list_np_fr_arr_on_off,list_np_fr_arr_on,list_np_fr_arr_off]\n",
    "f_path='mnist_dvs/'\n",
    "f_name='mnist_'+str(mnist_sym_start)+'_'+str(mnist_sym_end)+'_list_np_fr_'+str(frame_size)+'_ev_th_'+str(fr_ev_th)+'.pkl'\n",
    "full_path=f_path+f_name\n",
    "\n",
    "print(full_path)\n",
    "with open(full_path, 'wb') as f:\n",
    "    pickle.dump(list_np_fr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_np_fr_6000=[list_np_fr_arr_on_off,list_np_fr_arr_on,list_np_fr_arr_off]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load recorded events list from .pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "mnist_start = 12000\n",
    "mnist_end = 17999\n",
    "\n",
    "f_path='./'\n",
    "f_name='mnist_'+str(mnist_start)+'_'+str(mnist_end)+'_list_np_fr_'+str(frame_size)+'_ev_th_'+str(fr_ev_th)+'.pkl'\n",
    "full_path=f_path+f_name\n",
    "with open(full_path, 'rb') as f:\n",
    "    list_np_fr_6000 = pickle.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_np_fr_6000[0][869]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_np_fr = [[] for i in range(3)]\n",
    "\n",
    "mnist_sym_start=0\n",
    "mnist_sym_end=49999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_sym_start=0\n",
    "mnist_sym_end=24000\n",
    "\n",
    "# for i in range(3):\n",
    "#     list_np_fr[i] = list_np_fr[i][mnist_sym_start:mnist_sym_end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for frames in list_np_fr_6000[i]:\n",
    "        list_np_fr[i].append(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_np_fr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path='mnist_dvs/'\n",
    "f_name='mnist_'+str(mnist_sym_start)+'_'+str(mnist_sym_end)+'_list_np_fr_'+str(frame_size)+'_ev_th_'+str(fr_ev_th)+'.pkl'\n",
    "full_path=f_path+f_name\n",
    "\n",
    "print(full_path)\n",
    "with open(full_path, 'wb') as f:\n",
    "    pickle.dump(list_np_fr, f)\n",
    "    \n",
    "len(list_np_fr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and plot frame samples from the list of 6000 mnist symbol event frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_sample = 1\n",
    "mnist_sym_relative_ind=41999\n",
    "on_off = 0# 0-both,1-on,2-off\n",
    "print(y_train[mnist_sym_start+mnist_sym_relative_ind])\n",
    "# print(len(list_np_fr[1]))\n",
    "\n",
    "frames_n = list_np_fr[on_off][mnist_sym_relative_ind].shape[0]\n",
    "fig, axs = plt.subplots(1, frames_n, figsize=(10,6))\n",
    "image_sum = np.zeros(list_np_fr[on_off][mnist_sym_relative_ind][0,:,:].shape)\n",
    "\n",
    "for frame_sample in range(frames_n):\n",
    "    image = list_np_fr[on_off][mnist_sym_relative_ind][frame_sample,:,:] # list_np_fr_arr_on on events only\n",
    "    if frames_n>1:\n",
    "        axs[frame_sample].imshow(image, cmap='gray')\n",
    "        axs[frame_sample].set_title('frame'+str(frame_sample))\n",
    "    else:\n",
    "        axs.imshow(image, cmap='gray')\n",
    "        axs.set_title('frame'+str(frame_sample))\n",
    "    image_sum += image\n",
    "# plt.savefig('frames_image10004.png')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image_sum)\n",
    "# plt.savefig('image_sum7.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  generate single matrix with all events\n",
    "on_off = 0\n",
    "\n",
    "list_allevents = []\n",
    "list_eventsum = []\n",
    "for indx, mnist_sample in enumerate(list_np_fr[on_off]):\n",
    "    image_sum = np.zeros(mnist_sample[0,:,:].shape)\n",
    "    for frame in mnist_sample:\n",
    "        image_sum += frame\n",
    "        \n",
    "    list_allevents.append(image_sum[np.newaxis,:,:])\n",
    "    list_eventsum.append(np.sum(image_sum))\n",
    "\n",
    "print(len(list_allevents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dvs_generate_fr_dataset(list_np_fr, on_off = 0, ts = 6):\n",
    "\n",
    "    traj = np.zeros([ts,2])\n",
    "    missing_frames_count = 0\n",
    "    \n",
    "    dvs_images = []\n",
    "    trajectories = []\n",
    "    \n",
    "    for ind, samples in enumerate(list_np_fr):\n",
    "        dimim = np.zeros([ts, frame_size, frame_size, 1])\n",
    "        frames = np.min([ts,samples.shape[0]])#last frame contains less than 'th' events\n",
    "        dimim[-frames:,:,:,0] = samples[:frames,:,:]\n",
    "\n",
    "#         if samples.shape[0] < ts:\n",
    "#             missing_frames_count += 1\n",
    "        \n",
    "        dvs_images.append(dimim)\n",
    "        trajectories.append(traj)\n",
    "        \n",
    "    # dvs_images = np.array(dvs_images)[:,:,:,:, np.newaxis]\n",
    "    dvs_images = np.array(dvs_images)[:,:,:,:]\n",
    "    trajectories = np.array(trajectories)\n",
    "    x_dvs = (dvs_images, trajectories)\n",
    "        \n",
    "    return x_dvs\n",
    "\n",
    "def dvs_generate_onoff_lasagna(ds1, ds2):\n",
    "    ds_onoff = [[],[]]\n",
    "    ds_onoff[0] = np.stack((ds1[0].squeeze(),ds2[0].squeeze()),axis=4)\n",
    "    ds_onoff[1] = ds1[1]\n",
    "\n",
    "    return ds_onoff\n",
    "#     test_x_dvs_onoff = [[],[]]\n",
    "#     test_x_dvs_onoff[0] = np.stack((test_x_dvs[0].squeeze(),test_x_dvs_[0].squeeze()),axis=4)\n",
    "#     # test_x_dvs_onoff[0] = np.stack((test_x_dvs[0].squeeze(),test_x_dvs_[0].squeeze()))\n",
    "#     test_x_dvs_onoff[1] = test_x_dvs[1]\n",
    "\n",
    "def dvs_generate_ds(list_np_fr, on_off = 0, ts = 6, mnist_indx = [0,10000], sym_used = [0,10000], n_val = 1000, lasgna = True):\n",
    "# on_off = 1# 0-both,1-on,2-off,3-on,off\n",
    "# ts = 3\n",
    "# first_mnist_indx = 0 # absolute- relative to mnist dataset\n",
    "# last_mnist_indx = 49999\n",
    "\n",
    "# used_sym_start = 5000 # relative- samples to use to build dataset\n",
    "# used_sym_end = 49999\n",
    "#     val_n = 4000\n",
    "\n",
    "    y_train_aligned = y_train[mnist_indx[0]:mnist_indx[1]]\n",
    "    rand_perm = np.random.permutation(range(sym_used[0], sym_used[1]))\n",
    "    train_ind = rand_perm[0:-n_val]\n",
    "    test_ind = rand_perm[-n_val:]\n",
    "\n",
    "    #train data\n",
    "    list_fr_train = [list_np_fr[on_off[0]][i] for i in train_ind]\n",
    "    train_x_dvs = dvs_generate_fr_dataset(list_fr_train, on_off[0], ts)\n",
    "    train_y_dvs = y_train_aligned[train_ind]\n",
    "\n",
    "    #test data\n",
    "    list_fr_test = [list_np_fr[on_off[0]][i] for i in test_ind]\n",
    "    test_x_dvs = dvs_generate_fr_dataset(list_fr_test, on_off[0], ts)\n",
    "    test_y_dvs = y_train_aligned[test_ind]\n",
    "\n",
    "    if lasgna:\n",
    "       #second data set\n",
    "#        on_off = 2 # 0-both,1-on,2-off,3-on,off\n",
    "       #train data\n",
    "        list_fr_train = [list_np_fr[on_off[1]][i] for i in train_ind]\n",
    "        train_x_dvs_ = dvs_generate_fr_dataset(list_fr_train, on_off[1], ts)\n",
    "       #test data\n",
    "        list_fr_test = [list_np_fr[on_off[1]][i] for i in test_ind]\n",
    "        test_x_dvs_ = dvs_generate_fr_dataset(list_fr_test, on_off[1], ts)\n",
    "        \n",
    "        train_x_dvs = dvs_generate_onoff_lasagna(train_x_dvs, train_x_dvs_)\n",
    "        test_x_dvs = dvs_generate_onoff_lasagna(test_x_dvs, test_x_dvs_)\n",
    "\n",
    "    return train_x_dvs, train_y_dvs, test_x_dvs, test_y_dvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_dvs_onoff = [[],[]]\n",
    "# train_x_dvs_onoff[0] = np.stack((train_x_dvs[0].squeeze(),train_x_dvs_[0].squeeze()),axis=4)\n",
    "# # train_x_dvs_onoff[0] = np.stack((train_x_dvs[0].squeeze(),train_x_dvs_[0].squeeze()))\n",
    "# train_x_dvs_onoff[1] = train_x_dvs[1]\n",
    "\n",
    "# test_x_dvs_onoff = [[],[]]\n",
    "# test_x_dvs_onoff[0] = np.stack((test_x_dvs[0].squeeze(),test_x_dvs_[0].squeeze()),axis=4)\n",
    "# # test_x_dvs_onoff[0] = np.stack((test_x_dvs[0].squeeze(),test_x_dvs_[0].squeeze()))\n",
    "# test_x_dvs_onoff[1] = test_x_dvs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fr_ev_th = 2\n",
    "frame_size = 16\n",
    "mnist_start = 0\n",
    "mnist_end = 49999\n",
    "\n",
    "f_path='./mnist_dvs/'\n",
    "f_name='mnist_'+str(mnist_start)+'_'+str(mnist_end)+'_list_np_fr_'+str(frame_size)+'_ev_th_'+str(fr_ev_th)+'.pkl'\n",
    "full_path=f_path+f_name\n",
    "with open(full_path, 'rb') as f:\n",
    "    list_np_fr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras_networks\n",
    "import importlib\n",
    "importlib.reload(keras_networks)\n",
    "\n",
    "ts = 12\n",
    "mnist_indx = [0,49999]\n",
    "sym_used = [5000,49999]\n",
    "n_val = 4000\n",
    "lasgna = True\n",
    "on_off = [1,2]\n",
    "\n",
    "runs = 10\n",
    "val_accuracy_arr = np.zeros(10)\n",
    "for i in range(0,runs):\n",
    "    train_x_dvs, train_y_dvs, test_x_dvs, test_y_dvs = dvs_generate_ds(list_np_fr, on_off, \n",
    "                                                                  ts, mnist_indx, sym_used, \n",
    "                                                                  n_val, lasgna)\n",
    "\n",
    "    rnn102 = keras_networks.rnn_model_102(n_timesteps=ts,\n",
    "                                      lr=1e-3,dropout=0.0,\n",
    "                                      rnn_type='gru',\n",
    "                                      rnn_layers = 1,\n",
    "                                      input_size=(frame_size, frame_size, 2),\n",
    "                                      conv_fe=False,\n",
    "                                      rnn_units = 100)\n",
    "  \n",
    "\n",
    "    print(\"Fit model on training data, run \",i)\n",
    "    history = rnn102.fit(\n",
    "            train_x_dvs,\n",
    "            train_y_dvs,\n",
    "            batch_size=128,\n",
    "            epochs=10,\n",
    "            # We pass some validation for\n",
    "            # monitoring validation loss and metrics\n",
    "            # at the end of each epoch\n",
    "            validation_data=(test_x_dvs, test_y_dvs)) #\n",
    "    \n",
    "    val_accuracy_arr[i] = history.history['val_sparse_categorical_accuracy'][-1:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(val_accuracy_arr), np.std(val_accuracy_arr))\n",
    "# val_accuracy_arr_4_12 = val_accuracy_arr\n",
    "\n",
    "data = [val_accuracy_arr_2_24, val_accuracy_arr_2_12, val_accuracy_arr_4_12, val_accuracy_arr_8, val_accuracy_arr_16]\n",
    "plt.figure()\n",
    "plt.boxplot(data, labels = [\"ev_th = 2\\nts=24\", \"ev_th = 2\\nts=12\", \"ev_th = 4\\nts=12\", \"ev_th = 8\\nts=6\", \"ev_th = 16\\nts=3\"])\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "plt.savefig('performance_vs_ev-th.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred102 = rnn102.predict(test_x_dvs_onoff)\n",
    "\n",
    "plt.figure(figsize=[4,1])\n",
    "plt.bar(['0', '1' , '2', '3', '4', '5', '6', '7', '8', '9'], height = pred102[110])\n",
    "\n",
    "# plt.savefig('network_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path='mnist_dvs/'\n",
    "\n",
    "symbols_in_files= [[0, 5999], [6000, 11999], [12000, 17999], [18000, 23999], [24000, 29999]]\n",
    "num_of_events = [[] for i in symbols_in_files]\n",
    "for i, symbols in enumerate(symbols_in_files):\n",
    "    \n",
    "    f_name='mnist_'+str(symbols[0])+'_'+str(symbols[1])+'.pkl'\n",
    "    full_path=f_path+f_name\n",
    "    with open(full_path, 'rb') as f:\n",
    "        events = pickle.load(f)\n",
    "    \n",
    "    for sym in events:\n",
    "        num_of_events[i].append(len(sym))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plt.hist([num_of_events[0], num_of_events[1], num_of_events[2]], density = True)\n",
    "plt.hist(num_of_events[0], density = True)\n",
    "\n",
    "#  plt.figure()\n",
    "plt.hist(num_of_events[1], alpha=0.6, density=True)\n",
    "\n",
    "# plt.figure()\n",
    "plt.hist(num_of_events[2], alpha = 0.6, density=True)\n",
    "\n",
    "plt.hist(num_of_events[3], alpha = 0.6, density=True)\n",
    "\n",
    "plt.hist(num_of_events[4], alpha = 0.6, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(num_of_events[0]), np.mean(num_of_events[1]), np.mean(num_of_events[2]))\n",
    "print(np.std(num_of_events[0]), np.std(num_of_events[1]), np.std(num_of_events[2]))\n",
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plt.hist([num_of_events[0], num_of_events[1], num_of_events[2]], density = True)\n",
    "# plt.hist(list_eventsum[0:6000], density = True)\n",
    "\n",
    "#  plt.figure()\n",
    "# plt.hist(list_eventsum[6000:12000], alpha=0.6, density=True)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(list_eventsum[12000:18000], alpha = 0.6, density=True)\n",
    "\n",
    "# plt.hist(list_eventsum[18000:24000], alpha = 0.6 , density=True)\n",
    "\n",
    "# plt.hist(list_eventsum[24000:30000], alpha = 0.6, density=True)\n",
    "\n",
    "# plt.hist(list_eventsum[36000:42000], alpha = 0.6 , density=True)\n",
    "\n",
    "# plt.hist(list_eventsum[36000:48000], alpha = 0.6 , density=True)\n",
    "\n",
    "# plt.hist(list_eventsum[48000:54000], alpha = 0.6 , density=True)\n",
    "\n",
    "# print(np.mean(list_eventsum[36000:48000]))\n",
    "plt.hist(list_eventsum[5000:10000], alpha = 0.6 , density=True)\n",
    "# plt.hist(list_eventsum[0:10000], alpha = 0.6 , density=True)\n",
    "plt.hist(list_eventsum[10000:20000], alpha = 0.6 , density=True)\n",
    "plt.hist(list_eventsum[20000:30000], alpha = 0.6 , density=True)\n",
    "plt.hist(list_eventsum[30000:40000], alpha = 0.6 , density=True)\n",
    "plt.hist(list_eventsum[40000:50000], alpha = 0.6 , density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_eventsum[18000:18010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_eventsum_zero = [i==0.0 for i in list_eventsum[24000:36000]]\n",
    "all(list_eventsum_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = 'models/'\n",
    "model_name = 'mnist_dvs_keras_lowres'\n",
    "\n",
    "rnn102.save(models_path+model_name+'.h5')\n",
    "\n",
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "new_model = tf.keras.models.load_model(models_path+model_name+'.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_img = x_train[36100, :, :]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(mnist_img)\n",
    "plt.savefig('mnist_fig_36100.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (imagewalker-cpu)",
   "language": "python",
   "name": "imagewalker-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
