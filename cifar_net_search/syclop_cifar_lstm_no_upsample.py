'''
The follwing code runs a test lstm network on the CIFAR dataset 

I will explicitly write the networks here for ease of understanding 

with cnn_sropout = 0.4 and rnn dropout = 0.2  and  lr = 1e-3 res = 8
################# cnn_lstm_True Validation Accuracy =  [0.363, 0.4258, 0.4332, 0.4142, 0.4802, 0.4838, 0.4988, 0.4694, 0.5018, 0.5072, 0.5216, 0.5204, 0.5282, 0.5354, 0.5392, 0.541, 0.5372, 0.5496, 0.5488, 0.5458, 0.5514, 0.5464, 0.5598, 0.5612, 0.549, 0.561, 0.562, 0.5608, 0.572, 0.5562]
################# cnn_lstm_True Training Accuracy =  [0.2576222, 0.37971112, 0.41331112, 0.43568888, 0.45224443, 0.46142223, 0.4724, 0.48204446, 0.4924889, 0.49795556, 0.5046667, 0.50751114, 0.5161778, 0.5168, 0.5233778, 0.52584445, 0.53113335, 0.5362, 0.5368, 0.5395333, 0.5430667, 0.5438667, 0.54568887, 0.54833335, 0.5525111, 0.5526, 0.55462223, 0.5564889, 0.55682224, 0.5594]

with cnn_sropout = 0.4 and rnn dropout = 0.2  and  lr = 1e-3 res = 16
################# extended_cnn_one_img Validation Accuracy =  [0.4394, 0.481, 0.529, 0.5436, 0.5632, 0.5732, 0.5672, 0.5796, 0.5934, 0.6008, 0.5946, 0.5978, 0.6074, 0.6104, 0.6134, 0.6156, 0.6122, 0.6168, 0.6064, 0.6142, 0.6182, 0.6208, 0.6314, 0.6186, 0.614, 0.6234, 0.6166, 0.621, 0.6124, 0.6086]
################# extended_cnn_one_img Training Accuracy =  [0.28697777, 0.42, 0.46337777, 0.49582222, 0.52477777, 0.54244447, 0.5523111, 0.56891114, 0.58144444, 0.5856, 0.5954667, 0.60253334, 0.60866666, 0.61322224, 0.62204444, 0.6220889, 0.627, 0.6315778, 0.63177776, 0.63802224, 0.63993335, 0.64397776, 0.6459778, 0.6482889, 0.65115553, 0.64971113, 0.653, 0.65335554, 0.65393335, 0.6591778]
################# cnn_convlstm_True Validation Accuracy =  [0.4204, 0.4466, 0.5022, 0.5348, 0.5416, 0.542, 0.5822, 0.5834, 0.5962, 0.6112, 0.6086, 0.6198, 0.611, 0.6158, 0.6174, 0.6324, 0.6374, 0.6374, 0.6318, 0.639, 0.643, 0.6486, 0.6452, 0.6456, 0.6456, 0.644, 0.6628, 0.6512, 0.6426, 0.6474]
################# cnn_convlstm_True Training Accuracy =  [0.28697777, 0.42, 0.46337777, 0.49582222, 0.52477777, 0.54244447, 0.5523111, 0.56891114, 0.58144444, 0.5856, 0.5954667, 0.60253334, 0.60866666, 0.61322224, 0.62204444, 0.6220889, 0.627, 0.6315778, 0.63177776, 0.63802224, 0.63993335, 0.64397776, 0.6459778, 0.6482889, 0.65115553, 0.64971113, 0.653, 0.65335554, 0.65393335, 0.6591778]


with cnn_sropout = 0.4 and rnn dropout = 0.2  and  lr = 5e-4 res = 8 out.813114
################# cnn_lstm_True Validation Accuracy =  [0.3308, 0.3894, 0.4208, 0.4178, 0.4472, 0.4612, 0.4562, 0.4678, 0.4576, 0.4804, 0.4888, 0.4942, 0.504, 0.5104, 0.5124, 0.4714, 0.
5052, 0.5128, 0.5178, 0.5214, 0.5184, 0.5234, 0.5138, 0.5288, 0.534, 0.5274, 0.5338, 0.5252, 0.5196, 0.5238, 0.533, 0.5228, 0.5294, 0.541, 0.5432, 0.5308, 0.5396, 0.5438, 0.5548, 0.5496, 0.5416, 0.5376, 0.5434, 0.5482, 0.5476, 0.544, 0.5478, 0.5488, 0.5316, 0.541, 0.5458, 0.5502, 0.5538, 0.545, 0.5434, 0.5446, 0.5262, 0.565, 0.5524, 0.547, 0.558, 0.5534, 0.5504, 0.5572, 0.558, 0.5518, 0.5628, 0.5458, 0.5492, 0.554, 0.5502, 0.5662, 0.5554, 0.5544, 0.556, 0.5614, 0.556, 0.5494, 0.5626, 0.553, 0.5548, 0.5552, 0.5594, 0.5624, 0.5602, 0.5586, 0.5626, 0.5552, 0.5556, 0.5568, 0.5614, 0.5646, 0.5588, 0.5546, 0.5672, 0.5686, 0.5654, 0.5696, 0.561, 0.5594]
################# cnn_lstm_True Training Accuracy =  [0.24964444, 0.35444444, 0.39055556, 0.40411112, 0.42033333, 0.4307778, 0.4413778, 0.4502, 0.45922223, 0.4670222, 0.47257778, 0.47993332, 0.48346666, 0.48924443, 0.49424446, 0.49951112, 0.50535554, 0.507, 0.5122667, 0.51573336, 0.5187111, 0.52257776, 0.52484447, 0.52666664, 0.5283333, 0.5331333, 0.5357556, 0.5382222, 0.5380667, 0.54113334, 0.5438667, 0.5438222, 0.54602224, 0.5493778, 0.55248886, 0.55455554, 0.5526, 0.55502224, 0.5570222, 0.557, 0.5587556, 0.5602889, 0.56182224, 0.5638667, 0.5647111, 0.5691778, 0.5662889, 0.5692222, 0.56711113, 0.57075554, 0.5707333, 0.57548887, 0.5728667, 0.57446665, 0.5751778, 0.57706666, 0.5799556, 0.5784444, 0.5833333, 0.583, 0.58104444, 0.58404446, 0.58264446, 0.5810222, 0.5852444, 0.5855778, 0.5874, 0.5886889, 0.5931111, 0.5919333, 0.59191114, 0.5890222, 0.59022224, 0.59191114, 0.58986664, 0.5920445, 0.5929111, 0.5968222, 0.5930667, 0.59926665, 0.59415555, 0.5968889, 0.5962222, 0.59933335, 0.5995778, 0.5975111, 0.6013333, 0.6011111, 0.6008667, 0.60124445, 0.6018889, 0.60388887, 0.6032, 0.6028889, 0.60502225, 0.6044667, 0.60304445, 0.60517776, 0.6042445, 0.6062667]


with cnn_sropout = 0.2 and rnn dropout = 0.2  and  lr = 5e-4 res = 8 out.812846
################# extended_cnn_one_img Validation Accuracy =  [0.3528, 0.3696, 0.3942, 0.4074, 0.4162, 0.4102, 0.4304, 0.4336, 0.4502, 0.4432, 0.4534, 0.458, 0.4572, 0.4596, 0.453, 0.45
36, 0.468, 0.472, 0.47, 0.4638, 0.475, 0.4618, 0.466, 0.473, 0.4718, 0.4648, 0.467, 0.4684, 0.4666, 0.4708, 0.4746, 0.4752, 0.4722, 0.4814, 0.4782, 0.4836, 0.4778, 0.4712, 0.4828, 0.471
6, 0.481, 0.4762, 0.475, 0.4746, 0.4784, 0.479, 0.4806, 0.4776, 0.4786, 0.4798, 0.478, 0.4786, 0.4814, 0.4792, 0.4728, 0.4764, 0.471, 0.475, 0.467, 0.4794, 0.4802, 0.4814, 0.4766, 0.471
2, 0.4782, 0.4822, 0.4746, 0.473, 0.4758, 0.4748, 0.4726, 0.4756, 0.4758, 0.4782, 0.4786, 0.4714, 0.4752, 0.4752, 0.4728, 0.4814, 0.478, 0.4746, 0.4704, 0.481, 0.4728, 0.4734, 0.4778, 0
.4764, 0.4746, 0.4814, 0.4754, 0.4804, 0.4776, 0.4736, 0.4716, 0.475, 0.4754, 0.481, 0.4862, 0.4714]
################# extended_cnn_one_img Training Accuracy =  [0.2808, 0.39051113, 0.4234, 0.44626668, 0.4642, 0.47853333, 0.4906889, 0.50313336, 0.5134889, 0.52297777, 0.5306, 0.53653336
, 0.5456, 0.5512222, 0.5568, 0.56384444, 0.5692667, 0.5714667, 0.5779333, 0.5827778, 0.58984447, 0.5953778, 0.5967111, 0.60213333, 0.6068222, 0.61006665, 0.6106222, 0.6146, 0.6157111, 0
.61895555, 0.6228, 0.6257778, 0.625, 0.62846667, 0.63384444, 0.6364889, 0.63751113, 0.6385111, 0.64015555, 0.64522225, 0.6471111, 0.6487333, 0.6544222, 0.6525111, 0.6541778, 0.6599778, 
0.6571111, 0.6606445, 0.6649778, 0.66595554, 0.66293335, 0.6677333, 0.67242223, 0.6685778, 0.67415553, 0.67284447, 0.67606664, 0.67822224, 0.6788889, 0.68075556, 0.68237776, 0.6800889, 
0.68597776, 0.6885333, 0.6876444, 0.6862889, 0.6900667, 0.6915111, 0.6921333, 0.69166666, 0.6956667, 0.6958445, 0.69706666, 0.6955111, 0.6986, 0.70177776, 0.7012889, 0.70471114, 0.70537
776, 0.70317775, 0.7037778, 0.70611113, 0.7081556, 0.70717776, 0.71, 0.7101111, 0.71122223, 0.7131778, 0.7135556, 0.7114889, 0.7172889, 0.71206665, 0.7187333, 0.7174889, 0.7177333, 0.72
12667, 0.72146666, 0.71922225, 0.7218222, 0.7225556]
################# cnn_lstm_True Validation Accuracy =  [0.3562, 0.4108, 0.4418, 0.468, 0.48, 0.4946, 0.4864, 0.5004, 0.5002, 0.521, 0.5286, 0.5346, 0.5376, 0.5412, 0.55, 0.5554, 0.5374,
 0.5576, 0.571, 0.5506, 0.5658, 0.575, 0.5744, 0.5736, 0.5734, 0.5796, 0.573, 0.5888, 0.5766, 0.5766, 0.5814, 0.5784, 0.5756, 0.5816, 0.5844, 0.5826, 0.5878, 0.583, 0.5914, 0.5846, 0.5868, 0.5764, 0.5888, 0.5938, 0.5884, 0.5892, 0.5814, 0.5946, 0.5846, 0.5918, 0.5902, 0.5908, 0.5862, 0.5914, 0.5934, 0.5904, 0.591, 0.5996, 0.5876, 0.5954, 0.5924, 0.5976, 0.5858, 0.5944, 0.5912, 0.588, 0.594, 0.5902, 0.5898, 0.5882, 0.5852, 0.5844, 0.5862, 0.5878, 0.5884, 0.5972, 0.5944, 0.5882, 0.5828, 0.5908, 0.589, 0.5916, 0.5966, 0.593, 0.5802, 0.5934, 0.5908, 0.5952, 0.587, 0.5858, 0.5918, 0.591, 0.5912, 0.589, 0.5882, 0.5906, 0.5878, 0.588, 0.5894, 0.5916]
################# cnn_lstm_True Training Accuracy =  [0.2808, 0.39051113, 0.4234, 0.44626668, 0.4642, 0.47853333, 0.4906889, 0.50313336, 0.5134889, 0.52297777, 0.5306, 0.53653336, 0.5456, 0.5512222, 0.5568, 0.56384444, 0.5692667, 0.5714667, 0.5779333, 0.5827778, 0.58984447, 0.5953778, 0.5967111, 0.60213333, 0.6068222, 0.61006665, 0.6106222, 0.6146, 0.6157111, 0.61895555, 0.6228, 0.6257778, 0.625, 0.62846667, 0.63384444, 0.6364889, 0.63751113, 0.6385111, 0.64015555, 0.64522225, 0.6471111, 0.6487333, 0.6544222, 0.6525111, 0.6541778, 0.6599778, 0.6571111, 0.6606445, 0.6649778, 0.66595554, 0.66293335, 0.6677333, 0.67242223, 0.6685778, 0.67415553, 0.67284447, 0.67606664, 0.67822224, 0.6788889, 0.68075556, 0.68237776, 0.6800889, 0.68597776, 0.6885333, 0.6876444, 0.6862889, 0.6900667, 0.6915111, 0.6921333, 0.69166666, 0.6956667, 0.6958445, 0.69706666, 0.6955111, 0.6986, 0.70177776, 0.7012889, 0.70471114, 0.70537776, 0.70317775, 0.7037778, 0.70611113, 0.7081556, 0.70717776, 0.71, 0.7101111, 0.71122223, 0.7131778, 0.7135556, 0.7114889, 0.7172889, 0.71206665, 0.7187333, 0.7174889, 0.7177333, 0.7212667, 0.72146666, 0.71922225, 0.7218222, 0.7225556]


with cnn_sropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 and 500 epochs out.813765
################# extended_cnn_one_img Training Accuracy =  [0.24584444, 0.35053334, 0.38793334, 0.40875554, 0.4229111, 0.4331111, 0.44337776, 0.44886667, 0.45893332, 0.46524444, 0.47291112, 0.47626665, 0.4856889, 0.48975554, 0.4916, 0.4988, 0.5033111, 0.5056889, 0.5113556, 0.5152444, 0.5168222, 0.5198889, 0.5234889, 0.52444446, 0.5282889, 0.5336889, 0.53297776, 0.5376889, 0.54246664, 0.53664446, 0.5434, 0.5458, 0.5467333, 0.5486444, 0.5535111, 0.5506222, 0.5524667, 0.55295557, 0.55697775, 0.5574667, 0.5597111, 0.5593111, 0.56273335, 0.56615555, 0.56457776, 0.5658889, 0.56504446, 0.56744444, 0.5668667, 0.5719111, 0.56886667, 0.57093334, 0.5731556, 0.5764889, 0.5733111, 0.5746889, 0.5784889, 0.57868886, 0.5796667, 0.57868886, 0.58095556, 0.5823333, 0.5834444, 0.58566666, 0.5847778, 0.5849111, 0.58408886, 0.5852889, 0.58735555, 0.58813334, 0.58802223, 0.5897111, 0.5891778, 0.59106666, 0.58977777, 0.59164447, 0.5935778, 0.59155554, 0.5924444, 0.5958, 0.5974889, 0.5984222, 0.59635556, 0.59797776, 0.5967111, 0.59882224, 0.59864444, 0.59995556, 0.59793335, 0.6014, 0.5981111, 0.60044444, 0.60055554, 0.60208887, 0.6009778, 0.60113335, 0.6046, 0.6018889, 0.60624444, 0.6012, 0.60373336, 0.6020667, 0.60388887, 0.60364443, 0.60344446, 0.60855556, 0.6080667, 0.6056, 0.6059333, 0.61097777, 0.6092, 0.6079778, 0.6118444, 0.6070667, 0.6104, 0.60895556, 0.6101556, 0.6104444, 0.61002225, 0.6122, 0.6102222, 0.6121778, 0.61095554, 0.6102667, 0.61075556, 0.61311114, 0.6121778, 0.6160667, 0.61502224, 0.61444443, 0.61435556, 0.61344445, 0.61355555, 0.6150444, 0.6168444, 0.6152889, 0.6140444, 0.6184222, 0.61513335, 0.6177111, 0.61388886, 0.61653334, 0.61455554, 0.61595553, 0.62053335, 0.61826664, 0.6192667, 0.62093335, 0.6173556, 0.61895555, 0.61726665, 0.6194222, 0.6232, 0.62115556, 0.6197111, 0.62035555, 0.61844444, 0.62042224, 0.618, 0.6223556, 0.61902225, 0.62288886, 0.6220222, 0.6221778, 0.6239111, 0.6190889, 0.6250889, 0.6246, 0.6218, 0.62484443, 0.6250889, 0.6202889, 0.6244889, 0.62333333, 0.62457776, 0.6242, 0.6236, 0.6252, 0.6274667, 0.6245555, 0.6241111, 0.6262, 0.6253778, 0.6244444, 0.6254, 0.6255556, 0.6254, 0.62355554, 0.62986666, 0.6236445, 0.6255556, 0.62682223, 0.6252222, 0.6265778, 0.62684447, 0.62684447, 0.6264667, 0.6289333, 0.63042223, 0.62946665, 0.62873334, 0.6296889, 0.6269111, 0.6298222, 0.62975556, 0.6280889, 0.6296667, 0.6295556, 0.62873334, 0.6278222, 0.6266222, 0.6314222, 0.63211113, 0.62813336, 0.6260667, 0.6296889, 0.6291111, 0.63251114, 0.62813336, 0.63071114, 0.62946665, 0.6320889, 0.62913334, 0.63177776, 0.6306667, 0.63008887, 0.6332222, 0.6346, 0.63104445, 0.6309111, 0.6326, 0.6289333, 0.6310667, 0.6288889, 0.6339333, 0.63155556, 0.6330889, 0.6311111, 0.634, 0.63233334, 0.6316222, 0.63533336, 0.6349556, 0.63482225, 0.63224447, 0.6336667, 0.6331556, 0.6326, 0.63533336, 0.63024443, 0.6355778, 0.6332222, 0.6337111, 0.6360889, 0.6368222, 0.6360889, 0.6338222, 0.6360222, 0.63537776, 0.6355111, 0.6344444, 0.63255554, 0.634, 0.6377778, 0.6331111, 0.63493335, 0.63935554, 0.63553333, 0.63537776, 0.6352889, 0.63637775, 0.6369111, 0.6370889, 0.6368, 0.6359556, 0.6358889, 0.6383111, 0.6369111, 0.6352889, 0.6394445, 0.6378667, 0.63688886, 0.6374889, 0.6386222, 0.63902223, 0.6359556, 0.6384889, 0.63893336, 0.63897777, 0.6379111, 0.63611114, 0.6355778, 0.6419778, 0.6392, 0.6389111, 0.6415333, 0.63953334, 0.63868886, 0.6379111, 0.63913333, 0.63766664, 0.6381556, 0.6368222, 0.6410667, 0.6410667, 0.6398444, 0.63795555, 0.63913333, 0.6403111, 0.6416, 0.6383333, 0.63766664, 0.63964444, 0.64206666, 0.6381556, 0.63857776, 0.6432, 0.6412, 0.64064443, 0.63824445, 0.63766664, 0.6410889, 0.63915557, 0.64075553, 0.6404667, 0.64093333, 0.64284444, 0.6405111, 0.63835555, 0.6406, 0.6419111, 0.6398, 0.64115554, 0.6413556, 0.6391111, 0.64213336, 0.64104444, 0.64022225, 0.6409111, 0.64262223, 0.6416444, 0.6382, 0.6426889, 0.64284444, 0.64493334, 0.63913333, 0.6441111, 0.6398, 0.6419111, 0.6436, 0.6432, 0.63942224, 0.64055556, 0.6418889, 0.6424222, 0.63997775, 0.6433111, 0.644, 0.64306664, 0.6451333, 0.6403555, 0.6419111, 0.64273334, 0.6437111, 0.6440667, 0.6418667, 0.6412889, 0.6440667, 0.6432, 0.64526665, 0.64357775, 0.64475554, 0.6430445, 0.64426666, 0.6438444, 0.6436889, 0.6428, 0.64213336, 0.641, 0.64446664, 0.6433778, 0.6418, 0.6429333, 0.6446667, 0.6440667, 0.6464, 0.6455111, 0.6448889, 0.6445111, 0.644, 0.6441778, 0.6432222, 0.6438, 0.64408886, 0.64713335, 0.6451111, 0.6422222, 0.64706665, 0.6412889, 0.64408886, 0.6448445, 0.6454222, 0.6473333, 0.6446, 0.6433778, 0.6444889, 0.6439111, 0.64522225, 0.6419111, 0.6449111, 0.64486665, 0.6442889, 0.6469778, 0.6426889, 0.6433333, 0.6464222, 0.6457555, 0.6475111, 0.6465333, 0.64537776, 0.6464, 0.6455778, 0.6430445, 0.64566666, 0.6456889, 0.6478, 0.6455333, 0.64435554, 0.64713335, 0.647, 0.64662224, 0.6469778, 0.6496222, 0.6471556, 0.6464667, 0.64364445, 0.64504445, 0.6447778, 0.6450222, 0.64857775, 0.64626664, 0.6459778, 0.64437777, 0.6447333, 0.6483333, 0.64633334, 0.64768887, 0.64606667, 0.6450667, 0.6474, 0.6499111, 0.6482, 0.6452, 0.64455557, 0.64555556, 0.6479333, 0.64588886, 0.645, 0.64797777, 0.64615554, 0.6454, 0.64804447, 0.647, 0.6514, 0.64791113, 0.64966667, 0.6441778, 0.64926666, 0.6470444, 0.6458, 0.64806664, 0.6482889, 0.6496889, 0.6482222, 0.6465778, 0.6495111, 0.6444889, 0.64757776, 0.64746666, 0.65224445, 0.64753336, 0.6496889, 0.6497333, 0.64537776, 0.6504222, 0.6500889, 0.6496, 0.65062225, 0.6485556, 0.6477778, 0.6486889, 0.64784443, 0.6482667, 0.6487333, 0.64831114, 0.6482, 0.6495778, 0.6486, 0.64651114, 0.64964443]
################# cnn_lstm_True Validation Accuracy =  [0.3258, 0.3998, 0.4076, 0.4294, 0.4372, 0.4422, 0.451, 0.4632, 0.4748, 0.477, 0.4832, 0.4968, 0.4994, 0.503, 0.5034, 0.5236, 0.526, 0.5234, 0.5166, 0.534, 0.5202, 0.5254, 0.5302, 0.5328, 0.5454, 0.5486, 0.5388, 0.5438, 0.5492, 0.5422, 0.545, 0.5508, 0.5528, 0.5472, 0.5574, 0.5572, 0.553, 0.555, 0.5528, 0.556, 0.5542, 0.5568, 0.5648, 0.5676, 0.557, 0.5638, 0.56, 0.554, 0.5686, 0.568, 0.5676, 0.5602, 0.5674, 0.5626, 0.568, 0.5692, 0.5686, 0.5644, 0.568, 0.5724, 0.5688, 0.5652, 0.5766, 0.5758, 0.572, 0.5648, 0.5664, 0.571, 0.5752, 0.5738, 0.5834, 0.5786, 0.5676, 0.5814, 0.5706, 0.5756, 0.5734, 0.5784, 0.5702, 0.5754, 0.5702, 0.5686, 0.5736, 0.5814, 0.5752, 0.586, 0.576, 0.5808, 0.5864, 0.5776, 0.5764, 0.5796, 0.5734, 0.5688, 0.584, 0.583, 0.585, 0.5692, 0.5818, 0.5878, 0.582, 0.572, 0.5874, 0.5854, 0.5942, 0.5814, 0.5964, 0.5848, 0.5852, 0.5888, 0.5876, 0.5818, 0.5832, 0.5856, 0.584, 0.5798, 0.5872, 0.584, 0.5842, 0.5848, 0.5848, 0.5834, 0.5856, 0.5892, 0.5848, 0.5842, 0.5838, 0.5784, 0.5748, 0.5848, 0.5836, 0.5878, 0.5872, 0.5864, 0.5798, 0.5838, 0.5802, 0.588, 0.5904, 0.5854, 0.5834, 0.5856, 0.5928, 0.5916, 0.581, 0.5816, 0.5878, 0.5796, 0.5932, 0.584, 0.5938, 0.582, 0.5874, 0.5892, 0.5864, 0.583, 0.576, 0.5912, 0.5932, 0.5944, 0.5894, 0.5892, 0.5954, 0.5874, 0.5882, 0.5954, 0.591, 0.5912, 0.5826, 0.5888, 0.597, 0.594, 0.587, 0.5894, 0.5848, 0.5982, 0.5968, 0.5878, 0.5898, 0.5808, 0.5876, 0.5808, 0.5844, 0.5944, 0.5844, 0.5932, 0.5884, 0.594, 0.5948, 0.5848, 0.5964, 0.5794, 0.5872, 0.5864, 0.5858, 0.5858, 0.5912, 0.5888, 0.5924, 0.5912, 0.599, 0.5954, 0.5854, 0.5938, 0.591, 0.5896, 0.5952, 0.5858, 0.597, 0.585, 0.5852, 0.5906, 0.5926, 0.5814, 0.592, 0.589, 0.587, 0.5938, 0.5938, 0.592, 0.596, 0.5954, 0.587, 0.596, 0.586, 0.5954, 0.5908, 0.5916, 0.5946, 0.5874, 0.5982, 0.5922, 0.5972, 0.586, 0.5942, 0.5898, 0.5978, 0.5988, 0.5882, 0.5942, 0.5962, 0.5922, 0.5926, 0.591, 0.594, 0.5892, 0.587, 0.5884, 0.591, 0.5926, 0.5926, 0.5924, 0.5874, 0.593, 0.5952, 0.5902, 0.5958, 0.5832, 0.5942, 0.588, 0.5954, 0.59, 0.5908, 0.5888, 0.5976, 0.5936, 0.5938, 0.5904, 0.5988, 0.585, 0.5942, 0.5938, 0.5988, 0.5934, 0.5998, 0.5958, 0.5994, 0.5922, 0.5904, 0.5836, 0.5914, 0.589, 0.5942, 0.5918, 0.5976, 0.5876, 0.596, 0.602, 0.5864, 0.5852, 0.5938, 0.5944, 0.5908, 0.598, 0.5916, 0.5886, 0.5916, 0.5852, 0.58, 0.583, 0.5928, 0.5916, 0.5908, 0.5952, 0.5858, 0.5918, 0.5934, 0.5976, 0.586, 0.5906, 0.583, 0.5986, 0.5856, 0.5886, 0.5932, 0.5938, 0.5918, 0.5936, 0.5848, 0.5924, 0.5922, 0.5926, 0.589, 0.5928, 0.595, 0.5888, 0.5932, 0.5898, 0.5838, 0.5842, 0.5976, 0.5918, 0.5936, 0.593, 0.593, 0.5844, 0.5918, 0.5986, 0.6016, 0.5896, 0.5988, 0.601, 0.5956, 0.5932, 0.5904, 0.5974, 0.5862, 0.6016, 0.5966, 0.5908, 0.5886, 0.5918, 0.5906, 0.5944, 0.5902, 0.591, 0.5868, 0.5924, 0.5934, 0.5946, 0.596, 0.5918, 0.597, 0.5868, 0.5882, 0.5834, 0.5856, 0.5898, 0.5934, 0.5862, 0.5892, 0.5928, 0.5902, 0.592, 0.59, 0.5844, 0.5836, 0.5864, 0.5894, 0.5912, 0.5932, 0.5854, 0.5896, 0.593, 0.5864, 0.6004, 0.5906, 0.5868, 0.5984, 0.5912, 0.5892, 0.596, 0.592, 0.5972, 0.5964, 0.5996, 0.5936, 0.5958, 0.5942, 0.5904, 0.5966, 0.5952, 0.5882, 0.5966, 0.5958, 0.5948, 0.5932, 0.6024, 0.6, 0.5972, 0.5968, 0.5954, 0.595, 0.595, 0.5944, 0.5952, 0.5952, 0.6006, 0.597, 0.5948, 0.59, 0.5936, 0.5916, 0.5946, 0.5984, 0.5914, 0.5988, 0.5964, 0.5908, 0.5906, 0.593, 0.5894, 0.5938, 0.5916, 0.5916, 0.5908, 0.5994, 0.594, 0.5926, 0.5946, 0.601, 0.5966, 0.5992, 0.6, 0.5968, 0.5948, 0.591, 0.5972, 0.5952, 0.595, 0.591, 0.5948, 0.5956, 0.5956, 0.5932, 0.5962, 0.5992, 0.6028, 0.5988, 0.5962, 0.6004, 0.5978, 0.5924, 0.5922, 0.5952, 0.5982, 0.604, 0.5998, 0.6052, 0.5932, 0.602, 0.6012, 0.5986, 0.604, 0.5932, 0.5916, 0.5932, 0.5926, 0.5972, 0.5916, 0.5996, 0.5984, 0.5954, 0.5992, 0.6088, 0.5998, 0.5956, 0.5982, 0.5908, 0.5972, 0.5966, 0.5936, 0.5864, 0.5968, 0.587, 0.5912, 0.5936, 0.594, 0.605, 0.6]
################# cnn_lstm_True Training Accuracy =  [0.24584444, 0.35053334, 0.38793334, 0.40875554, 0.4229111, 0.4331111, 0.44337776, 0.44886667, 0.45893332, 0.46524444, 0.47291112, 0.47626665, 0.4856889, 0.48975554, 0.4916, 0.4988, 0.5033111, 0.5056889, 0.5113556, 0.5152444, 0.5168222, 0.5198889, 0.5234889, 0.52444446, 0.5282889, 0.5336889, 0.53297776, 0.5376889, 0.54246664, 0.53664446, 0.5434, 0.5458, 0.5467333, 0.5486444, 0.5535111, 0.5506222, 0.5524667, 0.55295557, 0.55697775, 0.5574667, 0.5597111, 0.5593111, 0.56273335, 0.56615555, 0.56457776, 0.5658889, 0.56504446, 0.56744444, 0.5668667, 0.5719111, 0.56886667, 0.57093334, 0.5731556, 0.5764889, 0.5733111, 0.5746889, 0.5784889, 0.57868886, 0.5796667, 0.57868886, 0.58095556, 0.5823333, 0.5834444, 0.58566666, 0.5847778, 0.5849111, 0.58408886, 0.5852889, 0.58735555, 0.58813334, 0.58802223, 0.5897111, 0.5891778, 0.59106666, 0.58977777, 0.59164447, 0.5935778, 0.59155554, 0.5924444, 0.5958, 0.5974889, 0.5984222, 0.59635556, 0.59797776, 0.5967111, 0.59882224, 0.59864444, 0.59995556, 0.59793335, 0.6014, 0.5981111, 0.60044444, 0.60055554, 0.60208887, 0.6009778, 0.60113335, 0.6046, 0.6018889, 0.60624444, 0.6012, 0.60373336, 0.6020667, 0.60388887, 0.60364443, 0.60344446, 0.60855556, 0.6080667, 0.6056, 0.6059333, 0.61097777, 0.6092, 0.6079778, 0.6118444, 0.6070667, 0.6104, 0.60895556, 0.6101556, 0.6104444, 0.61002225, 0.6122, 0.6102222, 0.6121778, 0.61095554, 0.6102667, 0.61075556, 0.61311114, 0.6121778, 0.6160667, 0.61502224, 0.61444443, 0.61435556, 0.61344445, 0.61355555, 0.6150444, 0.6168444, 0.6152889, 0.6140444, 0.6184222, 0.61513335, 0.6177111, 0.61388886, 0.61653334, 0.61455554, 0.61595553, 0.62053335, 0.61826664, 0.6192667, 0.62093335, 0.6173556, 0.61895555, 0.61726665, 0.6194222, 0.6232, 0.62115556, 0.6197111, 0.62035555, 0.61844444, 0.62042224, 0.618, 0.6223556, 0.61902225, 0.62288886, 0.6220222, 0.6221778, 0.6239111, 0.6190889, 0.6250889, 0.6246, 0.6218, 0.62484443, 0.6250889, 0.6202889, 0.6244889, 0.62333333, 0.62457776, 0.6242, 0.6236, 0.6252, 0.6274667, 0.6245555, 0.6241111, 0.6262, 0.6253778, 0.6244444, 0.6254, 0.6255556, 0.6254, 0.62355554, 0.62986666, 0.6236445, 0.6255556, 0.62682223, 0.6252222, 0.6265778, 0.62684447, 0.62684447, 0.6264667, 0.6289333, 0.63042223, 0.62946665, 0.62873334, 0.6296889, 0.6269111, 0.6298222, 0.62975556, 0.6280889, 0.6296667, 0.6295556, 0.62873334, 0.6278222, 0.6266222, 0.6314222, 0.63211113, 0.62813336, 0.6260667, 0.6296889, 0.6291111, 0.63251114, 0.62813336, 0.63071114, 0.62946665, 0.6320889, 0.62913334, 0.63177776, 0.6306667, 0.63008887, 0.6332222, 0.6346, 0.63104445, 0.6309111, 0.6326, 0.6289333, 0.6310667, 0.6288889, 0.6339333, 0.63155556, 0.6330889, 0.6311111, 0.634, 0.63233334, 0.6316222, 0.63533336, 0.6349556, 0.63482225, 0.63224447, 0.6336667, 0.6331556, 0.6326, 0.63533336, 0.63024443, 0.6355778, 0.6332222, 0.6337111, 0.6360889, 0.6368222, 0.6360889, 0.6338222, 0.6360222, 0.63537776, 0.6355111, 0.6344444, 0.63255554, 0.634, 0.6377778, 0.6331111, 0.63493335, 0.63935554, 0.63553333, 0.63537776, 0.6352889, 0.63637775, 0.6369111, 0.6370889, 0.6368, 0.6359556, 0.6358889, 0.6383111, 0.6369111, 0.6352889, 0.6394445, 0.6378667, 0.63688886, 0.6374889, 0.6386222, 0.63902223, 0.6359556, 0.6384889, 0.63893336, 0.63897777, 0.6379111, 0.63611114, 0.6355778, 0.6419778, 0.6392, 0.6389111, 0.6415333, 0.63953334, 0.63868886, 0.6379111, 0.63913333, 0.63766664, 0.6381556, 0.6368222, 0.6410667, 0.6410667, 0.6398444, 0.63795555, 0.63913333, 0.6403111, 0.6416, 0.6383333, 0.63766664, 0.63964444, 0.64206666, 0.6381556, 0.63857776, 0.6432, 0.6412, 0.64064443, 0.63824445, 0.63766664, 0.6410889, 0.63915557, 0.64075553, 0.6404667, 0.64093333, 0.64284444, 0.6405111, 0.63835555, 0.6406, 0.6419111, 0.6398, 0.64115554, 0.6413556, 0.6391111, 0.64213336, 0.64104444, 0.64022225, 0.6409111, 0.64262223, 0.6416444, 0.6382, 0.6426889, 0.64284444, 0.64493334, 0.63913333, 0.6441111, 0.6398, 0.6419111, 0.6436, 0.6432, 0.63942224, 0.64055556, 0.6418889, 0.6424222, 0.63997775, 0.6433111, 0.644, 0.64306664, 0.6451333, 0.6403555, 0.6419111, 0.64273334, 0.6437111, 0.6440667, 0.6418667, 0.6412889, 0.6440667, 0.6432, 0.64526665, 0.64357775, 0.64475554, 0.6430445, 0.64426666, 0.6438444, 0.6436889, 0.6428, 0.64213336, 0.641, 0.64446664, 0.6433778, 0.6418, 0.6429333, 0.6446667, 0.6440667, 0.6464, 0.6455111, 0.6448889, 0.6445111, 0.644, 0.6441778, 0.6432222, 0.6438, 0.64408886, 0.64713335, 0.6451111, 0.6422222, 0.64706665, 0.6412889, 0.64408886, 0.6448445, 0.6454222, 0.6473333, 0.6446, 0.6433778, 0.6444889, 0.6439111, 0.64522225, 0.6419111, 0.6449111, 0.64486665, 0.6442889, 0.6469778, 0.6426889, 0.6433333, 0.6464222, 0.6457555, 0.6475111, 0.6465333, 0.64537776, 0.6464, 0.6455778, 0.6430445, 0.64566666, 0.6456889, 0.6478, 0.6455333, 0.64435554, 0.64713335, 0.647, 0.64662224, 0.6469778, 0.6496222, 0.6471556, 0.6464667, 0.64364445, 0.64504445, 0.6447778, 0.6450222, 0.64857775, 0.64626664, 0.6459778, 0.64437777, 0.6447333, 0.6483333, 0.64633334, 0.64768887, 0.64606667, 0.6450667, 0.6474, 0.6499111, 0.6482, 0.6452, 0.64455557, 0.64555556, 0.6479333, 0.64588886, 0.645, 0.64797777, 0.64615554, 0.6454, 0.64804447, 0.647, 0.6514, 0.64791113, 0.64966667, 0.6441778, 0.64926666, 0.6470444, 0.6458, 0.64806664, 0.6482889, 0.6496889, 0.6482222, 0.6465778, 0.6495111, 0.6444889, 0.64757776, 0.64746666, 0.65224445, 0.64753336, 0.6496889, 0.6497333, 0.64537776, 0.6504222, 0.6500889, 0.6496, 0.65062225, 0.6485556, 0.6477778, 0.6486889, 0.64784443, 0.6482667, 0.6487333, 0.64831114, 0.6482, 0.6495778, 0.6486, 0.64651114, 0.64964443]

with cnn_sropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 500 epochs out.813128
################# cnn_lstm_True Validation Accuracy =  [0.3506, 0.4258, 0.437, 0.4526, 0.4624, 0.462, 0.49, 0.5016, 0.5074, 0.5234, 0.5178, 0.523, 0.5234, 0.5174, 0.5366, 0.5146, 0.545, 0.539, 0.5384, 0.5426, 0.546, 0.5574, 0.5508, 0.5322, 0.5694, 0.549, 0.563, 0.5702, 0.5606, 0.5674, 0.5814, 0.5678, 0.564, 0.5678, 0.5648, 0.5654, 0.5846, 0.5776, 0.5796, 0.5712, 0.591, 0.5772, 0.5746, 0.592, 0.587, 0.5906, 0.592, 0.5766, 0.5812, 0.5778, 0.5878, 0.5786, 0.5934, 0.594, 0.5948, 0.5892, 0.5896, 0.5884, 0.5826, 0.5834, 0.5848, 0.5862, 0.5932, 0.5956, 0.598, 0.5874, 0.5996, 0.5958, 0.5862, 0.5734, 0.5808, 0.5898, 0.5836, 0.599, 0.5804, 0.5862, 0.5952, 0.5968, 0.5952, 0.5926, 0.5896, 0.5932, 0.5944, 0.5842, 0.5878, 0.5882, 0.5846, 0.5954, 0.5968, 0.5872, 0.588, 0.6004, 0.5896, 0.5984, 0.5936, 0.5972, 0.5958, 0.5916, 0.5908, 0.5922, 0.5856, 0.598, 0.5912, 0.5946, 0.5886, 0.5952, 0.5884, 0.5944, 0.5968, 0.5792, 0.5984, 0.6004, 0.5882, 0.5906, 0.583, 0.6026, 0.5956, 0.5998, 0.5966, 0.5888, 0.591, 0.5936, 0.5928, 0.594, 0.6046, 0.5994, 0.5874, 0.589, 0.5996, 0.597, 0.599, 0.5946, 0.5896, 0.5952, 0.5892, 0.6004, 0.5942, 0.5884, 0.6062, 0.5984, 0.6058, 0.5998, 0.6006, 0.6016, 0.591, 0.5888, 0.578, 0.586, 0.5946, 0.5878, 0.5936, 0.5892, 0.5956, 0.5926, 0.6004, 0.5898, 0.5922, 0.5952, 0.5988, 0.5882, 0.593, 0.5928, 0.6006, 0.5946, 0.5946, 0.5988, 0.5986, 0.5928, 0.5908, 0.591, 0.602, 0.5976, 0.5986, 0.5966, 0.5898, 0.5978, 0.59, 0.5944, 0.5962, 0.6016, 0.5942, 0.5966, 0.5926, 0.5918, 0.5956, 0.5904, 0.5856, 0.6024, 0.5896, 0.5936, 0.5952, 0.6006, 0.5866, 0.5988, 0.5912, 0.6016, 0.5882, 0.5984, 0.6006, 0.6034, 0.5936, 0.603, 0.599, 0.602, 0.5978, 0.5914, 0.5958, 0.6014, 0.6006, 0.5844, 0.5888, 0.6066, 0.5986, 0.5916, 0.5946, 0.5972, 0.5948, 0.5968, 0.599, 0.6024, 0.594, 0.5946, 0.5978, 0.5954, 0.5934, 0.5964, 0.5946, 0.5938, 0.5886, 0.5948, 0.6006, 0.5954, 0.6036, 0.6, 0.601, 0.5994, 0.5932, 0.6024, 0.5894, 0.5936, 0.5906, 0.6042, 0.6076, 0.599, 0.598, 0.6006, 0.5958, 0.5972, 0.5964, 0.593, 0.5986, 0.5982, 0.5952, 0.5984, 0.5856, 0.5946, 0.6052, 0.6022, 0.5936, 0.6028, 0.5934, 0.5954, 0.5962, 0.6054, 0.6006, 0.5992, 0.5954, 0.5974, 0.605, 0.593, 0.6052, 0.5942, 0.6002, 0.6026, 0.5936, 0.6034, 0.5864, 0.6016, 0.5956, 0.6008, 0.5974, 0.6104, 0.6052, 0.6106, 0.6038, 0.6064, 0.5978, 0.5994, 0.5972, 0.6048, 0.6022, 0.6022, 0.6016, 0.5902, 0.6144, 0.6008, 0.6028, 0.6026, 0.5988, 0.6028, 0.603, 0.5952, 0.5992, 0.5926, 0.6, 0.5978, 0.6028, 0.5992, 0.5994, 0.5946, 0.6006, 0.6056, 0.6002, 0.6022, 0.6024, 0.6008, 0.5994, 0.5996, 0.5966, 0.6026, 0.6014, 0.601, 0.596, 0.5976, 0.5968, 0.603, 0.5968, 0.5982, 0.5998, 0.6012, 0.605, 0.6052, 0.6, 0.6084, 0.601, 0.604, 0.5938, 0.6006, 0.6022, 0.598, 0.597, 0.6066, 0.6022, 0.5992, 0.5964, 0.5956, 0.6026, 0.5998, 0.6074, 0.6016, 0.6036, 0.613, 0.596, 0.6, 0.6016, 0.5972, 0.6012, 0.6, 0.6074, 0.601, 0.6026, 0.6058, 0.6018, 0.6032, 0.5986, 0.6048, 0.6034, 0.6066, 0.6078, 0.6066, 0.5954, 0.6016, 0.5984, 0.5944, 0.6018, 0.6016, 0.603, 0.599, 0.6038, 0.591, 0.6014, 0.5974, 0.5966, 0.6004, 0.5954, 0.5908, 0.5892, 0.5892, 0.6, 0.593, 0.5986, 0.6034, 0.5994, 0.5988, 0.606, 0.6068, 0.6006, 0.6052, 0.5976, 0.5958, 0.5918, 0.5974, 0.5984, 0.6052, 0.6046, 0.601, 0.6138, 0.5994, 0.6016, 0.5964, 0.6054, 0.6004, 0.5936, 0.6012, 0.5992, 0.5976, 0.5868, 0.5986, 0.5872, 0.5934, 0.5958, 0.5954, 0.5932, 0.6008, 0.5954, 0.5902, 0.6016, 0.5998, 0.5956, 0.597, 0.5914, 0.5964, 0.6022, 0.5962, 0.593, 0.5982, 0.5926, 0.5968, 0.6006, 0.5952, 0.596, 0.5908, 0.6022, 0.5952, 0.6026, 0.599, 0.5986, 0.598, 0.5972, 0.602, 0.6014, 0.6022, 0.598, 0.6014, 0.5994, 0.5984, 0.602, 0.5976, 0.5946, 0.5998, 0.6026, 0.604, 0.598, 0.5948, 0.602, 0.5988, 0.5974, 0.6036, 0.5964, 0.599, 0.598, 0.5992, 0.5984, 0.601, 0.5956, 0.6012, 0.6014, 0.6034, 0.6004, 0.6004, 0.598, 0.5976, 0.6028, 0.5954, 0.598, 0.5976, 0.5932, 0.5944, 0.6046, 0.602, 0.6006, 0.6, 0.6036, 0.604, 0.6058, 0.5958, 0.6002, 0.6036, 0.6052, 0.6044]
################# cnn_lstm_True Training Accuracy =  [0.26002222, 0.37235555, 0.41088888, 0.43533334, 0.45157778, 0.46348888, 0.47482222, 0.48466668, 0.49411112, 0.4982, 0.5069111, 0.51173335, 0.5198445, 0.52104443, 0.5298667, 0.5331111, 0.5375556, 0.5388889, 0.5466, 0.54815555, 0.54928887, 0.5550445, 0.5568445, 0.56435555, 0.5647333, 0.56446666, 0.56495553, 0.5708, 0.5726, 0.57673335, 0.57835555, 0.57893336, 0.5798889, 0.58257776, 0.58442223, 0.58875555, 0.5867556, 0.59044445, 0.58846664, 0.5932889, 0.5962, 0.5942889, 0.5940667, 0.5989111, 0.60044444, 0.60028887, 0.6003778, 0.60044444, 0.60286665, 0.60444444, 0.60675555, 0.6076222, 0.6081111, 0.6105111, 0.6144889, 0.61135554, 0.6096, 0.614, 0.61671114, 0.61568886, 0.6184667, 0.61675555, 0.61737776, 0.62513334, 0.6207111, 0.62144446, 0.62255555, 0.6216, 0.623, 0.6256222, 0.62593335, 0.6261333, 0.6252889, 0.6291111, 0.62855554, 0.6308445, 0.6297111, 0.6308889, 0.63173336, 0.63395554, 0.6346, 0.63195556, 0.6347333, 0.63453335, 0.6356889, 0.63533336, 0.6365778, 0.6391778, 0.63802224, 0.6388, 0.63924444, 0.6368222, 0.64228886, 0.64086664, 0.6418222, 0.642, 0.6408, 0.6379778, 0.6434444, 0.6421111, 0.63993335, 0.64295554, 0.64522225, 0.64635557, 0.64482224, 0.64753336, 0.64824444, 0.64795554, 0.6487778, 0.64864445, 0.6474, 0.6500222, 0.6481111, 0.65037775, 0.647, 0.6498889, 0.6495111, 0.6509111, 0.6482, 0.65184444, 0.6502444, 0.64982224, 0.6518667, 0.6504889, 0.6533333, 0.6520889, 0.6557556, 0.6536889, 0.6522222, 0.65475553, 0.6579111, 0.6558889, 0.6580667, 0.6559111, 0.65533334, 0.65782225, 0.65662223, 0.6535111, 0.6564222, 0.6553556, 0.65855557, 0.65846664, 0.6594, 0.6558222, 0.6600222, 0.6581778, 0.6584, 0.6606445, 0.6607111, 0.65933335, 0.6636889, 0.65875554, 0.6593556, 0.6616222, 0.66224444, 0.6608222, 0.6646889, 0.6640222, 0.6595111, 0.66215557, 0.66393334, 0.6637333, 0.6625556, 0.66044444, 0.6639111, 0.66591114, 0.66595554, 0.66573334, 0.66591114, 0.6688667, 0.66573334, 0.6650444, 0.6655333, 0.6683111, 0.6671778, 0.66642225, 0.66826665, 0.66855556, 0.67113334, 0.6687111, 0.6679111, 0.66804445, 0.66844445, 0.6673333, 0.66977775, 0.6693556, 0.6670222, 0.6693556, 0.66744447, 0.66911113, 0.66915554, 0.66844445, 0.6711556, 0.6716667, 0.6711111, 0.672, 0.6707111, 0.67113334, 0.67064446, 0.67104447, 0.6739778, 0.67255557, 0.6720222, 0.67073333, 0.67271113, 0.67233336, 0.673, 0.6721778, 0.6762, 0.6724, 0.6713333, 0.6718, 0.67208886, 0.67642224, 0.6724667, 0.6754889, 0.6734, 0.67311114, 0.6744222, 0.6762, 0.6722444, 0.6767333, 0.6790444, 0.67377776, 0.6742667, 0.6772, 0.67646664, 0.67455554, 0.6768, 0.67242223, 0.6748889, 0.67646664, 0.6785333, 0.6776, 0.67333335, 0.6766667, 0.67704445, 0.67653334, 0.67542225, 0.6809555, 0.6784889, 0.6774222, 0.6812, 0.6820889, 0.6766, 0.67631114, 0.6772444, 0.67962223, 0.67855555, 0.68017775, 0.67906666, 0.67811114, 0.6754, 0.67957777, 0.6800889, 0.6798, 0.6809555, 0.6788222, 0.67866665, 0.6785333, 0.6772889, 0.68035555, 0.6809111, 0.6808889, 0.6816222, 0.68, 0.6821778, 0.67877775, 0.68024445, 0.6807778, 0.68002224, 0.67913336, 0.68306667, 0.68075556, 0.68186665, 0.6824889, 0.67917776, 0.68222225, 0.6826889, 0.6804889, 0.6809555, 0.68197775, 0.68146664, 0.68237776, 0.68277776, 0.67984444, 0.68002224, 0.6838222, 0.6838889, 0.6868, 0.6828667, 0.6806, 0.68124443, 0.68237776, 0.6806667, 0.6828667, 0.684, 0.6830889, 0.6838, 0.68197775, 0.6838889, 0.6784667, 0.6829778, 0.6866889, 0.6834222, 0.6838222, 0.68644446, 0.68351114, 0.6870222, 0.6800889, 0.68813336, 0.6838222, 0.6875333, 0.68633336, 0.6861778, 0.685, 0.6834667, 0.6867333, 0.68486667, 0.6841111, 0.6859111, 0.6858, 0.68597776, 0.6850889, 0.6895111, 0.68684447, 0.6862222, 0.6888222, 0.68813336, 0.6896222, 0.68586665, 0.68484443, 0.6884889, 0.68453336, 0.68575555, 0.68682224, 0.68693334, 0.6864222, 0.6860667, 0.6869556, 0.6881111, 0.6865111, 0.6879111, 0.68653333, 0.6862889, 0.69204444, 0.68648887, 0.6857778, 0.6902, 0.68873334, 0.6896667, 0.6882, 0.68582225, 0.6904, 0.6892667, 0.6860667, 0.6887778, 0.68906665, 0.6867333, 0.68957776, 0.69082224, 0.68986666, 0.6870222, 0.6892667, 0.6920222, 0.68953335, 0.6888222, 0.6920222, 0.6873556, 0.6916222, 0.6893778, 0.68968886, 0.6890889, 0.6911778, 0.6907333, 0.6916222, 0.6887556, 0.69375557, 0.6906222, 0.6885333, 0.68995553, 0.69233334, 0.6880889, 0.69166666, 0.69097775, 0.69082224, 0.69184446, 0.69002223, 0.69075555, 0.6938222, 0.69346666, 0.69137776, 0.6933333, 0.69233334, 0.69206667, 0.68997777, 0.6902222, 0.6944444, 0.69255555, 0.6926, 0.6915778, 0.69164443, 0.6934, 0.6918, 0.69435555, 0.6911333, 0.693, 0.69226664, 0.6914222, 0.69386667, 0.6925111, 0.6921333, 0.6954, 0.69355553, 0.69402224, 0.69406664, 0.69328886, 0.69211113, 0.69317776, 0.69086665, 0.6944444, 0.6914667, 0.6902222, 0.69533336, 0.69593334, 0.69306666, 0.69355553, 0.693, 0.6969333, 0.69064444, 0.69548887, 0.69442225, 0.69406664, 0.6944444, 0.6964889, 0.6958445, 0.6977556, 0.69424444, 0.69497776, 0.69526666, 0.6947333, 0.69777775, 0.6968222, 0.6994889, 0.6950667, 0.6911111, 0.6942222, 0.6976445, 0.69586664, 0.6967111, 0.6934, 0.6963556, 0.69366664, 0.6979333, 0.69677776, 0.6964889, 0.6929333, 0.6951333, 0.6950222, 0.6978667, 0.6924222, 0.69644445, 0.6926889, 0.6948, 0.6967111, 0.69884443, 0.69975555, 0.69802225, 0.69626665, 0.69817775, 0.6986222, 0.6982, 0.697, 0.6974667, 0.6984444, 0.6949111, 0.6966, 0.69615555, 0.69735557, 0.6937111, 0.6951333, 0.69906664, 0.69515556, 0.6963556, 0.6979778, 0.69582224, 0.697, 0.6959111, 0.69817775, 0.69688886, 0.6994889, 0.6986, 0.6960222, 0.6968, 0.69942224, 0.6992, 0.6955111, 0.70015556, 0.69395554, 0.69688886]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 5 samples and 200 epochs, hs = 256 out.836808

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 5 samples and 500 epochs, hs = 256 out.850568

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 200 epochs, hs = 256 out.846535
################# cnn_lstm_True Validation Accuracy =  [0.339, 0.3814, 0.4168, 0.4498, 0.453, 0.4722, 0.4766, 0.4924, 0.4932, 0.5042, 0.5004, 0.5084, 0.4992, 0.5072, 0.5184, 0.5258, 0.5286, 0.5358, 0.5306, 0.5372, 0.549, 0.5322, 0.55, 0.5528, 0.5488, 0.535, 0.555, 0.5514, 0.5558, 0.5496, 0.557, 0.5474, 0.5448, 0.5578, 0.5644, 0.5724, 0.5662, 0.5638, 0.5668, 0.5534, 0.5596, 0.56, 0.564, 0.5706, 0.5672, 0.5678, 0.5798, 0.5744, 0.5696, 0.5624, 0.5772, 0.5702, 0.5746, 0.5762, 0.5668, 0.5714, 0.5772, 0.5698, 0.5736, 0.5798, 0.5808, 0.5746, 0.5758, 0.578, 0.5758, 0.5756, 0.5728, 0.5818, 0.5848, 0.5808, 0.585, 0.5786, 0.5762, 0.572, 0.5806, 0.5826, 0.5728, 0.578, 0.5782, 0.5678, 0.5822, 0.5804, 0.5814, 0.584, 0.5852, 0.5842, 0.5844, 0.578, 0.5828, 0.583, 0.5746, 0.5766, 0.5778, 0.5736, 0.5686, 0.5838, 0.5718, 0.5912, 0.5802, 0.5782, 0.5892, 0.589, 0.586, 0.5764, 0.5836, 0.5846, 0.5784, 0.5856, 0.5924, 0.5794, 0.5812, 0.5802, 0.577, 0.574, 0.5758, 0.574, 0.5876, 0.5826, 0.576, 0.5904, 0.5784, 0.59, 0.5798, 0.5762, 0.587, 0.5818, 0.577, 0.584, 0.5872, 0.5818, 0.5798, 0.579, 0.5792, 0.5846, 0.59, 0.584, 0.581, 0.5812, 0.5834, 0.5798, 0.582, 0.5838, 0.5844, 0.575, 0.5864, 0.5876, 0.5784, 0.589, 0.5796, 0.5782, 0.5838, 0.5784, 0.5828, 0.5776, 0.5878, 0.5914, 0.5878, 0.584, 0.584, 0.5786, 0.5874, 0.5874, 0.5878, 0.584, 0.5854, 0.5878, 0.5788, 0.586, 0.5828, 0.5814, 0.5868, 0.5838, 0.5794, 0.5738, 0.5678, 0.584, 0.5646, 0.5784, 0.5874, 0.5862, 0.5814, 0.576, 0.579, 0.5842, 0.5866, 0.5828, 0.5788, 0.5824, 0.5784, 0.5804, 0.5918, 0.583, 0.5838, 0.581, 0.5772, 0.5854, 0.579, 0.5856, 0.5838, 0.5684]
################# cnn_lstm_True Training Accuracy =  [0.25933334, 0.35897776, 0.40086666, 0.41968888, 0.43133333, 0.4426222, 0.45602223, 0.464, 0.4725111, 0.4824, 0.48344445, 0.4886889, 0.4959778, 0.50024444, 0.5028222, 0.5082222, 0.50993335, 0.51266664, 0.51886666, 0.52084446, 0.52646667, 0.52404445, 0.5317111, 0.5313778, 0.5377778, 0.5391778, 0.5374889, 0.5467333, 0.54455554, 0.54733336, 0.54833335, 0.5536, 0.5534667, 0.5556222, 0.559, 0.5606889, 0.5632222, 0.56493336, 0.56306666, 0.5652889, 0.57233334, 0.57015556, 0.5712, 0.57144445, 0.57442224, 0.57515556, 0.57857776, 0.5813778, 0.5827111, 0.5814, 0.5822, 0.58335555, 0.58815557, 0.5899111, 0.5893111, 0.5896889, 0.5938889, 0.59315556, 0.59037775, 0.5937778, 0.59675556, 0.5979556, 0.60024446, 0.60231113, 0.5999111, 0.6032889, 0.6035333, 0.59848887, 0.60015553, 0.60568887, 0.6070222, 0.6063333, 0.6086889, 0.609, 0.6100444, 0.608, 0.61284447, 0.61013335, 0.6125778, 0.61417776, 0.6145778, 0.61653334, 0.61782223, 0.6174, 0.61873335, 0.6196667, 0.61877775, 0.6184667, 0.6224222, 0.6238222, 0.6217333, 0.6231111, 0.62384444, 0.6241111, 0.62593335, 0.62633336, 0.6242889, 0.62824446, 0.6285333, 0.62684447, 0.6272, 0.6290889, 0.6309111, 0.62942225, 0.6313556, 0.6296889, 0.63328886, 0.6348444, 0.63564444, 0.6356, 0.6346667, 0.63733333, 0.6368222, 0.63895553, 0.6392, 0.6382889, 0.6387778, 0.6429778, 0.6417111, 0.6396222, 0.6428, 0.6437333, 0.64342225, 0.6436889, 0.64651114, 0.6487778, 0.64806664, 0.6462, 0.64526665, 0.64604443, 0.64802223, 0.6433111, 0.6487333, 0.6496222, 0.64915556, 0.6493111, 0.6515778, 0.65037775, 0.6518667, 0.65433335, 0.65011114, 0.65484446, 0.6539111, 0.65246665, 0.6572222, 0.65664446, 0.6535778, 0.65617776, 0.65826666, 0.6585778, 0.6563778, 0.6543111, 0.6588889, 0.6595111, 0.66033334, 0.65742224, 0.66184443, 0.6596, 0.6606445, 0.66444445, 0.66106665, 0.6597111, 0.6607556, 0.6629111, 0.6594889, 0.6645333, 0.66524446, 0.6639778, 0.66444445, 0.66444445, 0.669, 0.6661111, 0.6661111, 0.6684667, 0.66915554, 0.66746664, 0.66884446, 0.6691333, 0.6706222, 0.66966665, 0.6698222, 0.67057776, 0.66962224, 0.6712889, 0.67204446, 0.6702222, 0.67388886, 0.6762889, 0.6710889, 0.67404443, 0.67264444, 0.67275554, 0.6758222, 0.6774222, 0.6727778, 0.6743111, 0.67755556, 0.6782, 0.6754, 0.6779778]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 500 epochs, hs = 256 out.850588

'''

from __future__ import division, print_function, absolute_import

print('Starting..................................')
import sys
sys.path.insert(1, '/home/labs/ahissarlab/orra/imagewalker')
import numpy as np
import cv2
import misc
import pandas as pd
import matplotlib.pyplot as plt
import pickle

from keras_utils import *
from misc import *

import tensorflow.keras as keras
import tensorflow as tf

from tensorflow.keras.datasets import cifar10

# load dataset
(trainX, trainy), (testX, testy) = cifar10.load_data()
images, labels = trainX, trainy


#Define function for low resolution lens on syclop
def bad_res101(img,res):
    sh=np.shape(img)
    dwnsmp=cv2.resize(img,res, interpolation = cv2.INTER_CUBIC)
    upsmp = cv2.resize(dwnsmp,sh[:2], interpolation = cv2.INTER_CUBIC)
    return upsmp

def bad_res102(img,res):
    sh=np.shape(img)
    dwnsmp=cv2.resize(img,res, interpolation = cv2.INTER_AREA)
    return dwnsmp

# import importlib
# importlib.reload(misc)
# from misc import Logger
import os 


# def deploy_logs():
#     if not os.path.exists(hp.save_path):
#         os.makedirs(hp.save_path)

#     dir_success = False
#     for sfx in range(1):  # todo legacy
#         candidate_path = hp.save_path + '/' + hp.this_run_name + '_' + str(os.getpid()) + '/'
#         if not os.path.exists(candidate_path):
#             hp.this_run_path = candidate_path
#             os.makedirs(hp.this_run_path)
#             dir_success = Truecnn_net = cnn_one_img(n_timesteps = sample, input_size = 28, input_dim = 1)
#             break
#     if not dir_success:
#         error('run name already exists!')

#     sys.stdout = Logger(hp.this_run_path+'log.log')
#     print('results are in:', hp.this_run_path)
#     print('description: ', hp.description)
    #print('hyper-parameters (partial):', hp.dict)

epochs = int(sys.argv[1])

sample = int(sys.argv[2])

res = int(sys.argv[3])

hidden_size = int(sys.argv[4])
   
cnn_dropout = 0.4

rnn_dropout = 0.2

n_timesteps = sample
def split_dataset_xy(dataset):
    dataset_x1 = [uu[0] for uu in dataset]
    dataset_x2 = [uu[1] for uu in dataset]
    dataset_y = [uu[-1] for uu in dataset]
    return (np.array(dataset_x1),np.array(dataset_x2)[:,:n_timesteps,:]),np.array(dataset_y)

def cnn_lstm(n_timesteps = 5, hidden_size = 128,input_size = 32, concat = True):
    '''
    
    CNN RNN combination that extends the CNN to a network that achieves 
    ~80% accuracy on full res cifar.

    Parameters
    ----------
    n_timesteps : TYPE, optional
        DESCRIPTION. The default is 5.
    img_dim : TYPE, optional
        DESCRIPTION. The default is 32.
    hidden_size : TYPE, optional
        DESCRIPTION. The default is 128.
    input_size : TYPE, optional
        DESCRIPTION. The default is 32.

    Returns
    -------
    model : TYPE
        DESCRIPTION.

    '''
    inputA = keras.layers.Input(shape=(n_timesteps,input_size,input_size,3))
    inputB = keras.layers.Input(shape=(n_timesteps,2))

    # define CNN model

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same'))(inputA)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)
    print(x1.shape)


    x1=keras.layers.TimeDistributed(keras.layers.Flatten())(x1)
    print(x1.shape)
    if concat:
        x = keras.layers.Concatenate()([x1,inputB])
    else:
        x = x1
    print(x.shape)

    # define LSTM model
    x = keras.layers.LSTM(hidden_size,input_shape=(n_timesteps, None),return_sequences=True,recurrent_dropout=rnn_dropout)(x)
    x = keras.layers.Flatten()(x)
    x = keras.layers.Dense(10,activation="softmax")(x)
    model = keras.models.Model(inputs=[inputA,inputB],outputs=x, name = 'cnn_lstm_{}'.format(concat))
    opt=tf.keras.optimizers.Adam(lr=5e-4)

    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=["sparse_categorical_accuracy"],
    )
    return model

rnn_net = cnn_lstm(n_timesteps = sample, hidden_size = hidden_size,input_size = res, concat = True)
# cnn_net = extended_cnn_one_img(n_timesteps = sample, input_size = res, dropout = cnn_dropout)


# hp = HP()
# hp.save_path = 'saved_runs'

# hp.description = "syclop cifar net search runs"
# hp.this_run_name = 'syclop_{}'.format(rnn_net.name)
# deploy_logs()

train_dataset, test_dataset = create_cifar_dataset(images, labels,res = res,
                                    sample = sample, return_datasets=True, 
                                    mixed_state = False, add_seed = 0,
                                    )
                                    #bad_res_func = bad_res101, up_sample = True)

train_dataset_x, train_dataset_y = split_dataset_xy(train_dataset)
test_dataset_x, test_dataset_y = split_dataset_xy(test_dataset)

# print("##################### Fit {} and trajectories model on training data res = {} ##################".format(cnn_net.name,res))
# cnn_history = cnn_net.fit(
#     train_dataset_x,
#     train_dataset_y,
#     batch_size=64,
#     epochs=epochs,
#     # We pass some validation for
#     # monitoring validation loss and metrics
#     # at the end of each epoch
#     validation_data=(test_dataset_x, test_dataset_y),
#     verbose = 0)
# print('################# {} Validation Accuracy = '.format(cnn_net.name),cnn_history.history['val_sparse_categorical_accuracy'])
print("##################### Fit {} and trajectories model on training data res = {} ##################".format(rnn_net.name,res))
rnn_history = rnn_net.fit(
    train_dataset_x,
    train_dataset_y,
    batch_size=64,
    epochs=epochs,
    # We pass some validation for
    # monitoring validation loss and metrics
    # at the end of each epoch
    validation_data=(test_dataset_x, test_dataset_y),
    verbose = 0)

# print('################# {} Validation Accuracy = '.format(cnn_net.name),cnn_history.history['val_sparse_categorical_accuracy'])
# print('################# {} Training Accuracy = '.format(cnn_net.name),rnn_history.history['sparse_categorical_accuracy'])

print('################# {} Validation Accuracy = '.format(rnn_net.name),rnn_history.history['val_sparse_categorical_accuracy'])
print('################# {} Training Accuracy = '.format(rnn_net.name),rnn_history.history['sparse_categorical_accuracy'])


plt.figure()
plt.plot(rnn_history.history['sparse_categorical_accuracy'], label = 'train')
plt.plot(rnn_history.history['val_sparse_categorical_accuracy'], label = 'val')
# plt.plot(cnn_history.history['sparse_categorical_accuracy'], label = 'cnn train')
# plt.plot(cnn_history.history['val_sparse_categorical_accuracy'], label = 'cnn val')
plt.legend()
plt.title('{} on cifar res = {} hs = {} dropout = {} num samples = {}'.format(rnn_net.name, res, hidden_size,cnn_dropout, sample))
plt.savefig('{} on Cifar res = {}, no upsample, val accur = {} hs = {} dropout = {}.png'.format(rnn_net.name,res,rnn_history.history['val_sparse_categorical_accuracy'][-1], hidden_size,cnn_dropout))

with open('/home/labs/ahissarlab/orra/imagewalker/cifar_net_search/{}HistoryDict{}_{}'.format(rnn_net.name, hidden_size,cnn_dropout), 'wb') as file_pi:
    pickle.dump(rnn_history.history, file_pi)
    
# with open('/home/labs/ahissarlab/orra/imagewalker/cifar_net_search/{}HistoryDict'.format(cnn_net.name), 'wb') as file_pi:
#     pickle.dump(cnn_history.history, file_pi)
    