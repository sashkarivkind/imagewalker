'''
The follwing code runs a test lstm network on the CIFAR dataset 

I will explicitly write the networks here for ease of understanding 

with cnn_sropout = 0.4 and rnn dropout = 0.2and lr = 1e-3 and res = 8
################# cnn_gru_True Validation Accuracy =  [0.3408, 0.411, 0.44, 0.4448, 0.466, 0.4684, 0.4802, 0.4846, 0.4848, 0.512, 0.5098, 0.5154, 0.5212, 0.5276, 0.5352, 0.5306, 0.5354, 0.5388, 0.5374, 0.5418, 0.55, 0.537, 0.5556, 0.543, 0.5458, 0.548, 0.5462, 0.554, 0.5596, 0.5438]
################# cnn_gru_True Training Accuracy =  [0.2734222, 0.3752889, 0.40646666, 0.42904446, 0.44386667, 0.45495555, 0.46284443, 0.47604445, 0.4802889, 0.48911113, 0.4968222, 0.4992, 0.50622225, 0.51126665, 0.5147333, 0.52275556, 0.5224444, 0.52537775, 0.5287778, 0.53275555, 0.53286666, 0.5396444, 0.5384222, 0.5423333, 0.542, 0.5485333, 0.547, 0.5458, 0.5524222, 0.55104446]

with cnn_sropout = 0.4 and rnn dropout = 0.2and lr = 1e-3 and res = 16
################# extended_cnn_one_img Validation Accuracy =  [0.416, 0.4696, 0.5168, 0.5424, 0.557, 0.5658, 0.5782, 0.5884, 0.5902, 0.5978, 0.5996, 0.6034, 0.6122, 0.606, 0.6112, 0.6104, 0.618, 0.6158, 0.6162, 0.6132, 0.6132, 0.6178, 0.6122, 0.626, 0.6168, 0.6164, 0.62, 0.6288, 0.6304, 0.6328]
################# extended_cnn_one_img Training Accuracy =  [0.2964, 0.42106667, 0.46775556, 0.49335554, 0.51544446, 0.52937776, 0.5436889, 0.5556889, 0.56684446, 0.57053334, 0.5798444, 0.58955556, 0.5917778, 0.59702224, 0.6014444, 0.60657775, 0.6142222, 0.6137556, 0.6195111, 0.6193111, 0.6226444, 0.6248, 0.6245555, 0.62575555, 0.6321333, 0.6330889, 0.6327556, 0.63677776, 0.63571113, 0.6396889]
################# cnn_convlstm_True Validation Accuracy =  [0.4038, 0.4724, 0.521, 0.5402, 0.52, 0.5516, 0.5658, 0.5654, 0.5904, 0.5866, 0.6024, 0.6026, 0.6114, 0.6224, 0.5982, 0.6178, 0.6314, 0.6208, 0.6158, 0.6352, 0.6412, 0.63, 0.6424, 0.6278, 0.6336, 0.6278, 0.646, 0.6272, 0.6414, 0.6406]
################# cnn_convlstm_True Training Accuracy =  [0.2964, 0.42106667, 0.46775556, 0.49335554, 0.51544446, 0.52937776, 0.5436889, 0.5556889, 0.56684446, 0.57053334, 0.5798444, 0.58955556, 0.5917778, 0.59702224, 0.6014444, 0.60657775, 0.6142222, 0.6137556, 0.6195111, 0.6193111, 0.6226444, 0.6248, 0.6245555, 0.62575555, 0.6321333, 0.6330889, 0.6327556, 0.63677776, 0.63571113, 0.6396889]

with cnn_sropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 out.812929
################# cnn_gru_True Validation Accuracy =  [0.3452, 0.41, 0.4206, 0.4382, 0.4626, 0.4786, 0.481, 0.4984, 0.5006, 0.5038, 0.5112, 0.5022, 0.522, 0.527, 0.5314, 0.5362, 0.5434,
 0.53, 0.543, 0.5534, 0.5528, 0.5456, 0.548, 0.5492, 0.5602, 0.5662, 0.5554, 0.5626, 0.5732, 0.5608, 0.5612, 0.5678, 0.578, 0.5572, 0.575, 0.5674, 0.5674, 0.5678, 0.574, 0.5832, 0.567, 0.5676, 0.5872, 0.5856, 0.5908, 0.5916, 0.586, 0.5628, 0.582, 0.5772, 0.5702, 0.5756, 0.5792, 0.5726, 0.59, 0.5784, 0.576, 0.5752, 0.5894, 0.5844, 0.583, 0.5832, 0.5782, 0.5696, 0.5812, 0.589, 0.5818, 0.5826, 0.5922, 0.5896, 0.5816, 0.5798, 0.5818, 0.5834, 0.5822, 0.5836, 0.5828, 0.569, 0.5914, 0.5822, 0.5974, 0.5928, 0.5956, 0.5936, 0.5888, 0.5932, 0.5986, 0.593, 0.5802, 0.5878, 0.5876, 0.5846, 0.6018, 0.5932, 0.5862, 0.5898, 0.5902, 0.5948, 0.5952, 0.596]
################# cnn_gru_True Training Accuracy =  [0.2522, 0.35944444, 0.40026668, 0.42453334, 0.4369111, 0.45024446, 0.46413332, 0.47453332, 0.47904444, 0.48753333, 0.4946, 0.50115556, 0.50531113, 0.5134, 0.5142, 0.5196222, 0.5276667, 0.529, 0.5313778, 0.5318889, 0.5356445, 0.54084444, 0.54051113, 0.5448889, 0.54855555, 0.5504444, 0.5562889, 0.5566889, 0.55655557, 0.5622889, 0.5615111, 0.5605111, 0.5638, 0.56615555, 0.5662444, 0.56953335, 0.5730444, 0.5717555, 0.5730444, 0.57368886, 0.5764889, 0.5782222, 0.58004445, 0.5802889, 0.5833778, 0.5824222, 0.58437777, 0.5869111, 0.58375555, 0.5871556, 0.5907556, 0.58444446, 0.58846664, 0.5914889, 0.59033334, 0.59257776, 0.5913333, 0.59606665, 0.5928222, 0.59577775, 0.5945333, 0.59613335, 0.5953556, 0.59786665, 0.5990222, 0.5993556, 0.60215557, 0.60344446, 0.6027111, 0.60364443, 0.6039111, 0.6062222, 0.60364443, 0.6062667, 0.6060445, 0.6081333, 0.6075778, 0.6094, 0.60568887, 0.6079556, 0.6064444, 0.61113334, 0.61322224, 0.6088667, 0.6125778, 0.61248887, 0.61282223, 0.61244446, 0.6136444, 0.61337775, 0.6174667, 0.61248887, 0.61535555, 0.6160667, 0.6134, 0.6155556, 0.6161111, 0.6158444, 0.61855555, 0.61642224]

with cnn_sropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 500 epochs out.813849 
################# cnn_gru_True Validation Accuracy =  [0.3136, 0.4024, 0.4436, 0.4546, 0.4648, 0.4552, 0.4766, 0.5058, 0.5028, 0.5182, 0.522, 0.5142, 0.5306, 0.5324, 0.5302, 0.5424, 0.5392, 0.543, 0.5328, 0.5276, 0.5474, 0.549, 0.5512, 0.5326, 0.5482, 0.5558, 0.5548, 0.5594, 0.5546, 0.566, 0.559, 0.5674, 0.564, 0.5584, 0.5698, 0.5718, 0.567, 0.5618, 0.5632, 0.574, 0.5696, 0.5758, 0.5636, 0.5744, 0.5706, 0.5734, 0.5508, 0.5692, 0.5802, 0.5704, 0.572, 0.5706, 0.5888, 0.5828, 0.583, 0.5812, 0.5872, 0.5748, 0.5844, 0.5784, 0.5838, 0.5862, 0.5826, 0.5838, 0.5894, 0.5942, 0.5932, 0.5818, 0.5836, 0.5914, 0.592, 0.5956, 0.5772, 0.5936, 0.5908, 0.5808, 0.5898, 0.5734, 0.578, 0.5868, 0.578, 0.5998, 0.59, 0.5956, 0.5708, 0.585, 0.5902, 0.5922, 0.5826, 0.5936, 0.5916, 0.5846, 0.6012, 0.5852, 0.5892, 0.592, 0.5806, 0.5938, 0.5916, 0.5866, 0.5952, 0.5944, 0.5956, 0.59, 0.592, 0.5922, 0.5962, 0.5906, 0.6006, 0.5912, 0.596, 0.6004, 0.596, 0.5838, 0.5918, 0.581, 0.5912, 0.587, 0.5942, 0.586, 0.591, 0.5906, 0.583, 0.5874, 0.5976, 0.5866, 0.5884, 0.5894, 0.5968, 0.5992, 0.5912, 0.5932, 0.5828, 0.5958, 0.5878, 0.5888, 0.595, 0.5948, 0.5898, 0.5956, 0.5896, 0.5942, 0.5938, 0.5884, 0.5874, 0.5954, 0.5908, 0.5948, 0.5972, 0.5986, 0.5984, 0.5952, 0.589, 0.5892, 0.6044, 0.6028, 0.5944, 0.591, 0.6018, 0.5932, 0.5982, 0.5896, 0.598, 0.6026, 0.6028, 0.6034, 0.5916, 0.5952, 0.5932, 0.597, 0.6008, 0.6026, 0.5974, 0.5954, 0.6014, 0.5988, 0.606, 0.6056, 0.5944, 0.6048, 0.6084, 0.6026, 0.599, 0.6022, 0.6022, 0.6022, 0.601, 0.5928, 0.5988, 0.6008, 0.599, 0.6016, 0.6036, 0.6056, 0.6142, 0.6064, 0.6082, 0.6032, 0.5974, 0.6082, 0.61, 0.6032, 0.6018, 0.6026, 0.6088, 0.6014, 0.6022, 0.6094, 0.6034, 0.5938, 0.6066, 0.5838, 0.5978, 0.6012, 0.5988, 0.6062, 0.6044, 0.5946, 0.597, 0.5954, 0.5944, 0.594, 0.5934, 0.5984, 0.6038, 0.607, 0.6056, 0.5948, 0.604, 0.6012, 0.5988, 0.608, 0.601, 0.6016, 0.5996, 0.6008, 0.6048, 0.6076, 0.6038, 0.6058, 0.6038, 0.6078, 0.5968, 0.605, 0.6046, 0.5982, 0.6002, 0.6092, 0.5956, 0.605, 0.6006, 0.5998, 0.5922, 0.6044, 0.5946, 0.602, 0.6008, 0.6068, 0.6018, 0.602, 0.594, 0.6046, 0.5992, 0.6006, 0.5962, 0.6092, 0.6026, 0.5984, 0.6078, 0.6024, 0.6048, 0.6032, 0.598, 0.6072, 0.6014, 0.5888, 0.6136, 0.605, 0.6032, 0.6032, 0.5988, 0.6014, 0.5988, 0.6054, 0.6038, 0.599, 0.5976, 0.5962, 0.602, 0.6028, 0.6082, 0.5936, 0.6052, 0.6014, 0.6022, 0.5976, 0.606, 0.6038, 0.6018, 0.6066, 0.601, 0.6038, 0.601, 0.6028, 0.6104, 0.5994, 0.6048, 0.5996, 0.6054, 0.597, 0.6042, 0.6048, 0.5962, 0.5968, 0.6036, 0.598, 0.6002, 0.593, 0.5972, 0.6024, 0.6018, 0.6102, 0.601, 0.6038, 0.594, 0.6068, 0.606, 0.6138, 0.6048, 0.602, 0.591, 0.6118, 0.6074, 0.5994, 0.5962, 0.6048, 0.6006, 0.6058, 0.6026, 0.6032, 0.6028, 0.608, 0.6036, 0.5968, 0.6004, 0.6054, 0.601, 0.6038, 0.6058, 0.6052, 0.5996, 0.6044, 0.598, 0.5986, 0.6018, 0.6002, 0.6064, 0.6064, 0.5918, 0.6004, 0.601, 0.605, 0.5974, 0.608, 0.608, 0.5968, 0.6042, 0.6034, 0.5984, 0.597, 0.6006, 0.6038, 0.603, 0.6004, 0.594, 0.5924, 0.5986, 0.5994, 0.6108, 0.5988, 0.6052, 0.6006, 0.6028, 0.602, 0.6016, 0.5996, 0.6012, 0.6014, 0.6042, 0.5988, 0.6064, 0.5982, 0.6, 0.6066, 0.609, 0.6096, 0.5948, 0.605, 0.6036, 0.5952, 0.6086, 0.6008, 0.5934, 0.6066, 0.608, 0.5998, 0.6042, 0.6016, 0.6018, 0.6062, 0.6068, 0.6194, 0.6032, 0.6116, 0.6058, 0.6022, 0.6056, 0.6, 0.6034, 0.6054, 0.6124, 0.6092, 0.603, 0.6016, 0.6018, 0.6084, 0.6026, 0.6154, 0.6034, 0.6118, 0.6102, 0.601, 0.603, 0.606, 0.6114, 0.6024, 0.6112, 0.6094, 0.6026, 0.598, 0.6074, 0.6066, 0.602, 0.6058, 0.603, 0.6078, 0.604, 0.605, 0.607, 0.605, 0.6044, 0.6026, 0.6006, 0.5988, 0.6056, 0.6016, 0.6054, 0.6004, 0.6024, 0.6092, 0.5954, 0.5962, 0.6036, 0.6008, 0.602, 0.6088, 0.6022, 0.6052, 0.5982, 0.6036, 0.601, 0.5956, 0.6024, 0.6104, 0.6028, 0.5898, 0.5994, 0.5946, 0.6054, 0.6064, 0.6102, 0.609, 0.6024, 0.599, 0.601, 0.6074, 0.6018, 0.595, 0.6034, 0.6028, 0.6008, 0.5996, 0.5992, 0.6006, 0.5996, 0.6018, 0.5968, 0.6016, 0.602, 0.6018]
################# cnn_gru_True Training Accuracy =  [0.26466668, 0.36813334, 0.40513334, 0.4256, 0.44268888, 0.4564222, 0.46568888, 0.4769111, 0.48531112, 0.491, 0.49744445, 0.50593334, 0.5138222, 0.51564443, 0.5213778, 0.5223778, 0.5283778, 0.5326222, 0.53275555, 0.53764445, 0.54586667, 0.5451556, 0.54735553, 0.5526, 0.5533111, 0.55424446, 0.5568889, 0.56262225, 0.5646, 0.5660667, 0.56333333, 0.5680889, 0.5706889, 0.5710889, 0.5733111, 0.5754667, 0.57637775, 0.5764667, 0.5768222, 0.5766889, 0.57817775, 0.5839555, 0.5825111, 0.5855778, 0.58424443, 0.5876, 0.58786666, 0.58806664, 0.58966666, 0.5938445, 0.5907111, 0.5939556, 0.59331113, 0.59475553, 0.5945333, 0.59515554, 0.59853333, 0.59635556, 0.6008667, 0.59893334, 0.5993556, 0.6007111, 0.6008889, 0.6032889, 0.6000444, 0.6049778, 0.60246664, 0.60384446, 0.60564446, 0.6048889, 0.6089778, 0.6061111, 0.60966665, 0.60686666, 0.60895556, 0.60973334, 0.60944444, 0.6095778, 0.6099778, 0.6114889, 0.6125778, 0.6149333, 0.61322224, 0.6185333, 0.6148, 0.61682224, 0.6157333, 0.6142, 0.6166222, 0.6152, 0.6158222, 0.61653334, 0.62155557, 0.6175333, 0.6168889, 0.61995554, 0.6193778, 0.6175778, 0.6207111, 0.62277776, 0.62144446, 0.62013334, 0.62328887, 0.62633336, 0.62722224, 0.62171113, 0.6248222, 0.62586665, 0.6251778, 0.6256889, 0.6254, 0.6249111, 0.62648886, 0.62468886, 0.6260889, 0.6276, 0.6266, 0.6273556, 0.6258444, 0.6287778, 0.6277111, 0.63026667, 0.6285333, 0.62846667, 0.62813336, 0.6326889, 0.6296, 0.63177776, 0.6323778, 0.6324, 0.63215554, 0.63104445, 0.6322889, 0.6328667, 0.63173336, 0.63515556, 0.6334, 0.63575554, 0.63404447, 0.6330444, 0.63526666, 0.6344444, 0.6337778, 0.63335556, 0.63386667, 0.6336222, 0.6369333, 0.63553333, 0.63713336, 0.63677776, 0.6365333, 0.6353111, 0.6347333, 0.6371111, 0.637, 0.63688886, 0.6344, 0.6371111, 0.636, 0.6394889, 0.638, 0.63946664, 0.63566667, 0.63857776, 0.6413111, 0.6376889, 0.63493335, 0.6387111, 0.6397778, 0.64055556, 0.64073336, 0.63766664, 0.6411333, 0.6392222, 0.6402444, 0.6413556, 0.64077777, 0.6387333, 0.6377778, 0.63884443, 0.64177775, 0.6401111, 0.64, 0.6415111, 0.64166665, 0.6448, 0.6414667, 0.64228886, 0.6416889, 0.63975555, 0.6437778, 0.6429778, 0.6421555, 0.64346665, 0.64155555, 0.64284444, 0.6429333, 0.64415556, 0.64611113, 0.64555556, 0.6452444, 0.64522225, 0.64824444, 0.64275557, 0.64593333, 0.64662224, 0.6431556, 0.6444, 0.6441111, 0.64482224, 0.6471556, 0.64584446, 0.6441778, 0.6448, 0.6446, 0.64775556, 0.64764446, 0.64677775, 0.646, 0.6472222, 0.6472, 0.6481111, 0.6465333, 0.6469778, 0.6510222, 0.64677775, 0.6503556, 0.647, 0.64944446, 0.64655554, 0.64724445, 0.65128887, 0.64955556, 0.6482222, 0.6444889, 0.6488, 0.64797777, 0.6509111, 0.6520444, 0.65022224, 0.6516, 0.645, 0.65044445, 0.64702225, 0.65264446, 0.6487778, 0.64944446, 0.6492222, 0.6536889, 0.6499778, 0.6486222, 0.6539556, 0.64806664, 0.6488, 0.65055555, 0.6541778, 0.6518667, 0.6526667, 0.65155554, 0.6526, 0.65202224, 0.64977777, 0.65315557, 0.65128887, 0.64773333, 0.6536222, 0.65335554, 0.6523778, 0.6494, 0.6510889, 0.6496889, 0.6514, 0.65117776, 0.65375555, 0.65415555, 0.6495778, 0.65055555, 0.6507556, 0.65346664, 0.6548, 0.65115553, 0.6553111, 0.6517778, 0.6532889, 0.6548, 0.6546222, 0.65533334, 0.6521556, 0.6543555, 0.65217775, 0.65275556, 0.6522, 0.65555555, 0.65482223, 0.6541111, 0.6546889, 0.65533334, 0.6541111, 0.6554, 0.6537333, 0.6537778, 0.6528444, 0.65331113, 0.65455556, 0.6544, 0.65477777, 0.6572667, 0.65606666, 0.6556, 0.65606666, 0.6553556, 0.65353334, 0.6518, 0.6536667, 0.65595555, 0.65775555, 0.65657777, 0.6549778, 0.65764445, 0.6557111, 0.6556, 0.6590222, 0.6538889, 0.6591778, 0.65444446, 0.6562, 0.6564, 0.6607778, 0.6556444, 0.65826666, 0.6562, 0.6581333, 0.6578889, 0.65853333, 0.6584, 0.65782225, 0.6594667, 0.6552, 0.6586667, 0.658, 0.6588, 0.66135556, 0.65668887, 0.6561555, 0.6581111, 0.6599111, 0.6588, 0.6568, 0.6608667, 0.6603778, 0.6602889, 0.6592, 0.6594667, 0.65706664, 0.6567111, 0.6608667, 0.65886664, 0.65966666, 0.66035557, 0.66175556, 0.65584445, 0.65966666, 0.6606889, 0.65922225, 0.6595111, 0.65515554, 0.65984446, 0.6612667, 0.6605333, 0.662, 0.6613778, 0.6611556, 0.6580667, 0.66135556, 0.65882224, 0.65655553, 0.65955555, 0.65988886, 0.6593556, 0.65808886, 0.6616667, 0.6614222, 0.6634, 0.6632222, 0.6618, 0.6599778, 0.66013336, 0.6608, 0.66146666, 0.65944445, 0.65966666, 0.66135556, 0.66004443, 0.6608222, 0.6630222, 0.6620889, 0.66195554, 0.6582222, 0.6606445, 0.6629556, 0.66164446, 0.66055554, 0.6608889, 0.66175556, 0.6606, 0.6614222, 0.6640222, 0.66364443, 0.6643556, 0.66191113, 0.6626667, 0.6630222, 0.6656889, 0.6631333, 0.66293335, 0.6617778, 0.6610889, 0.6614889, 0.662, 0.6593111, 0.6612667, 0.66102225, 0.6631333, 0.66395557, 0.66282225, 0.66713333, 0.6623778, 0.6648222, 0.6622667, 0.66746664, 0.6616667, 0.6630222, 0.6622, 0.6624, 0.66415554, 0.662, 0.6612222, 0.6618222, 0.6629111, 0.66426665, 0.66315556, 0.6640667, 0.6640889, 0.66533333, 0.6626, 0.6617778, 0.66477776, 0.6654889, 0.66477776, 0.6624889, 0.6622222, 0.6642, 0.6663111, 0.66293335, 0.6636889, 0.6643556, 0.6652, 0.6680889, 0.6658222, 0.66415554, 0.6677778, 0.6622889, 0.6688, 0.6630222, 0.66848886, 0.66355556, 0.6624889, 0.6658222, 0.66602224, 0.6631778, 0.6618889, 0.6654222, 0.6662889, 0.66726667, 0.66384447, 0.6662, 0.66477776, 0.6650889, 0.66293335, 0.66484445, 0.66371113, 0.6646, 0.6661556, 0.66191113, 0.6656889, 0.6649333, 0.66686666, 0.66544443, 0.66624445, 0.66455555, 0.6698222, 0.6665556, 0.6648, 0.6663111, 0.66455555, 0.6653778, 0.6675556, 0.66404444, 0.66484445, 0.66617775]

with cnn_dropout = 0.2 and rnn dropout = 0.2and lr = 5e-4 with res = 8 out.812847
################# cnn_gru_True Validation Accuracy =  [0.3598, 0.4126, 0.4454, 0.4714, 0.4722, 0.506, 0.5062, 0.5154, 0.5382, 0.5296, 0.5368, 0.5352, 0.5364, 0.5584, 0.5564, 0.5624, 0.5
704, 0.5622, 0.5612, 0.5568, 0.5656, 0.5572, 0.572, 0.5718, 0.569, 0.576, 0.5718, 0.5726, 0.5732, 0.5754, 0.5758, 0.5754, 0.5802, 0.5778, 0.5778, 0.5818, 0.5808, 0.573, 0.5764, 0.5782, 0.578, 0.5828, 0.5656, 0.5796, 0.5704, 0.5808, 0.5764, 0.5774, 0.5644, 0.5794, 0.5794, 0.5834, 0.57, 0.5724, 0.5806, 0.5784, 0.5794, 0.5834, 0.5756, 0.5786, 0.5802, 0.5746, 0.571, 0.5812, 0.569, 0.5724, 0.5794, 0.5762, 0.581, 0.5664, 0.574, 0.5782, 0.5738, 0.5714, 0.5754, 0.5716, 0.5638, 0.5696, 0.5706, 0.5758, 0.567, 0.571, 0.5716, 0.5788, 0.559, 0.5682, 0.5716, 0.5728, 0.5718, 0.5758, 0.569, 0.573, 0.5756, 0.5746, 0.5744, 0.571, 0.5762, 0.5792, 0.5688, 0.5796]
################# cnn_gru_True Training Accuracy =  [0.27786666, 0.3842222, 0.42204446, 0.44537777, 0.4655111, 0.48406667, 0.49457777, 0.50564444, 0.5188889, 0.5279111, 0.5366667, 0.544, 0.5515111, 0.5573556, 0.56457776, 0.5718222, 0.5748889, 0.5826667, 0.5850222, 0.5921556, 0.59155554, 0.5960889, 0.6028889, 0.60664445, 0.6115556, 0.61553335, 0.61968887, 0.6218889, 0.6240444, 0.6262222, 0.6306889, 0.6329778, 0.6356, 0.6404, 0.6475111, 0.6451333, 0.64626664, 0.6536889, 0.65573335, 0.65842223, 0.65977776, 0.6573111, 0.6640889, 0.6664, 0.66866666, 0.6700889, 0.6704222, 0.6747556, 0.6781333, 0.6785111, 0.67693335, 0.68086666, 0.68293333, 0.6823111, 0.6862444, 0.69013333, 0.69044447, 0.6957778, 0.6952, 0.6944889, 0.69953334, 0.6963111, 0.7000222, 0.7018667, 0.7029333, 0.7018222, 0.70446664, 0.7051111, 0.7105778, 0.70993334, 0.71308887, 0.71331114, 0.71128887, 0.7160444, 0.7176222, 0.71793336, 0.71846664, 0.72062224, 0.7216222, 0.7220889, 0.72117776, 0.72617775, 0.72535557, 0.72904444, 0.72675556, 0.73215556, 0.7297556, 0.72926664, 0.7349333, 0.73224443, 0.7335778, 0.73744446, 0.73384446, 0.73735553, 0.73744446, 0.7404889, 0.73928887, 0.742, 0.7410667, 0.7395778]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 500 epochs out.813851
################# cnn_gru_True Validation Accuracy =  [0.3354, 0.4208, 0.4522, 0.463, 0.4448, 0.4934, 0.5048, 0.5036, 0.5082, 0.5202, 0.4958, 0.5184, 0.5302, 0.5364, 0.5474, 0.5298, 0.5382, 0.5446, 0.5486, 0.5496, 0.5468, 0.5616, 0.5516, 0.5542, 0.5606, 0.5624, 0.5744, 0.5644, 0.5624, 0.5712, 0.5714, 0.5746, 0.5638, 0.5622, 0.5768, 0.5792, 0.5852, 0.5758, 0.5768, 0.5708, 0.5882, 0.5814, 0.5778, 0.5884, 0.5892, 0.5862, 0.5828, 0.5838, 0.5892, 0.58, 0.595, 0.5872, 0.58, 0.5868, 0.5888, 0.592, 0.5848, 0.5824, 0.5852, 0.5832, 0.5898, 0.5846, 0.584, 0.5942, 0.5858, 0.5918, 0.5826, 0.597, 0.5984, 0.5928, 0.5802, 0.5972, 0.5976, 0.5964, 0.5894, 0.5888, 0.5948, 0.5944, 0.594, 0.5934, 0.5952, 0.5976, 0.5994, 0.6002, 0.5926, 0.5984, 0.5976, 0.591, 0.593, 0.6076, 0.5888, 0.6018, 0.5908, 0.5964, 0.5966, 0.5968, 0.5912, 0.5976, 0.5912, 0.597, 0.5934, 0.588, 0.6014, 0.592, 0.5952, 0.606, 0.6026, 0.5932, 0.6, 0.5944, 0.5898, 0.5914, 0.5976, 0.6008, 0.5894, 0.6058, 0.6038, 0.5974, 0.5996, 0.6064, 0.6014, 0.5914, 0.6012, 0.5922, 0.5938, 0.6008, 0.6058, 0.6046, 0.6012, 0.593, 0.6, 0.6046, 0.5946, 0.5962, 0.592, 0.5968, 0.5946, 0.5966, 0.5968, 0.588, 0.6004, 0.6008, 0.592, 0.5976, 0.5998, 0.5854, 0.6012, 0.5994, 0.5908, 0.5996, 0.6056, 0.5924, 0.5974, 0.5986, 0.5926, 0.5938, 0.5902, 0.5924, 0.598, 0.5988, 0.6028, 0.601, 0.5976, 0.597, 0.6044, 0.5894, 0.5904, 0.6, 0.595, 0.5974, 0.5998, 0.594, 0.5946, 0.5968, 0.5938, 0.5858, 0.6016, 0.5934, 0.6052, 0.598, 0.608, 0.6, 0.6008, 0.5956, 0.591, 0.6024, 0.6076, 0.5986, 0.5974, 0.6004, 0.6046, 0.597, 0.6048, 0.588, 0.5902, 0.5868, 0.5928, 0.5986, 0.5994, 0.5962, 0.5946, 0.594, 0.5972, 0.592, 0.5916, 0.589, 0.6042, 0.5908, 0.5922, 0.5924, 0.5902, 0.5914, 0.6026, 0.5992, 0.5956, 0.5954, 0.6034, 0.5906, 0.6052, 0.5918, 0.6, 0.6004, 0.5912, 0.5942, 0.5972, 0.6066, 0.5946, 0.5972, 0.5854, 0.5994, 0.5954, 0.592, 0.5904, 0.5956, 0.5946, 0.5838, 0.5872, 0.5948, 0.5972, 0.5996, 0.605, 0.5962, 0.604, 0.5976, 0.6, 0.6016, 0.6014, 0.6044, 0.5928, 0.598, 0.6, 0.59, 0.5978, 0.5902, 0.5934, 0.6026, 0.5956, 0.6012, 0.5932, 0.6, 0.5952, 0.602, 0.5942, 0.5988, 0.6024, 0.597, 0.5964, 0.5882, 0.6008, 0.5958, 0.6006, 0.5964, 0.594, 0.5882, 0.6028, 0.6032, 0.5982, 0.6, 0.5988, 0.6018, 0.6028, 0.609, 0.6032, 0.5954, 0.5988, 0.6074, 0.6014, 0.6086, 0.6002, 0.605, 0.603, 0.6058, 0.6084, 0.5894, 0.6046, 0.6006, 0.605, 0.5972, 0.5964, 0.5972, 0.603, 0.5986, 0.601, 0.5972, 0.6058, 0.6028, 0.596, 0.603, 0.598, 0.6008, 0.5958, 0.5906, 0.6024, 0.6024, 0.6014, 0.6078, 0.6006, 0.5996, 0.603, 0.6068, 0.6046, 0.6064, 0.5948, 0.5988, 0.6074, 0.6024, 0.605, 0.5974, 0.6014, 0.6054, 0.5966, 0.6006, 0.601, 0.592, 0.6108, 0.5944, 0.6008, 0.599, 0.6072, 0.6034, 0.5964, 0.6104, 0.592, 0.6044, 0.6026, 0.6032, 0.6058, 0.6094, 0.6042, 0.6062, 0.6016, 0.6084, 0.6028, 0.608, 0.604, 0.6012, 0.6012, 0.6072, 0.6008, 0.607, 0.6018, 0.597, 0.6008, 0.6092, 0.6044, 0.594, 0.6026, 0.6082, 0.6078, 0.6092, 0.6064, 0.6052, 0.6052, 0.6004, 0.6078, 0.6102, 0.6, 0.615, 0.605, 0.5942, 0.6044, 0.6084, 0.6002, 0.6034, 0.5998, 0.5982, 0.5974, 0.598, 0.601, 0.597, 0.6062, 0.6036, 0.6048, 0.599, 0.604, 0.607, 0.6036, 0.5992, 0.6018, 0.6022, 0.6044, 0.5984, 0.6006, 0.5986, 0.6056, 0.6062, 0.5942, 0.6032, 0.6026, 0.5994, 0.6064, 0.599, 0.6008, 0.5986, 0.5984, 0.5962, 0.5972, 0.6016, 0.6014, 0.604, 0.6026, 0.6002, 0.6076, 0.605, 0.5988, 0.6006, 0.6006, 0.5992, 0.5994, 0.6016, 0.601, 0.5924, 0.597, 0.5998, 0.6012, 0.6064, 0.5968, 0.6012, 0.604, 0.603, 0.602, 0.595, 0.6044, 0.5952, 0.6016, 0.6058, 0.6012, 0.6042, 0.5966, 0.6054, 0.6066, 0.6016, 0.594, 0.6042, 0.607, 0.6038, 0.5942, 0.6064, 0.6044, 0.6022, 0.6056, 0.6036, 0.594, 0.605, 0.6042, 0.6062, 0.591, 0.5988, 0.6056, 0.608, 0.6014, 0.605, 0.5996, 0.6046, 0.6066, 0.6032, 0.5998, 0.6028, 0.6, 0.5948, 0.6046, 0.6066, 0.603, 0.6038, 0.6066, 0.6034, 0.6034, 0.5978, 0.6014, 0.602, 0.592, 0.6008, 0.6066, 0.6046, 0.6072, 0.6106, 0.6062, 0.6074, 0.5986, 0.6034]
################# cnn_gru_True Training Accuracy =  [0.24648888, 0.3745778, 0.41557777, 0.43804446, 0.4576, 0.4678, 0.47815555, 0.4868, 0.49584445, 0.5020667, 0.50942224, 0.5155333, 0.51953334, 0.52253336, 0.5287778, 0.5311555, 0.5374, 0.5400222, 0.54744446, 0.54553336, 0.55102223, 0.55517775, 0.5588667, 0.55873334, 0.56222224, 0.56906664, 0.56704444, 0.57048887, 0.5709556, 0.57553333, 0.58104444, 0.57677776, 0.5827111, 0.5832, 0.58533335, 0.5862667, 0.5885556, 0.5909333, 0.5918, 0.59326667, 0.5958222, 0.5950222, 0.59848887, 0.59871113, 0.6015555, 0.60064447, 0.60433334, 0.6062222, 0.6030667, 0.6063333, 0.6067333, 0.6074889, 0.60944444, 0.6112889, 0.61002225, 0.61248887, 0.6134, 0.61333334, 0.6154, 0.6148, 0.61473334, 0.618, 0.6176222, 0.61884445, 0.6212889, 0.62226665, 0.6203778, 0.62186664, 0.6224667, 0.626, 0.6241111, 0.6243333, 0.62524444, 0.6258889, 0.6276444, 0.62704444, 0.62773335, 0.62866664, 0.62637776, 0.62784445, 0.63368887, 0.63137776, 0.63233334, 0.6337778, 0.63453335, 0.6339778, 0.6327556, 0.6346667, 0.6375333, 0.63571113, 0.6359111, 0.63633335, 0.63897777, 0.6382667, 0.6386667, 0.6386667, 0.6402, 0.6410889, 0.63853335, 0.6414222, 0.6431111, 0.64084446, 0.6423333, 0.6404222, 0.64386666, 0.6427778, 0.64442223, 0.64526665, 0.6431778, 0.6445111, 0.6468222, 0.6451333, 0.6484889, 0.64537776, 0.64544445, 0.6438889, 0.65073335, 0.6497333, 0.6512667, 0.6492222, 0.64784443, 0.64622223, 0.6495111, 0.6498, 0.6488889, 0.6512667, 0.6499111, 0.6527333, 0.6570889, 0.65253335, 0.65371114, 0.65015554, 0.6525111, 0.6505778, 0.64982224, 0.65437776, 0.6553778, 0.6556889, 0.6545333, 0.65713334, 0.65573335, 0.6571111, 0.65706664, 0.6573333, 0.65397775, 0.6564889, 0.6561111, 0.65691113, 0.65595555, 0.6564889, 0.6577778, 0.65757775, 0.6575111, 0.65835553, 0.6568889, 0.65746665, 0.65602225, 0.6579111, 0.65724444, 0.6560444, 0.6582222, 0.65844446, 0.6604667, 0.6612667, 0.6575111, 0.6612667, 0.6634222, 0.6617333, 0.6640889, 0.6603111, 0.66286665, 0.66135556, 0.6610889, 0.6615555, 0.6611556, 0.6604889, 0.66477776, 0.6643556, 0.6623333, 0.6612222, 0.66353333, 0.6625556, 0.66186666, 0.66333336, 0.66395557, 0.66355556, 0.66575557, 0.66433334, 0.6652, 0.6616667, 0.66602224, 0.6647556, 0.6646444, 0.66708887, 0.6645333, 0.6630667, 0.66844445, 0.6675111, 0.668, 0.6643556, 0.6670222, 0.6701111, 0.6662222, 0.66546667, 0.66364443, 0.6655333, 0.6684667, 0.6691778, 0.66922224, 0.6661111, 0.6691778, 0.66804445, 0.6721333, 0.6696889, 0.66775554, 0.66642225, 0.6698, 0.66884446, 0.6692889, 0.66713333, 0.66962224, 0.6699778, 0.67197776, 0.6676222, 0.6693556, 0.66926664, 0.67282224, 0.6721778, 0.6653111, 0.67164445, 0.6734222, 0.66951114, 0.67384446, 0.6722, 0.6716889, 0.6684667, 0.67164445, 0.6717778, 0.6716, 0.67102224, 0.6719555, 0.6747111, 0.6744222, 0.67253333, 0.672, 0.67362225, 0.6738222, 0.6768889, 0.6722, 0.67182225, 0.67775553, 0.6749111, 0.67495555, 0.6774667, 0.67304444, 0.6748667, 0.6732889, 0.67513335, 0.6786444, 0.6725111, 0.6751111, 0.6779111, 0.6733111, 0.6766667, 0.67653334, 0.6767778, 0.67755556, 0.6733556, 0.6755111, 0.67646664, 0.67513335, 0.6769556, 0.6732, 0.6803778, 0.67642224, 0.67595553, 0.6792667, 0.6769111, 0.6782889, 0.67833334, 0.67917776, 0.67422223, 0.67873335, 0.6778889, 0.67495555, 0.677, 0.67962223, 0.68053335, 0.6788222, 0.67664444, 0.6814, 0.681, 0.67826664, 0.6806222, 0.68153334, 0.6809555, 0.6798667, 0.6808889, 0.67764443, 0.6803111, 0.6794222, 0.67646664, 0.6801111, 0.6809111, 0.6828667, 0.67866665, 0.68137777, 0.6797111, 0.67991114, 0.67913336, 0.6791111, 0.68164444, 0.68042225, 0.68126667, 0.6821333, 0.6833111, 0.6835778, 0.67884445, 0.68593335, 0.6798, 0.67928886, 0.682, 0.6838667, 0.6833111, 0.68648887, 0.6845111, 0.6812889, 0.6846222, 0.6825778, 0.6810222, 0.68273336, 0.68315554, 0.6806667, 0.68648887, 0.68295556, 0.6824, 0.6821111, 0.681, 0.6835333, 0.68524444, 0.68455553, 0.6817333, 0.6833111, 0.6825333, 0.68675554, 0.6819111, 0.68475556, 0.6879333, 0.68473333, 0.68384445, 0.6862222, 0.6841111, 0.6841111, 0.68277776, 0.6884, 0.6818, 0.6853778, 0.6822444, 0.68637776, 0.6852889, 0.68615556, 0.6869556, 0.6840444, 0.6870667, 0.68564445, 0.68497777, 0.68531114, 0.6839111, 0.6844, 0.68924445, 0.68635553, 0.68484443, 0.6872, 0.6852889, 0.6884889, 0.68435556, 0.68475556, 0.6860667, 0.68664443, 0.6854889, 0.6857333, 0.68864447, 0.6874889, 0.6874, 0.6852889, 0.6850889, 0.6857778, 0.6856889, 0.6898444, 0.6896667, 0.6880222, 0.68762225, 0.68873334, 0.68815553, 0.6851111, 0.68813336, 0.6874667, 0.69233334, 0.6897111, 0.6887778, 0.68846667, 0.6905778, 0.6882222, 0.69188887, 0.6883111, 0.6878, 0.6901111, 0.6859556, 0.68902224, 0.69188887, 0.6915778, 0.69206667, 0.6874889, 0.6928, 0.689, 0.6896, 0.6896667, 0.6893111, 0.68997777, 0.6876, 0.6924667, 0.6876889, 0.6892222, 0.6910889, 0.6886, 0.6886889, 0.69391114, 0.6886889, 0.69284445, 0.69211113, 0.6900667, 0.6905556, 0.6885778, 0.6871333, 0.69188887, 0.69204444, 0.6908, 0.693, 0.69355553, 0.69211113, 0.6909556, 0.6921333, 0.6925333, 0.69126666, 0.69211113, 0.69277775, 0.6929111, 0.69075555, 0.69093335, 0.69075555, 0.6912, 0.68862224, 0.69346666, 0.6921778, 0.6904889, 0.69486666, 0.69166666, 0.6924, 0.69355553, 0.69373333, 0.6925111, 0.69295555, 0.69515556, 0.69184446, 0.69206667, 0.69537777, 0.6911111, 0.6930444, 0.69335556, 0.6888667, 0.69364446, 0.6946222, 0.6948444, 0.6927111, 0.6944444, 0.6907333, 0.69357777, 0.6952222, 0.69155556, 0.6915333, 0.69537777, 0.6924889, 0.69035554, 0.69366664, 0.6966, 0.6922, 0.6918667, 0.6926, 0.6960667, 0.6926, 0.69564444, 0.69328886, 0.6952889, 0.6944444, 0.69571114, 0.69546664, 0.694, 0.6939333, 0.6952889, 0.6956667]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 5 samples and 200 epochs, hs = 256 out.836806
################# cnn_gru_True Validation Accuracy =  [0.3074, 0.3502, 0.3972, 0.4236, 0.4458, 0.4612, 0.478, 0.4846, 0.4832, 0.494, 0.4936, 0.5028, 0.511, 0.5, 0.4942, 0.5186, 0.5216, 0.5274, 0.5356, 0.5306, 0.5296, 0.535, 0.5346, 0.5346, 0.5448, 0.534, 0.5384, 0.5442, 0.5434, 0.539, 0.5478, 0.552, 0.549, 0.5404, 0.5448, 0.5434, 0.5568, 0.5462, 0.5462, 0.5558, 0.5612, 0.5484, 0.5606, 0.5666, 0.5698, 0.5582, 0.5578, 0.5744, 0.56, 0.5466, 0.5554, 0.563, 0.5592, 0.5566, 0.5674, 0.5536, 0.5606, 0.5678, 0.5618, 0.559, 0.5676, 0.571, 0.563, 0.5646, 0.563, 0.5732, 0.565, 0.5738, 0.572, 0.5774, 0.5652, 0.5636, 0.5688, 0.5718, 0.5734, 0.558, 0.571, 0.577, 0.5674, 0.579, 0.5706, 0.5764, 0.567, 0.5772, 0.5738, 0.5688, 0.5706, 0.5712, 0.575, 0.5748, 0.5804, 0.5708, 0.566, 0.57, 0.5768, 0.5814, 0.569, 0.5796, 0.5776, 0.5702, 0.5806, 0.5834, 0.5708, 0.5748, 0.5794, 0.585, 0.5792, 0.5738, 0.5736, 0.5776, 0.5812, 0.5804, 0.5762, 0.5806, 0.5822, 0.5786, 0.5768, 0.5752, 0.5822, 0.5808, 0.5822, 0.5844, 0.5876, 0.589, 0.5872, 0.5764, 0.5808, 0.5738, 0.581, 0.5828, 0.5688, 0.577, 0.5798, 0.587, 0.5766, 0.5798, 0.5834, 0.5802, 0.5826, 0.578, 0.5786, 0.565, 0.5742, 0.5894, 0.5808, 0.5708, 0.5766, 0.5866, 0.5806, 0.577, 0.5794, 0.5802, 0.5776, 0.5824, 0.586, 0.574, 0.5804, 0.5834, 0.5834, 0.578, 0.5784, 0.571, 0.5668, 0.5798, 0.5792, 0.5748, 0.5824, 0.5628, 0.5814, 0.5796, 0.581, 0.575, 0.5802, 0.5786, 0.5802, 0.5852, 0.5818, 0.5826, 0.59, 0.5762, 0.59, 0.577, 0.5798, 0.5796, 0.581, 0.5806, 0.5774, 0.5772, 0.5798, 0.585, 0.588, 0.5856, 0.5836, 0.5858, 0.5842, 0.5826, 0.5818, 0.5764, 0.5814, 0.5812]
################# cnn_gru_True Training Accuracy =  [0.24148889, 0.35346666, 0.39324445, 0.4138, 0.4300222, 0.44151112, 0.45264444, 0.4583111, 0.46951112, 0.47684443, 0.48144445, 0.4867111, 0.4934, 0.49924445, 0.5006667, 0.5047333, 0.51008886, 0.5148889, 0.51364446, 0.5214889, 0.5223111, 0.5267556, 0.5283778, 0.52993333, 0.5365111, 0.5373333, 0.5393111, 0.5411556, 0.5418444, 0.5449778, 0.54704446, 0.55093336, 0.55095553, 0.5567778, 0.5597778, 0.5578, 0.55826664, 0.5587111, 0.56135553, 0.5613111, 0.5638222, 0.5689778, 0.5655111, 0.5698, 0.56924444, 0.57137775, 0.57251114, 0.57457775, 0.5736667, 0.578, 0.5786222, 0.5777556, 0.57964444, 0.5810889, 0.5809778, 0.5831556, 0.5817556, 0.584, 0.5824444, 0.5857111, 0.58357775, 0.58804446, 0.58624446, 0.5888444, 0.5892222, 0.59157777, 0.59275556, 0.5909333, 0.5932, 0.5918667, 0.59206665, 0.59437776, 0.5966, 0.5946889, 0.59984446, 0.59511113, 0.5969333, 0.6005333, 0.59893334, 0.5999333, 0.6010889, 0.60175556, 0.6009333, 0.6008, 0.60035557, 0.6005778, 0.6013778, 0.6052667, 0.6039111, 0.6061556, 0.60355556, 0.603, 0.60344446, 0.6076889, 0.6047556, 0.6068222, 0.60406667, 0.6079778, 0.60693336, 0.6074889, 0.6102889, 0.6061111, 0.61104447, 0.61002225, 0.6100444, 0.60866666, 0.6106, 0.61131114, 0.6118889, 0.61204445, 0.61377776, 0.61182225, 0.61311114, 0.61197776, 0.61635554, 0.6154889, 0.6140444, 0.61644447, 0.61704445, 0.61833334, 0.61795557, 0.6198222, 0.6174667, 0.6174, 0.61766666, 0.6165778, 0.6163778, 0.61793333, 0.61946666, 0.62144446, 0.6208444, 0.6163333, 0.61624444, 0.6175111, 0.62124443, 0.6211333, 0.6183778, 0.62288886, 0.6214667, 0.6212889, 0.6186889, 0.6230222, 0.62313336, 0.6221333, 0.6222, 0.62453336, 0.6224889, 0.6257333, 0.6224667, 0.6254445, 0.6226889, 0.62384444, 0.6247111, 0.6238889, 0.6228222, 0.6233778, 0.6265333, 0.6257333, 0.62604445, 0.6287111, 0.6253778, 0.6269111, 0.63024443, 0.6262889, 0.62766665, 0.62615556, 0.6257333, 0.6289778, 0.6282, 0.62615556, 0.62993336, 0.6257111, 0.6315111, 0.6270222, 0.6297333, 0.6268889, 0.6298222, 0.6300667, 0.6293333, 0.62995553, 0.6311333, 0.63037777, 0.6307333, 0.62993336, 0.6329111, 0.6297333, 0.63217777, 0.6298444, 0.6303333, 0.6312, 0.6305111, 0.6304, 0.6334444, 0.63204443, 0.63064444, 0.6292, 0.63317776, 0.63226664, 0.6315778, 0.6300667]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 5 samples and 500 epochs, hs = 256 out.848468

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 200 epochs, hs = 256 out.846686
################# cnn_gru_True Validation Accuracy =  [0.3584, 0.427, 0.4594, 0.4528, 0.4746, 0.4934, 0.5094, 0.5078, 0.5196, 0.5242, 0.5342, 0.5258, 0.5292, 0.533, 0.5444, 0.5422, 0.5572, 0.5486, 0.5644, 0.5618, 0.5692, 0.5666, 0.5764, 0.5676, 0.5674, 0.5466, 0.5744, 0.5802, 0.5782, 0.5784, 0.5742, 0.5786, 0.5762, 0.5692, 0.5916, 0.5654, 0.5772, 0.5744, 0.5854, 0.582, 0.5882, 0.5814, 0.595, 0.5838, 0.5866, 0.5888, 0.5876, 0.5888, 0.5866, 0.5782, 0.5958, 0.5926, 0.5914, 0.5778, 0.5944, 0.58, 0.5944, 0.5878, 0.5926, 0.5954, 0.595, 0.5844, 0.588, 0.5934, 0.5942, 0.598, 0.5974, 0.5944, 0.5924, 0.5944, 0.5908, 0.5952, 0.5966, 0.5966, 0.5992, 0.5966, 0.5956, 0.5836, 0.5956, 0.5832, 0.5938, 0.5992, 0.5976, 0.5952, 0.5904, 0.5906, 0.5924, 0.5878, 0.6094, 0.604, 0.5884, 0.5986, 0.5922, 0.5806, 0.5932, 0.5914, 0.603, 0.5888, 0.5892, 0.588, 0.5942, 0.6024, 0.5898, 0.5992, 0.6, 0.5928, 0.5958, 0.5824, 0.6004, 0.5842, 0.5914, 0.603, 0.5946, 0.5928, 0.5956, 0.5828, 0.608, 0.6058, 0.5928, 0.5934, 0.5938, 0.5958, 0.5952, 0.598, 0.5868, 0.6004, 0.5884, 0.593, 0.5936, 0.6094, 0.5996, 0.5984, 0.5976, 0.5984, 0.6084, 0.5964, 0.5886, 0.6, 0.6, 0.596, 0.5936, 0.6028, 0.5986, 0.5992, 0.5784, 0.5882, 0.5942, 0.598, 0.605, 0.5904, 0.6, 0.586, 0.5894, 0.5984, 0.5824, 0.5944, 0.5906, 0.5922, 0.588, 0.5952, 0.593, 0.5846, 0.5932, 0.5978, 0.5942, 0.5958, 0.5992, 0.5938, 0.5914, 0.5968, 0.5946, 0.5978, 0.6004, 0.588, 0.5982, 0.5992, 0.6012, 0.5976, 0.594, 0.5912, 0.5854, 0.5954, 0.5922, 0.5908, 0.5842, 0.6034, 0.5978, 0.6012, 0.5974, 0.5924, 0.5952, 0.6004, 0.5942, 0.6014, 0.5882, 0.5978, 0.5992, 0.5938, 0.5946, 0.6006]
################# cnn_gru_True Training Accuracy =  [0.25715557, 0.3822, 0.41933334, 0.4448889, 0.4602, 0.47442222, 0.48344445, 0.49475557, 0.5034222, 0.5122, 0.5181111, 0.5222, 0.5295778, 0.5335111, 0.54168886, 0.54411113, 0.54735553, 0.5506667, 0.5548667, 0.56093335, 0.5622444, 0.5642889, 0.56453335, 0.56953335, 0.57226664, 0.57644445, 0.57728887, 0.5796, 0.58304447, 0.58397776, 0.5872, 0.58673334, 0.58926666, 0.59195554, 0.59515554, 0.59691113, 0.59655553, 0.5989778, 0.60253334, 0.6033111, 0.60406667, 0.60415554, 0.6044889, 0.6035333, 0.6082444, 0.6112222, 0.60873336, 0.61075556, 0.61517775, 0.61646664, 0.61586666, 0.61855555, 0.6187111, 0.6170667, 0.62135553, 0.6203778, 0.6225111, 0.62142223, 0.62326664, 0.6216889, 0.62733334, 0.6271778, 0.6263555, 0.6276444, 0.62946665, 0.6291556, 0.63175553, 0.6302222, 0.63251114, 0.63193333, 0.63204443, 0.6330444, 0.63902223, 0.63384444, 0.6354222, 0.63735557, 0.63368887, 0.6359556, 0.63611114, 0.6389111, 0.63964444, 0.6369333, 0.6382667, 0.64206666, 0.64086664, 0.6418222, 0.64115554, 0.6411778, 0.6412, 0.6436, 0.64566666, 0.64433336, 0.6452444, 0.64735556, 0.64573336, 0.6467111, 0.6476, 0.64442223, 0.6466889, 0.64964443, 0.6488889, 0.64835554, 0.649, 0.6499556, 0.65151113, 0.65037775, 0.6474444, 0.64915556, 0.6519778, 0.6518222, 0.6531111, 0.6531778, 0.6557556, 0.65566665, 0.65246665, 0.6557556, 0.65124446, 0.6572222, 0.6570889, 0.6565111, 0.65326667, 0.6576889, 0.6542889, 0.656, 0.6550889, 0.6578444, 0.6576889, 0.65628886, 0.6586, 0.6575556, 0.6598667, 0.6606445, 0.6608, 0.6623778, 0.65937775, 0.6572222, 0.66206664, 0.6606445, 0.6616, 0.6620889, 0.6596, 0.6650222, 0.6609778, 0.66595554, 0.66095555, 0.6631111, 0.6647111, 0.66466665, 0.66433334, 0.6637333, 0.6649778, 0.6666222, 0.6659778, 0.6642889, 0.6621778, 0.6644222, 0.6658889, 0.66775554, 0.6658, 0.6669111, 0.6663778, 0.67017776, 0.67053336, 0.66724443, 0.6712889, 0.6671111, 0.668, 0.6692889, 0.66815555, 0.6710889, 0.6708, 0.6714, 0.66873336, 0.6704889, 0.66646665, 0.67095554, 0.67095554, 0.67053336, 0.6717333, 0.6691778, 0.6693778, 0.67091113, 0.6690889, 0.6716667, 0.6713333, 0.6724667, 0.67404443, 0.6733556, 0.67417777, 0.6732889, 0.6716667, 0.6734222, 0.6757333, 0.672, 0.6742, 0.67446667, 0.67435557, 0.6749111, 0.67593336, 0.6772889]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 500 epochs, hs = 256 out.848400
################# cnn_gru_True Validation Accuracy =  [0.3422, 0.4174, 0.4266, 0.4656, 0.4878, 0.4868, 0.5108, 0.4958, 0.5298, 0.5346, 0.5252, 0.5476, 0.5532, 0.5586, 0.5608, 0.56, 0.5424, 0.552, 0.565, 0.5654, 0.5628, 0.5552, 0.566, 0.5534, 0.5706, 0.57, 0.5748, 0.5714, 0.5522, 0.581, 0.5688, 0.5702, 0.5862, 0.5836, 0.5872, 0.5894, 0.5886, 0.5872, 0.5716, 0.5824, 0.5968, 0.5756, 0.5814, 0.5984, 0.6004, 0.588, 0.5806, 0.5666, 0.5892, 0.5862, 0.6026, 0.6034, 0.5834, 0.6026, 0.588, 0.5896, 0.589, 0.5998, 0.6068, 0.5786, 0.5922, 0.5984, 0.588, 0.5906, 0.6004, 0.5922, 0.5968, 0.5908, 0.5972, 0.5956, 0.6088, 0.5998, 0.5846, 0.609, 0.6006, 0.5986, 0.5984, 0.595, 0.6062, 0.5976, 0.6038, 0.5802, 0.6034, 0.593, 0.5772, 0.6036, 0.61, 0.599, 0.594, 0.6002, 0.6044, 0.592, 0.604, 0.6078, 0.591, 0.5972, 0.6098, 0.5998, 0.6018, 0.5908, 0.5952, 0.614, 0.6072, 0.603, 0.5918, 0.603, 0.6098, 0.6048, 0.606, 0.5926, 0.6008, 0.5958, 0.5998, 0.607, 0.6032, 0.6086, 0.5964, 0.608, 0.6158, 0.5996, 0.5914, 0.6034, 0.603, 0.6036, 0.6128, 0.5926, 0.613, 0.608, 0.6028, 0.602, 0.6024, 0.612, 0.604, 0.6016, 0.6036, 0.5968, 0.6098, 0.6142, 0.5884, 0.6148, 0.5884, 0.5962, 0.6038, 0.6088, 0.6098, 0.5998, 0.602, 0.6018, 0.6102, 0.6006, 0.6066, 0.6016, 0.609, 0.6046, 0.5858, 0.6038, 0.6022, 0.6066, 0.6052, 0.6014, 0.603, 0.5988, 0.598, 0.6032, 0.609, 0.6096, 0.6096, 0.5942, 0.6008, 0.5954, 0.5966, 0.6092, 0.6054, 0.5938, 0.6022, 0.6036, 0.6066, 0.5944, 0.5964, 0.6042, 0.6046, 0.5956, 0.6056, 0.6048, 0.6092, 0.6034, 0.6014, 0.6008, 0.5894, 0.5952, 0.6084, 0.6072, 0.608, 0.6064, 0.6062, 0.6026, 0.599, 0.595, 0.5918, 0.6014, 0.5986, 0.6024, 0.5964, 0.6014, 0.6036, 0.6006, 0.6052, 0.5994, 0.605, 0.6022, 0.6058, 0.6006, 0.6038, 0.5968, 0.6096, 0.598, 0.6094, 0.5934, 0.6022, 0.604, 0.6044, 0.5962, 0.5952, 0.6002, 0.607, 0.6152, 0.6024, 0.5966, 0.6064, 0.6066, 0.6078, 0.6096, 0.6076, 0.6092, 0.598, 0.6006, 0.604, 0.6048, 0.6094, 0.6078, 0.5972, 0.6056, 0.5918, 0.6028, 0.5942, 0.5938, 0.5986, 0.602, 0.5932, 0.6038, 0.6024, 0.6042, 0.5962, 0.5994, 0.6064, 0.6028, 0.6044, 0.6074, 0.606, 0.6006, 0.5976, 0.6048, 0.608, 0.6004, 0.598, 0.6062, 0.5986, 0.5984, 0.6084, 0.6106, 0.6048, 0.5988, 0.5934, 0.5998, 0.6094, 0.6014, 0.6024, 0.6076, 0.6012, 0.6098, 0.6066, 0.6018, 0.6056, 0.5964, 0.609, 0.6002, 0.5914, 0.6038, 0.5978, 0.6022, 0.598, 0.6034, 0.6032, 0.6058, 0.608, 0.6082, 0.6048, 0.608, 0.6088, 0.6108, 0.598, 0.6016, 0.6194, 0.6022, 0.6106, 0.616, 0.5984, 0.6086, 0.6124, 0.6126, 0.6032, 0.6102, 0.6154, 0.606, 0.6088, 0.6006, 0.601, 0.5996, 0.6024, 0.6094, 0.6088, 0.604, 0.5984, 0.6076, 0.606, 0.6062, 0.6068, 0.6022, 0.6122, 0.6036, 0.6082, 0.6, 0.608, 0.6104, 0.6032, 0.6082, 0.606, 0.6076, 0.6082, 0.6086, 0.6002, 0.5988, 0.5968, 0.6116, 0.5958, 0.6006, 0.5976, 0.5986, 0.606, 0.6088, 0.6, 0.6066, 0.606, 0.6048, 0.6128, 0.6148, 0.6074, 0.606, 0.6038, 0.6014, 0.6088, 0.591, 0.6028, 0.6108, 0.6042, 0.596, 0.6042, 0.6084, 0.6064, 0.6104, 0.5972, 0.604, 0.607, 0.6078, 0.6062, 0.6054, 0.6052, 0.6122, 0.6028, 0.6034, 0.6042, 0.6114, 0.6056, 0.6072, 0.6006, 0.6014, 0.5964, 0.6074, 0.5986, 0.61, 0.603, 0.601, 0.6156, 0.6092, 0.6018, 0.603, 0.6056, 0.613, 0.6078, 0.6044, 0.6134, 0.6088, 0.612, 0.607, 0.5956, 0.6046, 0.6078, 0.5996, 0.612, 0.6066, 0.6052, 0.6046, 0.607, 0.6124, 0.5974, 0.6032, 0.6022, 0.6074, 0.6016, 0.6124, 0.5958, 0.6084, 0.5974, 0.597, 0.5938, 0.603, 0.6044, 0.612, 0.6006, 0.6048, 0.605, 0.5996, 0.603, 0.6054, 0.605, 0.6014, 0.6058, 0.5986, 0.603, 0.603, 0.6018, 0.5996, 0.6074, 0.6138, 0.6052, 0.5958, 0.5992, 0.6008, 0.6004, 0.5978, 0.6022, 0.6096, 0.6016, 0.599, 0.604, 0.6032, 0.6, 0.6056, 0.6116, 0.6002, 0.6028, 0.6002, 0.6038, 0.6056, 0.6078, 0.5992, 0.6094, 0.6082, 0.6, 0.602, 0.6034, 0.6102, 0.6114, 0.6104, 0.6136, 0.6012, 0.6062, 0.609, 0.6106, 0.5994, 0.6104, 0.6082, 0.5986, 0.6128, 0.6068, 0.5956, 0.6094, 0.6056, 0.604, 0.6074, 0.6092, 0.6052, 0.609, 0.6018, 0.5988, 0.603, 0.6046, 0.6136, 0.601, 0.6096]
################# cnn_gru_True Training Accuracy =  [0.25786668, 0.3790222, 0.4166, 0.44213334, 0.45866665, 0.47633332, 0.48535556, 0.50006664, 0.50684446, 0.51404446, 0.52144444, 0.5278222, 0.5355333, 0.54028887, 0.5446, 0.54928887, 0.55237776, 0.55777776, 0.5623111, 0.5605556, 0.56864446, 0.57137775, 0.57482225, 0.5767556, 0.5815333, 0.58213335, 0.58206666, 0.5878222, 0.5881111, 0.58835554, 0.59375554, 0.5946222, 0.5930222, 0.59404445, 0.59864444, 0.6018222, 0.6023778, 0.60584444, 0.60555553, 0.60855556, 0.61115557, 0.60906667, 0.6112222, 0.6102, 0.6166667, 0.6162, 0.61568886, 0.6193333, 0.62104446, 0.61957777, 0.62513334, 0.6252667, 0.6237778, 0.6237556, 0.62633336, 0.62648886, 0.62726665, 0.6284889, 0.62704444, 0.6317111, 0.6308889, 0.6312, 0.6331111, 0.6336667, 0.63615555, 0.6372, 0.63375556, 0.63975555, 0.63442224, 0.6397111, 0.64255553, 0.64144444, 0.64077777, 0.6404222, 0.6431778, 0.6439111, 0.64435554, 0.6445111, 0.6450222, 0.64213336, 0.6482, 0.6462889, 0.64706665, 0.6511111, 0.6474222, 0.6480889, 0.6508222, 0.64915556, 0.65268886, 0.64933336, 0.6503111, 0.6513111, 0.6528889, 0.6526667, 0.65404445, 0.6509333, 0.6538, 0.6513778, 0.65573335, 0.65655553, 0.6541333, 0.65477777, 0.65444446, 0.6593111, 0.6591778, 0.6595778, 0.65826666, 0.66051114, 0.6603778, 0.66026664, 0.659, 0.6608667, 0.65797776, 0.6610889, 0.66084445, 0.6592444, 0.66, 0.6586889, 0.66231114, 0.66215557, 0.6639111, 0.6621111, 0.66371113, 0.6640222, 0.66415554, 0.6679556, 0.6629111, 0.6644, 0.6658222, 0.6660445, 0.6674889, 0.6696444, 0.6634222, 0.66653335, 0.6698, 0.66893333, 0.669, 0.6704222, 0.66926664, 0.6688222, 0.66642225, 0.6698667, 0.6676222, 0.6658889, 0.6681111, 0.66704446, 0.6712222, 0.67017776, 0.6698222, 0.6735111, 0.6719111, 0.6718, 0.6729111, 0.67315555, 0.6712667, 0.67226666, 0.67506665, 0.6686444, 0.6717333, 0.6743778, 0.67602223, 0.67553335, 0.6758889, 0.67446667, 0.67624444, 0.6772889, 0.6788667, 0.6779111, 0.6726889, 0.6772444, 0.6759111, 0.6738, 0.67546666, 0.6734222, 0.67833334, 0.6772889, 0.6770222, 0.6786, 0.6766222, 0.6764889, 0.6778889, 0.67606664, 0.6789111, 0.67928886, 0.6781778, 0.6788222, 0.68126667, 0.6812222, 0.67973334, 0.6762, 0.6797778, 0.68186665, 0.67995554, 0.6798, 0.6818445, 0.6811111, 0.6828222, 0.68024445, 0.6838889, 0.682, 0.68144447, 0.6811111, 0.68135554, 0.6801111, 0.6824, 0.68222225, 0.6816889, 0.67984444, 0.6815778, 0.68197775, 0.6831333, 0.68146664, 0.68053335, 0.6860222, 0.68604445, 0.68237776, 0.6853333, 0.6854, 0.6826444, 0.6863111, 0.68366665, 0.6824667, 0.6824889, 0.684, 0.68531114, 0.6867333, 0.6889778, 0.68464446, 0.6875111, 0.69002223, 0.6878, 0.68851113, 0.68542224, 0.6865778, 0.6861111, 0.6869111, 0.6848222, 0.6862222, 0.6854, 0.6863111, 0.68866664, 0.6878667, 0.6876, 0.68891114, 0.68546665, 0.68855554, 0.68815553, 0.6881111, 0.6870222, 0.6885333, 0.68806666, 0.68997777, 0.6918, 0.69086665, 0.6901778, 0.68635553, 0.6895555, 0.6906889, 0.6894, 0.68833333, 0.6897111, 0.68891114, 0.6886, 0.68795556, 0.6924, 0.6933778, 0.6904, 0.69211113, 0.6924, 0.6911333, 0.69093335, 0.68993336, 0.69042224, 0.6904889, 0.6910222, 0.6911778, 0.6888667, 0.6914, 0.6926444, 0.6955778, 0.69064444, 0.6924222, 0.69362223, 0.69233334, 0.69306666, 0.69122225, 0.6976, 0.6951333, 0.69173336, 0.69368887, 0.6961778, 0.6952, 0.69604445, 0.6980889, 0.6949111, 0.6916889, 0.6931111, 0.6956, 0.6932667, 0.69353336, 0.697, 0.6961333, 0.6938, 0.69346666, 0.69442225, 0.6922889, 0.69626665, 0.6917111, 0.6957333, 0.69722223, 0.6960889, 0.6982, 0.69773334, 0.69226664, 0.6975778, 0.69533336, 0.6971111, 0.69475555, 0.6984, 0.6978667, 0.69593334, 0.6959778, 0.6983111, 0.69575554, 0.6993778, 0.6959111, 0.6962, 0.69935554, 0.6978, 0.696, 0.69902223, 0.69673336, 0.6992889, 0.6993778, 0.6979111, 0.6999556, 0.6964222, 0.70004445, 0.6965333, 0.69884443, 0.6974889, 0.69713336, 0.7003111, 0.7003555, 0.7014, 0.69457775, 0.7014667, 0.69924444, 0.7006889, 0.6995111, 0.7011778, 0.7010222, 0.6969333, 0.70262223, 0.7001333, 0.7018667, 0.69795555, 0.6986, 0.7020444, 0.7001778, 0.7016444, 0.7002, 0.70111114, 0.69891113, 0.7023778, 0.70324445, 0.70346665, 0.70306665, 0.70228887, 0.7036222, 0.7012445, 0.6997111, 0.6986667, 0.70246667, 0.70431113, 0.70162225, 0.7001111, 0.7006889, 0.69895554, 0.7040667, 0.70306665, 0.7046889, 0.7016889, 0.70026666, 0.7020889, 0.70413333, 0.70615554, 0.7049556, 0.7029333, 0.7014889, 0.70184445, 0.70464444, 0.70408887, 0.7024, 0.70368886, 0.7046, 0.70493335, 0.7007333, 0.7032889, 0.70882225, 0.7028, 0.70486665, 0.70482224, 0.7062889, 0.70166665, 0.70786667, 0.704, 0.7037778, 0.7055111, 0.7028889, 0.70342225, 0.7040667, 0.70306665, 0.70435554, 0.7055778, 0.7054667, 0.7053111, 0.70566666, 0.7066444, 0.70442224, 0.70768887, 0.70593333, 0.70526665, 0.70604444, 0.7021111, 0.7046667, 0.7046, 0.70886666, 0.70624447, 0.7060889, 0.70622224, 0.7082667, 0.7096222, 0.7075111, 0.70575553, 0.7061778, 0.70728886, 0.7036667, 0.70233333, 0.7112, 0.7081556, 0.70831114, 0.70795554, 0.70633334, 0.7097333, 0.7103111, 0.70684445, 0.7074444, 0.7085111, 0.7087333, 0.7066444, 0.7101333, 0.7085111, 0.7079333, 0.7072222, 0.70857775, 0.7102444, 0.70644444, 0.7094667, 0.70773333, 0.70717776, 0.70966667, 0.71055555, 0.7103556, 0.70813334, 0.70915556, 0.7103556, 0.70926666, 0.7116445, 0.7065333, 0.7049111, 0.7116889, 0.7102444, 0.70795554, 0.7082222, 0.7115778, 0.70904446, 0.70948887, 0.7095111, 0.70964444, 0.7116, 0.70773333, 0.70982224, 0.7082, 0.7102, 0.70713335, 0.7127111, 0.7073333, 0.7090667, 0.7134445, 0.71062225, 0.7124, 0.7098, 0.7069111, 0.71, 0.70924443, 0.71117777, 0.7089555, 0.7138889, 0.7097333]
'''

from __future__ import division, print_function, absolute_import

print('Starting..................................')
import sys
sys.path.insert(1, '/home/labs/ahissarlab/orra/imagewalker')
import numpy as np
import cv2
import misc
import pandas as pd
import matplotlib.pyplot as plt
import pickle

from keras_utils import *
from misc import *

import tensorflow.keras as keras
import tensorflow as tf

from tensorflow.keras.datasets import cifar10

# load dataset
(trainX, trainy), (testX, testy) = cifar10.load_data()
images, labels = trainX, trainy


#Define function for low resolution lens on syclop
def bad_res101(img,res):
    sh=np.shape(img)
    dwnsmp=cv2.resize(img,res, interpolation = cv2.INTER_CUBIC)
    upsmp = cv2.resize(dwnsmp,sh[:2], interpolation = cv2.INTER_CUBIC)
    return upsmp

def bad_res102(img,res):
    sh=np.shape(img)
    dwnsmp=cv2.resize(img,res, interpolation = cv2.INTER_AREA)
    return dwnsmp

import importlib
importlib.reload(misc)
from misc import Logger
import os 


def deploy_logs():
    if not os.path.exists(hp.save_path):
        os.makedirs(hp.save_path)

    dir_success = False
    for sfx in range(1):  # todo legacy
        candidate_path = hp.save_path + '/' + hp.this_run_name + '_' + str(os.getpid()) + '/'
        if not os.path.exists(candidate_path):
            hp.this_run_path = candidate_path
            os.makedirs(hp.this_run_path)
            dir_success = Truecnn_net = cnn_one_img(n_timesteps = sample, input_size = 28, input_dim = 1)
            break
    if not dir_success:
        error('run name already exists!')

    sys.stdout = Logger(hp.this_run_path+'log.log')
    print('results are in:', hp.this_run_path)
    print('description: ', hp.description)
    #print('hyper-parameters (partial):', hp.dict)

epochs = int(sys.argv[1])

sample = int(sys.argv[2])

res = int(sys.argv[3])

hidden_size = int(sys.argv[4])
   
cnn_dropout = 0.4

rnn_dropout = 0.2

lr = 5e-4

n_timesteps = sample
def split_dataset_xy(dataset):
    dataset_x1 = [uu[0] for uu in dataset]
    dataset_x2 = [uu[1] for uu in dataset]
    dataset_y = [uu[-1] for uu in dataset]
    return (np.array(dataset_x1),np.array(dataset_x2)[:,:n_timesteps,:]),np.array(dataset_y)

def cnn_gru(n_timesteps = 5, hidden_size = 128,input_size = 32, concat = True):
    '''
    
    CNN RNN combination that extends the CNN to a network that achieves 
    ~80% accuracy on full res cifar.

    Parameters
    ----------
    n_timesteps : TYPE, optional
        DESCRIPTION. The default is 5.
    img_dim : TYPE, optional
        DESCRIPTION. The default is 32.
    hidden_size : TYPE, optional
        DESCRIPTION. The default is 128.
    input_size : TYPE, optional
        DESCRIPTION. The default is 32.

    Returns
    -------
    model : TYPE
        DESCRIPTION.

    '''
    inputA = keras.layers.Input(shape=(n_timesteps,input_size,input_size,3))
    inputB = keras.layers.Input(shape=(n_timesteps,2))

    # define CNN model

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same'))(inputA)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)
    print(x1.shape)


    x1=keras.layers.TimeDistributed(keras.layers.Flatten())(x1)
    print(x1.shape)
    if concat:
        x = keras.layers.Concatenate()([x1,inputB])
    else:
        x = x1
    print(x.shape)

    # define LSTM model
    x = keras.layers.GRU(hidden_size,input_shape=(n_timesteps, None),return_sequences=True,recurrent_dropout=rnn_dropout)(x)
    x = keras.layers.Flatten()(x)
    x = keras.layers.Dense(10,activation="softmax")(x)
    model = keras.models.Model(inputs=[inputA,inputB],outputs=x, name = 'cnn_gru_{}'.format(concat))
    opt=tf.keras.optimizers.Adam(lr=lr)

    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=["sparse_categorical_accuracy"],
    )
    return model

rnn_net = cnn_gru(n_timesteps = sample, hidden_size = hidden_size,input_size = res, concat = True)
#cnn_net = cnn_net = extended_cnn_one_img(n_timesteps = sample, input_size = res, dropout = cnn_dropout)


# hp = HP()
# hp.save_path = 'saved_runs'

# hp.description = "syclop cifar net search runs"
# hp.this_run_name = 'syclop_{}'.format(rnn_net.name)
# deploy_logs()

train_dataset, test_dataset = create_cifar_dataset(images, labels,res = res,
                                    sample = sample, return_datasets=True, 
                                    mixed_state = False, add_seed = 0,
                                    )
                                    #bad_res_func = bad_res101, up_sample = True)

train_dataset_x, train_dataset_y = split_dataset_xy(train_dataset)
test_dataset_x, test_dataset_y = split_dataset_xy(test_dataset)

# print("##################### Fit {} and trajectories model on training data res = {} ##################".format(cnn_net.name,res))
# cnn_history = cnn_net.fit(
#     train_dataset_x,
#     train_dataset_y,
#     batch_size=64,
#     epochs=epochs,
#     # We pass some validation for
#     # monitoring validation loss and metrics
#     # at the end of each epoch
#     validation_data=(test_dataset_x, test_dataset_y),
#     verbose = 0)
# print('################# {} Validation Accuracy = '.format(cnn_net.name),cnn_history.history['val_sparse_categorical_accuracy'])
print("##################### Fit {} and trajectories model on training data res = {} ##################".format(rnn_net.name,res))
rnn_history = rnn_net.fit(
    train_dataset_x,
    train_dataset_y,
    batch_size=64,
    epochs=epochs,
    # We pass some validation for
    # monitoring validation loss and metrics
    # at the end of each epoch
    validation_data=(test_dataset_x, test_dataset_y),
    verbose = 0)

# print('################# {} Validation Accuracy = '.format(cnn_net.name),cnn_history.history['val_sparse_categorical_accuracy'])
# print('################# {} Training Accuracy = '.format(cnn_net.name),rnn_history.history['sparse_categorical_accuracy'])

print('################# {} Validation Accuracy = '.format(rnn_net.name),rnn_history.history['val_sparse_categorical_accuracy'])
print('################# {} Training Accuracy = '.format(rnn_net.name),rnn_history.history['sparse_categorical_accuracy'])


plt.figure()
plt.plot(rnn_history.history['sparse_categorical_accuracy'], label = 'train')
plt.plot(rnn_history.history['val_sparse_categorical_accuracy'], label = 'val')
# plt.plot(cnn_history.history['sparse_categorical_accuracy'], label = 'cnn train')
# plt.plot(cnn_history.history['val_sparse_categorical_accuracy'], label = 'cnn val')
plt.legend()
plt.title('{} on cifar res = {} hs = {} dropout = {}, num samples = {}'.format(rnn_net.name, res, hidden_size,cnn_dropout,sample))
plt.savefig('{} on Cifar res = {}, no upsample, val accur = {} hs = {} dropout = {}.png'.format(rnn_net.name,res,rnn_history.history['val_sparse_categorical_accuracy'][-1], hidden_size,cnn_dropout))

with open('/home/labs/ahissarlab/orra/imagewalker/cifar_net_search/{}HistoryDict{}_{}'.format(rnn_net.name, hidden_size,cnn_dropout), 'wb') as file_pi:
    pickle.dump(rnn_history.history, file_pi)
    
# with open('/home/labs/ahissarlab/orra/imagewalker/cifar_net_search/{}HistoryDict'.format(cnn_net.name), 'wb') as file_pi:
#     pickle.dump(cnn_history.history, file_pi)
    