'''
The follwing code runs a test lstm network on the CIFAR dataset 

I will explicitly write the networks here for ease of understanding 

with cnn_sropout = 0.4 and rnn dropout = 0.2and lr = 1e-3 and res = 8
################# cnn_gru_True Validation Accuracy =  [0.3408, 0.411, 0.44, 0.4448, 0.466, 0.4684, 0.4802, 0.4846, 0.4848, 0.512, 0.5098, 0.5154, 0.5212, 0.5276, 0.5352, 0.5306, 0.5354, 0.5388, 0.5374, 0.5418, 0.55, 0.537, 0.5556, 0.543, 0.5458, 0.548, 0.5462, 0.554, 0.5596, 0.5438]
################# cnn_gru_True Training Accuracy =  [0.2734222, 0.3752889, 0.40646666, 0.42904446, 0.44386667, 0.45495555, 0.46284443, 0.47604445, 0.4802889, 0.48911113, 0.4968222, 0.4992, 0.50622225, 0.51126665, 0.5147333, 0.52275556, 0.5224444, 0.52537775, 0.5287778, 0.53275555, 0.53286666, 0.5396444, 0.5384222, 0.5423333, 0.542, 0.5485333, 0.547, 0.5458, 0.5524222, 0.55104446]

with cnn_sropout = 0.4 and rnn dropout = 0.2and lr = 1e-3 and res = 16
################# extended_cnn_one_img Validation Accuracy =  [0.416, 0.4696, 0.5168, 0.5424, 0.557, 0.5658, 0.5782, 0.5884, 0.5902, 0.5978, 0.5996, 0.6034, 0.6122, 0.606, 0.6112, 0.6104, 0.618, 0.6158, 0.6162, 0.6132, 0.6132, 0.6178, 0.6122, 0.626, 0.6168, 0.6164, 0.62, 0.6288, 0.6304, 0.6328]
################# extended_cnn_one_img Training Accuracy =  [0.2964, 0.42106667, 0.46775556, 0.49335554, 0.51544446, 0.52937776, 0.5436889, 0.5556889, 0.56684446, 0.57053334, 0.5798444, 0.58955556, 0.5917778, 0.59702224, 0.6014444, 0.60657775, 0.6142222, 0.6137556, 0.6195111, 0.6193111, 0.6226444, 0.6248, 0.6245555, 0.62575555, 0.6321333, 0.6330889, 0.6327556, 0.63677776, 0.63571113, 0.6396889]
################# cnn_convlstm_True Validation Accuracy =  [0.4038, 0.4724, 0.521, 0.5402, 0.52, 0.5516, 0.5658, 0.5654, 0.5904, 0.5866, 0.6024, 0.6026, 0.6114, 0.6224, 0.5982, 0.6178, 0.6314, 0.6208, 0.6158, 0.6352, 0.6412, 0.63, 0.6424, 0.6278, 0.6336, 0.6278, 0.646, 0.6272, 0.6414, 0.6406]
################# cnn_convlstm_True Training Accuracy =  [0.2964, 0.42106667, 0.46775556, 0.49335554, 0.51544446, 0.52937776, 0.5436889, 0.5556889, 0.56684446, 0.57053334, 0.5798444, 0.58955556, 0.5917778, 0.59702224, 0.6014444, 0.60657775, 0.6142222, 0.6137556, 0.6195111, 0.6193111, 0.6226444, 0.6248, 0.6245555, 0.62575555, 0.6321333, 0.6330889, 0.6327556, 0.63677776, 0.63571113, 0.6396889]

with cnn_sropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 out.812929
################# cnn_gru_True Validation Accuracy =  [0.3452, 0.41, 0.4206, 0.4382, 0.4626, 0.4786, 0.481, 0.4984, 0.5006, 0.5038, 0.5112, 0.5022, 0.522, 0.527, 0.5314, 0.5362, 0.5434,
 0.53, 0.543, 0.5534, 0.5528, 0.5456, 0.548, 0.5492, 0.5602, 0.5662, 0.5554, 0.5626, 0.5732, 0.5608, 0.5612, 0.5678, 0.578, 0.5572, 0.575, 0.5674, 0.5674, 0.5678, 0.574, 0.5832, 0.567, 0.5676, 0.5872, 0.5856, 0.5908, 0.5916, 0.586, 0.5628, 0.582, 0.5772, 0.5702, 0.5756, 0.5792, 0.5726, 0.59, 0.5784, 0.576, 0.5752, 0.5894, 0.5844, 0.583, 0.5832, 0.5782, 0.5696, 0.5812, 0.589, 0.5818, 0.5826, 0.5922, 0.5896, 0.5816, 0.5798, 0.5818, 0.5834, 0.5822, 0.5836, 0.5828, 0.569, 0.5914, 0.5822, 0.5974, 0.5928, 0.5956, 0.5936, 0.5888, 0.5932, 0.5986, 0.593, 0.5802, 0.5878, 0.5876, 0.5846, 0.6018, 0.5932, 0.5862, 0.5898, 0.5902, 0.5948, 0.5952, 0.596]
################# cnn_gru_True Training Accuracy =  [0.2522, 0.35944444, 0.40026668, 0.42453334, 0.4369111, 0.45024446, 0.46413332, 0.47453332, 0.47904444, 0.48753333, 0.4946, 0.50115556, 0.50531113, 0.5134, 0.5142, 0.5196222, 0.5276667, 0.529, 0.5313778, 0.5318889, 0.5356445, 0.54084444, 0.54051113, 0.5448889, 0.54855555, 0.5504444, 0.5562889, 0.5566889, 0.55655557, 0.5622889, 0.5615111, 0.5605111, 0.5638, 0.56615555, 0.5662444, 0.56953335, 0.5730444, 0.5717555, 0.5730444, 0.57368886, 0.5764889, 0.5782222, 0.58004445, 0.5802889, 0.5833778, 0.5824222, 0.58437777, 0.5869111, 0.58375555, 0.5871556, 0.5907556, 0.58444446, 0.58846664, 0.5914889, 0.59033334, 0.59257776, 0.5913333, 0.59606665, 0.5928222, 0.59577775, 0.5945333, 0.59613335, 0.5953556, 0.59786665, 0.5990222, 0.5993556, 0.60215557, 0.60344446, 0.6027111, 0.60364443, 0.6039111, 0.6062222, 0.60364443, 0.6062667, 0.6060445, 0.6081333, 0.6075778, 0.6094, 0.60568887, 0.6079556, 0.6064444, 0.61113334, 0.61322224, 0.6088667, 0.6125778, 0.61248887, 0.61282223, 0.61244446, 0.6136444, 0.61337775, 0.6174667, 0.61248887, 0.61535555, 0.6160667, 0.6134, 0.6155556, 0.6161111, 0.6158444, 0.61855555, 0.61642224]

with cnn_sropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 500 epochs out.813849 
################# cnn_gru_True Validation Accuracy =  [0.3136, 0.4024, 0.4436, 0.4546, 0.4648, 0.4552, 0.4766, 0.5058, 0.5028, 0.5182, 0.522, 0.5142, 0.5306, 0.5324, 0.5302, 0.5424, 0.5392, 0.543, 0.5328, 0.5276, 0.5474, 0.549, 0.5512, 0.5326, 0.5482, 0.5558, 0.5548, 0.5594, 0.5546, 0.566, 0.559, 0.5674, 0.564, 0.5584, 0.5698, 0.5718, 0.567, 0.5618, 0.5632, 0.574, 0.5696, 0.5758, 0.5636, 0.5744, 0.5706, 0.5734, 0.5508, 0.5692, 0.5802, 0.5704, 0.572, 0.5706, 0.5888, 0.5828, 0.583, 0.5812, 0.5872, 0.5748, 0.5844, 0.5784, 0.5838, 0.5862, 0.5826, 0.5838, 0.5894, 0.5942, 0.5932, 0.5818, 0.5836, 0.5914, 0.592, 0.5956, 0.5772, 0.5936, 0.5908, 0.5808, 0.5898, 0.5734, 0.578, 0.5868, 0.578, 0.5998, 0.59, 0.5956, 0.5708, 0.585, 0.5902, 0.5922, 0.5826, 0.5936, 0.5916, 0.5846, 0.6012, 0.5852, 0.5892, 0.592, 0.5806, 0.5938, 0.5916, 0.5866, 0.5952, 0.5944, 0.5956, 0.59, 0.592, 0.5922, 0.5962, 0.5906, 0.6006, 0.5912, 0.596, 0.6004, 0.596, 0.5838, 0.5918, 0.581, 0.5912, 0.587, 0.5942, 0.586, 0.591, 0.5906, 0.583, 0.5874, 0.5976, 0.5866, 0.5884, 0.5894, 0.5968, 0.5992, 0.5912, 0.5932, 0.5828, 0.5958, 0.5878, 0.5888, 0.595, 0.5948, 0.5898, 0.5956, 0.5896, 0.5942, 0.5938, 0.5884, 0.5874, 0.5954, 0.5908, 0.5948, 0.5972, 0.5986, 0.5984, 0.5952, 0.589, 0.5892, 0.6044, 0.6028, 0.5944, 0.591, 0.6018, 0.5932, 0.5982, 0.5896, 0.598, 0.6026, 0.6028, 0.6034, 0.5916, 0.5952, 0.5932, 0.597, 0.6008, 0.6026, 0.5974, 0.5954, 0.6014, 0.5988, 0.606, 0.6056, 0.5944, 0.6048, 0.6084, 0.6026, 0.599, 0.6022, 0.6022, 0.6022, 0.601, 0.5928, 0.5988, 0.6008, 0.599, 0.6016, 0.6036, 0.6056, 0.6142, 0.6064, 0.6082, 0.6032, 0.5974, 0.6082, 0.61, 0.6032, 0.6018, 0.6026, 0.6088, 0.6014, 0.6022, 0.6094, 0.6034, 0.5938, 0.6066, 0.5838, 0.5978, 0.6012, 0.5988, 0.6062, 0.6044, 0.5946, 0.597, 0.5954, 0.5944, 0.594, 0.5934, 0.5984, 0.6038, 0.607, 0.6056, 0.5948, 0.604, 0.6012, 0.5988, 0.608, 0.601, 0.6016, 0.5996, 0.6008, 0.6048, 0.6076, 0.6038, 0.6058, 0.6038, 0.6078, 0.5968, 0.605, 0.6046, 0.5982, 0.6002, 0.6092, 0.5956, 0.605, 0.6006, 0.5998, 0.5922, 0.6044, 0.5946, 0.602, 0.6008, 0.6068, 0.6018, 0.602, 0.594, 0.6046, 0.5992, 0.6006, 0.5962, 0.6092, 0.6026, 0.5984, 0.6078, 0.6024, 0.6048, 0.6032, 0.598, 0.6072, 0.6014, 0.5888, 0.6136, 0.605, 0.6032, 0.6032, 0.5988, 0.6014, 0.5988, 0.6054, 0.6038, 0.599, 0.5976, 0.5962, 0.602, 0.6028, 0.6082, 0.5936, 0.6052, 0.6014, 0.6022, 0.5976, 0.606, 0.6038, 0.6018, 0.6066, 0.601, 0.6038, 0.601, 0.6028, 0.6104, 0.5994, 0.6048, 0.5996, 0.6054, 0.597, 0.6042, 0.6048, 0.5962, 0.5968, 0.6036, 0.598, 0.6002, 0.593, 0.5972, 0.6024, 0.6018, 0.6102, 0.601, 0.6038, 0.594, 0.6068, 0.606, 0.6138, 0.6048, 0.602, 0.591, 0.6118, 0.6074, 0.5994, 0.5962, 0.6048, 0.6006, 0.6058, 0.6026, 0.6032, 0.6028, 0.608, 0.6036, 0.5968, 0.6004, 0.6054, 0.601, 0.6038, 0.6058, 0.6052, 0.5996, 0.6044, 0.598, 0.5986, 0.6018, 0.6002, 0.6064, 0.6064, 0.5918, 0.6004, 0.601, 0.605, 0.5974, 0.608, 0.608, 0.5968, 0.6042, 0.6034, 0.5984, 0.597, 0.6006, 0.6038, 0.603, 0.6004, 0.594, 0.5924, 0.5986, 0.5994, 0.6108, 0.5988, 0.6052, 0.6006, 0.6028, 0.602, 0.6016, 0.5996, 0.6012, 0.6014, 0.6042, 0.5988, 0.6064, 0.5982, 0.6, 0.6066, 0.609, 0.6096, 0.5948, 0.605, 0.6036, 0.5952, 0.6086, 0.6008, 0.5934, 0.6066, 0.608, 0.5998, 0.6042, 0.6016, 0.6018, 0.6062, 0.6068, 0.6194, 0.6032, 0.6116, 0.6058, 0.6022, 0.6056, 0.6, 0.6034, 0.6054, 0.6124, 0.6092, 0.603, 0.6016, 0.6018, 0.6084, 0.6026, 0.6154, 0.6034, 0.6118, 0.6102, 0.601, 0.603, 0.606, 0.6114, 0.6024, 0.6112, 0.6094, 0.6026, 0.598, 0.6074, 0.6066, 0.602, 0.6058, 0.603, 0.6078, 0.604, 0.605, 0.607, 0.605, 0.6044, 0.6026, 0.6006, 0.5988, 0.6056, 0.6016, 0.6054, 0.6004, 0.6024, 0.6092, 0.5954, 0.5962, 0.6036, 0.6008, 0.602, 0.6088, 0.6022, 0.6052, 0.5982, 0.6036, 0.601, 0.5956, 0.6024, 0.6104, 0.6028, 0.5898, 0.5994, 0.5946, 0.6054, 0.6064, 0.6102, 0.609, 0.6024, 0.599, 0.601, 0.6074, 0.6018, 0.595, 0.6034, 0.6028, 0.6008, 0.5996, 0.5992, 0.6006, 0.5996, 0.6018, 0.5968, 0.6016, 0.602, 0.6018]
################# cnn_gru_True Training Accuracy =  [0.26466668, 0.36813334, 0.40513334, 0.4256, 0.44268888, 0.4564222, 0.46568888, 0.4769111, 0.48531112, 0.491, 0.49744445, 0.50593334, 0.5138222, 0.51564443, 0.5213778, 0.5223778, 0.5283778, 0.5326222, 0.53275555, 0.53764445, 0.54586667, 0.5451556, 0.54735553, 0.5526, 0.5533111, 0.55424446, 0.5568889, 0.56262225, 0.5646, 0.5660667, 0.56333333, 0.5680889, 0.5706889, 0.5710889, 0.5733111, 0.5754667, 0.57637775, 0.5764667, 0.5768222, 0.5766889, 0.57817775, 0.5839555, 0.5825111, 0.5855778, 0.58424443, 0.5876, 0.58786666, 0.58806664, 0.58966666, 0.5938445, 0.5907111, 0.5939556, 0.59331113, 0.59475553, 0.5945333, 0.59515554, 0.59853333, 0.59635556, 0.6008667, 0.59893334, 0.5993556, 0.6007111, 0.6008889, 0.6032889, 0.6000444, 0.6049778, 0.60246664, 0.60384446, 0.60564446, 0.6048889, 0.6089778, 0.6061111, 0.60966665, 0.60686666, 0.60895556, 0.60973334, 0.60944444, 0.6095778, 0.6099778, 0.6114889, 0.6125778, 0.6149333, 0.61322224, 0.6185333, 0.6148, 0.61682224, 0.6157333, 0.6142, 0.6166222, 0.6152, 0.6158222, 0.61653334, 0.62155557, 0.6175333, 0.6168889, 0.61995554, 0.6193778, 0.6175778, 0.6207111, 0.62277776, 0.62144446, 0.62013334, 0.62328887, 0.62633336, 0.62722224, 0.62171113, 0.6248222, 0.62586665, 0.6251778, 0.6256889, 0.6254, 0.6249111, 0.62648886, 0.62468886, 0.6260889, 0.6276, 0.6266, 0.6273556, 0.6258444, 0.6287778, 0.6277111, 0.63026667, 0.6285333, 0.62846667, 0.62813336, 0.6326889, 0.6296, 0.63177776, 0.6323778, 0.6324, 0.63215554, 0.63104445, 0.6322889, 0.6328667, 0.63173336, 0.63515556, 0.6334, 0.63575554, 0.63404447, 0.6330444, 0.63526666, 0.6344444, 0.6337778, 0.63335556, 0.63386667, 0.6336222, 0.6369333, 0.63553333, 0.63713336, 0.63677776, 0.6365333, 0.6353111, 0.6347333, 0.6371111, 0.637, 0.63688886, 0.6344, 0.6371111, 0.636, 0.6394889, 0.638, 0.63946664, 0.63566667, 0.63857776, 0.6413111, 0.6376889, 0.63493335, 0.6387111, 0.6397778, 0.64055556, 0.64073336, 0.63766664, 0.6411333, 0.6392222, 0.6402444, 0.6413556, 0.64077777, 0.6387333, 0.6377778, 0.63884443, 0.64177775, 0.6401111, 0.64, 0.6415111, 0.64166665, 0.6448, 0.6414667, 0.64228886, 0.6416889, 0.63975555, 0.6437778, 0.6429778, 0.6421555, 0.64346665, 0.64155555, 0.64284444, 0.6429333, 0.64415556, 0.64611113, 0.64555556, 0.6452444, 0.64522225, 0.64824444, 0.64275557, 0.64593333, 0.64662224, 0.6431556, 0.6444, 0.6441111, 0.64482224, 0.6471556, 0.64584446, 0.6441778, 0.6448, 0.6446, 0.64775556, 0.64764446, 0.64677775, 0.646, 0.6472222, 0.6472, 0.6481111, 0.6465333, 0.6469778, 0.6510222, 0.64677775, 0.6503556, 0.647, 0.64944446, 0.64655554, 0.64724445, 0.65128887, 0.64955556, 0.6482222, 0.6444889, 0.6488, 0.64797777, 0.6509111, 0.6520444, 0.65022224, 0.6516, 0.645, 0.65044445, 0.64702225, 0.65264446, 0.6487778, 0.64944446, 0.6492222, 0.6536889, 0.6499778, 0.6486222, 0.6539556, 0.64806664, 0.6488, 0.65055555, 0.6541778, 0.6518667, 0.6526667, 0.65155554, 0.6526, 0.65202224, 0.64977777, 0.65315557, 0.65128887, 0.64773333, 0.6536222, 0.65335554, 0.6523778, 0.6494, 0.6510889, 0.6496889, 0.6514, 0.65117776, 0.65375555, 0.65415555, 0.6495778, 0.65055555, 0.6507556, 0.65346664, 0.6548, 0.65115553, 0.6553111, 0.6517778, 0.6532889, 0.6548, 0.6546222, 0.65533334, 0.6521556, 0.6543555, 0.65217775, 0.65275556, 0.6522, 0.65555555, 0.65482223, 0.6541111, 0.6546889, 0.65533334, 0.6541111, 0.6554, 0.6537333, 0.6537778, 0.6528444, 0.65331113, 0.65455556, 0.6544, 0.65477777, 0.6572667, 0.65606666, 0.6556, 0.65606666, 0.6553556, 0.65353334, 0.6518, 0.6536667, 0.65595555, 0.65775555, 0.65657777, 0.6549778, 0.65764445, 0.6557111, 0.6556, 0.6590222, 0.6538889, 0.6591778, 0.65444446, 0.6562, 0.6564, 0.6607778, 0.6556444, 0.65826666, 0.6562, 0.6581333, 0.6578889, 0.65853333, 0.6584, 0.65782225, 0.6594667, 0.6552, 0.6586667, 0.658, 0.6588, 0.66135556, 0.65668887, 0.6561555, 0.6581111, 0.6599111, 0.6588, 0.6568, 0.6608667, 0.6603778, 0.6602889, 0.6592, 0.6594667, 0.65706664, 0.6567111, 0.6608667, 0.65886664, 0.65966666, 0.66035557, 0.66175556, 0.65584445, 0.65966666, 0.6606889, 0.65922225, 0.6595111, 0.65515554, 0.65984446, 0.6612667, 0.6605333, 0.662, 0.6613778, 0.6611556, 0.6580667, 0.66135556, 0.65882224, 0.65655553, 0.65955555, 0.65988886, 0.6593556, 0.65808886, 0.6616667, 0.6614222, 0.6634, 0.6632222, 0.6618, 0.6599778, 0.66013336, 0.6608, 0.66146666, 0.65944445, 0.65966666, 0.66135556, 0.66004443, 0.6608222, 0.6630222, 0.6620889, 0.66195554, 0.6582222, 0.6606445, 0.6629556, 0.66164446, 0.66055554, 0.6608889, 0.66175556, 0.6606, 0.6614222, 0.6640222, 0.66364443, 0.6643556, 0.66191113, 0.6626667, 0.6630222, 0.6656889, 0.6631333, 0.66293335, 0.6617778, 0.6610889, 0.6614889, 0.662, 0.6593111, 0.6612667, 0.66102225, 0.6631333, 0.66395557, 0.66282225, 0.66713333, 0.6623778, 0.6648222, 0.6622667, 0.66746664, 0.6616667, 0.6630222, 0.6622, 0.6624, 0.66415554, 0.662, 0.6612222, 0.6618222, 0.6629111, 0.66426665, 0.66315556, 0.6640667, 0.6640889, 0.66533333, 0.6626, 0.6617778, 0.66477776, 0.6654889, 0.66477776, 0.6624889, 0.6622222, 0.6642, 0.6663111, 0.66293335, 0.6636889, 0.6643556, 0.6652, 0.6680889, 0.6658222, 0.66415554, 0.6677778, 0.6622889, 0.6688, 0.6630222, 0.66848886, 0.66355556, 0.6624889, 0.6658222, 0.66602224, 0.6631778, 0.6618889, 0.6654222, 0.6662889, 0.66726667, 0.66384447, 0.6662, 0.66477776, 0.6650889, 0.66293335, 0.66484445, 0.66371113, 0.6646, 0.6661556, 0.66191113, 0.6656889, 0.6649333, 0.66686666, 0.66544443, 0.66624445, 0.66455555, 0.6698222, 0.6665556, 0.6648, 0.6663111, 0.66455555, 0.6653778, 0.6675556, 0.66404444, 0.66484445, 0.66617775]

with cnn_dropout = 0.2 and rnn dropout = 0.2and lr = 5e-4 with res = 8 out.812847
################# cnn_gru_True Validation Accuracy =  [0.3598, 0.4126, 0.4454, 0.4714, 0.4722, 0.506, 0.5062, 0.5154, 0.5382, 0.5296, 0.5368, 0.5352, 0.5364, 0.5584, 0.5564, 0.5624, 0.5
704, 0.5622, 0.5612, 0.5568, 0.5656, 0.5572, 0.572, 0.5718, 0.569, 0.576, 0.5718, 0.5726, 0.5732, 0.5754, 0.5758, 0.5754, 0.5802, 0.5778, 0.5778, 0.5818, 0.5808, 0.573, 0.5764, 0.5782, 0.578, 0.5828, 0.5656, 0.5796, 0.5704, 0.5808, 0.5764, 0.5774, 0.5644, 0.5794, 0.5794, 0.5834, 0.57, 0.5724, 0.5806, 0.5784, 0.5794, 0.5834, 0.5756, 0.5786, 0.5802, 0.5746, 0.571, 0.5812, 0.569, 0.5724, 0.5794, 0.5762, 0.581, 0.5664, 0.574, 0.5782, 0.5738, 0.5714, 0.5754, 0.5716, 0.5638, 0.5696, 0.5706, 0.5758, 0.567, 0.571, 0.5716, 0.5788, 0.559, 0.5682, 0.5716, 0.5728, 0.5718, 0.5758, 0.569, 0.573, 0.5756, 0.5746, 0.5744, 0.571, 0.5762, 0.5792, 0.5688, 0.5796]
################# cnn_gru_True Training Accuracy =  [0.27786666, 0.3842222, 0.42204446, 0.44537777, 0.4655111, 0.48406667, 0.49457777, 0.50564444, 0.5188889, 0.5279111, 0.5366667, 0.544, 0.5515111, 0.5573556, 0.56457776, 0.5718222, 0.5748889, 0.5826667, 0.5850222, 0.5921556, 0.59155554, 0.5960889, 0.6028889, 0.60664445, 0.6115556, 0.61553335, 0.61968887, 0.6218889, 0.6240444, 0.6262222, 0.6306889, 0.6329778, 0.6356, 0.6404, 0.6475111, 0.6451333, 0.64626664, 0.6536889, 0.65573335, 0.65842223, 0.65977776, 0.6573111, 0.6640889, 0.6664, 0.66866666, 0.6700889, 0.6704222, 0.6747556, 0.6781333, 0.6785111, 0.67693335, 0.68086666, 0.68293333, 0.6823111, 0.6862444, 0.69013333, 0.69044447, 0.6957778, 0.6952, 0.6944889, 0.69953334, 0.6963111, 0.7000222, 0.7018667, 0.7029333, 0.7018222, 0.70446664, 0.7051111, 0.7105778, 0.70993334, 0.71308887, 0.71331114, 0.71128887, 0.7160444, 0.7176222, 0.71793336, 0.71846664, 0.72062224, 0.7216222, 0.7220889, 0.72117776, 0.72617775, 0.72535557, 0.72904444, 0.72675556, 0.73215556, 0.7297556, 0.72926664, 0.7349333, 0.73224443, 0.7335778, 0.73744446, 0.73384446, 0.73735553, 0.73744446, 0.7404889, 0.73928887, 0.742, 0.7410667, 0.7395778]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 500 epochs out.813851
################# cnn_gru_True Validation Accuracy =  [0.3354, 0.4208, 0.4522, 0.463, 0.4448, 0.4934, 0.5048, 0.5036, 0.5082, 0.5202, 0.4958, 0.5184, 0.5302, 0.5364, 0.5474, 0.5298, 0.5382, 0.5446, 0.5486, 0.5496, 0.5468, 0.5616, 0.5516, 0.5542, 0.5606, 0.5624, 0.5744, 0.5644, 0.5624, 0.5712, 0.5714, 0.5746, 0.5638, 0.5622, 0.5768, 0.5792, 0.5852, 0.5758, 0.5768, 0.5708, 0.5882, 0.5814, 0.5778, 0.5884, 0.5892, 0.5862, 0.5828, 0.5838, 0.5892, 0.58, 0.595, 0.5872, 0.58, 0.5868, 0.5888, 0.592, 0.5848, 0.5824, 0.5852, 0.5832, 0.5898, 0.5846, 0.584, 0.5942, 0.5858, 0.5918, 0.5826, 0.597, 0.5984, 0.5928, 0.5802, 0.5972, 0.5976, 0.5964, 0.5894, 0.5888, 0.5948, 0.5944, 0.594, 0.5934, 0.5952, 0.5976, 0.5994, 0.6002, 0.5926, 0.5984, 0.5976, 0.591, 0.593, 0.6076, 0.5888, 0.6018, 0.5908, 0.5964, 0.5966, 0.5968, 0.5912, 0.5976, 0.5912, 0.597, 0.5934, 0.588, 0.6014, 0.592, 0.5952, 0.606, 0.6026, 0.5932, 0.6, 0.5944, 0.5898, 0.5914, 0.5976, 0.6008, 0.5894, 0.6058, 0.6038, 0.5974, 0.5996, 0.6064, 0.6014, 0.5914, 0.6012, 0.5922, 0.5938, 0.6008, 0.6058, 0.6046, 0.6012, 0.593, 0.6, 0.6046, 0.5946, 0.5962, 0.592, 0.5968, 0.5946, 0.5966, 0.5968, 0.588, 0.6004, 0.6008, 0.592, 0.5976, 0.5998, 0.5854, 0.6012, 0.5994, 0.5908, 0.5996, 0.6056, 0.5924, 0.5974, 0.5986, 0.5926, 0.5938, 0.5902, 0.5924, 0.598, 0.5988, 0.6028, 0.601, 0.5976, 0.597, 0.6044, 0.5894, 0.5904, 0.6, 0.595, 0.5974, 0.5998, 0.594, 0.5946, 0.5968, 0.5938, 0.5858, 0.6016, 0.5934, 0.6052, 0.598, 0.608, 0.6, 0.6008, 0.5956, 0.591, 0.6024, 0.6076, 0.5986, 0.5974, 0.6004, 0.6046, 0.597, 0.6048, 0.588, 0.5902, 0.5868, 0.5928, 0.5986, 0.5994, 0.5962, 0.5946, 0.594, 0.5972, 0.592, 0.5916, 0.589, 0.6042, 0.5908, 0.5922, 0.5924, 0.5902, 0.5914, 0.6026, 0.5992, 0.5956, 0.5954, 0.6034, 0.5906, 0.6052, 0.5918, 0.6, 0.6004, 0.5912, 0.5942, 0.5972, 0.6066, 0.5946, 0.5972, 0.5854, 0.5994, 0.5954, 0.592, 0.5904, 0.5956, 0.5946, 0.5838, 0.5872, 0.5948, 0.5972, 0.5996, 0.605, 0.5962, 0.604, 0.5976, 0.6, 0.6016, 0.6014, 0.6044, 0.5928, 0.598, 0.6, 0.59, 0.5978, 0.5902, 0.5934, 0.6026, 0.5956, 0.6012, 0.5932, 0.6, 0.5952, 0.602, 0.5942, 0.5988, 0.6024, 0.597, 0.5964, 0.5882, 0.6008, 0.5958, 0.6006, 0.5964, 0.594, 0.5882, 0.6028, 0.6032, 0.5982, 0.6, 0.5988, 0.6018, 0.6028, 0.609, 0.6032, 0.5954, 0.5988, 0.6074, 0.6014, 0.6086, 0.6002, 0.605, 0.603, 0.6058, 0.6084, 0.5894, 0.6046, 0.6006, 0.605, 0.5972, 0.5964, 0.5972, 0.603, 0.5986, 0.601, 0.5972, 0.6058, 0.6028, 0.596, 0.603, 0.598, 0.6008, 0.5958, 0.5906, 0.6024, 0.6024, 0.6014, 0.6078, 0.6006, 0.5996, 0.603, 0.6068, 0.6046, 0.6064, 0.5948, 0.5988, 0.6074, 0.6024, 0.605, 0.5974, 0.6014, 0.6054, 0.5966, 0.6006, 0.601, 0.592, 0.6108, 0.5944, 0.6008, 0.599, 0.6072, 0.6034, 0.5964, 0.6104, 0.592, 0.6044, 0.6026, 0.6032, 0.6058, 0.6094, 0.6042, 0.6062, 0.6016, 0.6084, 0.6028, 0.608, 0.604, 0.6012, 0.6012, 0.6072, 0.6008, 0.607, 0.6018, 0.597, 0.6008, 0.6092, 0.6044, 0.594, 0.6026, 0.6082, 0.6078, 0.6092, 0.6064, 0.6052, 0.6052, 0.6004, 0.6078, 0.6102, 0.6, 0.615, 0.605, 0.5942, 0.6044, 0.6084, 0.6002, 0.6034, 0.5998, 0.5982, 0.5974, 0.598, 0.601, 0.597, 0.6062, 0.6036, 0.6048, 0.599, 0.604, 0.607, 0.6036, 0.5992, 0.6018, 0.6022, 0.6044, 0.5984, 0.6006, 0.5986, 0.6056, 0.6062, 0.5942, 0.6032, 0.6026, 0.5994, 0.6064, 0.599, 0.6008, 0.5986, 0.5984, 0.5962, 0.5972, 0.6016, 0.6014, 0.604, 0.6026, 0.6002, 0.6076, 0.605, 0.5988, 0.6006, 0.6006, 0.5992, 0.5994, 0.6016, 0.601, 0.5924, 0.597, 0.5998, 0.6012, 0.6064, 0.5968, 0.6012, 0.604, 0.603, 0.602, 0.595, 0.6044, 0.5952, 0.6016, 0.6058, 0.6012, 0.6042, 0.5966, 0.6054, 0.6066, 0.6016, 0.594, 0.6042, 0.607, 0.6038, 0.5942, 0.6064, 0.6044, 0.6022, 0.6056, 0.6036, 0.594, 0.605, 0.6042, 0.6062, 0.591, 0.5988, 0.6056, 0.608, 0.6014, 0.605, 0.5996, 0.6046, 0.6066, 0.6032, 0.5998, 0.6028, 0.6, 0.5948, 0.6046, 0.6066, 0.603, 0.6038, 0.6066, 0.6034, 0.6034, 0.5978, 0.6014, 0.602, 0.592, 0.6008, 0.6066, 0.6046, 0.6072, 0.6106, 0.6062, 0.6074, 0.5986, 0.6034]
################# cnn_gru_True Training Accuracy =  [0.24648888, 0.3745778, 0.41557777, 0.43804446, 0.4576, 0.4678, 0.47815555, 0.4868, 0.49584445, 0.5020667, 0.50942224, 0.5155333, 0.51953334, 0.52253336, 0.5287778, 0.5311555, 0.5374, 0.5400222, 0.54744446, 0.54553336, 0.55102223, 0.55517775, 0.5588667, 0.55873334, 0.56222224, 0.56906664, 0.56704444, 0.57048887, 0.5709556, 0.57553333, 0.58104444, 0.57677776, 0.5827111, 0.5832, 0.58533335, 0.5862667, 0.5885556, 0.5909333, 0.5918, 0.59326667, 0.5958222, 0.5950222, 0.59848887, 0.59871113, 0.6015555, 0.60064447, 0.60433334, 0.6062222, 0.6030667, 0.6063333, 0.6067333, 0.6074889, 0.60944444, 0.6112889, 0.61002225, 0.61248887, 0.6134, 0.61333334, 0.6154, 0.6148, 0.61473334, 0.618, 0.6176222, 0.61884445, 0.6212889, 0.62226665, 0.6203778, 0.62186664, 0.6224667, 0.626, 0.6241111, 0.6243333, 0.62524444, 0.6258889, 0.6276444, 0.62704444, 0.62773335, 0.62866664, 0.62637776, 0.62784445, 0.63368887, 0.63137776, 0.63233334, 0.6337778, 0.63453335, 0.6339778, 0.6327556, 0.6346667, 0.6375333, 0.63571113, 0.6359111, 0.63633335, 0.63897777, 0.6382667, 0.6386667, 0.6386667, 0.6402, 0.6410889, 0.63853335, 0.6414222, 0.6431111, 0.64084446, 0.6423333, 0.6404222, 0.64386666, 0.6427778, 0.64442223, 0.64526665, 0.6431778, 0.6445111, 0.6468222, 0.6451333, 0.6484889, 0.64537776, 0.64544445, 0.6438889, 0.65073335, 0.6497333, 0.6512667, 0.6492222, 0.64784443, 0.64622223, 0.6495111, 0.6498, 0.6488889, 0.6512667, 0.6499111, 0.6527333, 0.6570889, 0.65253335, 0.65371114, 0.65015554, 0.6525111, 0.6505778, 0.64982224, 0.65437776, 0.6553778, 0.6556889, 0.6545333, 0.65713334, 0.65573335, 0.6571111, 0.65706664, 0.6573333, 0.65397775, 0.6564889, 0.6561111, 0.65691113, 0.65595555, 0.6564889, 0.6577778, 0.65757775, 0.6575111, 0.65835553, 0.6568889, 0.65746665, 0.65602225, 0.6579111, 0.65724444, 0.6560444, 0.6582222, 0.65844446, 0.6604667, 0.6612667, 0.6575111, 0.6612667, 0.6634222, 0.6617333, 0.6640889, 0.6603111, 0.66286665, 0.66135556, 0.6610889, 0.6615555, 0.6611556, 0.6604889, 0.66477776, 0.6643556, 0.6623333, 0.6612222, 0.66353333, 0.6625556, 0.66186666, 0.66333336, 0.66395557, 0.66355556, 0.66575557, 0.66433334, 0.6652, 0.6616667, 0.66602224, 0.6647556, 0.6646444, 0.66708887, 0.6645333, 0.6630667, 0.66844445, 0.6675111, 0.668, 0.6643556, 0.6670222, 0.6701111, 0.6662222, 0.66546667, 0.66364443, 0.6655333, 0.6684667, 0.6691778, 0.66922224, 0.6661111, 0.6691778, 0.66804445, 0.6721333, 0.6696889, 0.66775554, 0.66642225, 0.6698, 0.66884446, 0.6692889, 0.66713333, 0.66962224, 0.6699778, 0.67197776, 0.6676222, 0.6693556, 0.66926664, 0.67282224, 0.6721778, 0.6653111, 0.67164445, 0.6734222, 0.66951114, 0.67384446, 0.6722, 0.6716889, 0.6684667, 0.67164445, 0.6717778, 0.6716, 0.67102224, 0.6719555, 0.6747111, 0.6744222, 0.67253333, 0.672, 0.67362225, 0.6738222, 0.6768889, 0.6722, 0.67182225, 0.67775553, 0.6749111, 0.67495555, 0.6774667, 0.67304444, 0.6748667, 0.6732889, 0.67513335, 0.6786444, 0.6725111, 0.6751111, 0.6779111, 0.6733111, 0.6766667, 0.67653334, 0.6767778, 0.67755556, 0.6733556, 0.6755111, 0.67646664, 0.67513335, 0.6769556, 0.6732, 0.6803778, 0.67642224, 0.67595553, 0.6792667, 0.6769111, 0.6782889, 0.67833334, 0.67917776, 0.67422223, 0.67873335, 0.6778889, 0.67495555, 0.677, 0.67962223, 0.68053335, 0.6788222, 0.67664444, 0.6814, 0.681, 0.67826664, 0.6806222, 0.68153334, 0.6809555, 0.6798667, 0.6808889, 0.67764443, 0.6803111, 0.6794222, 0.67646664, 0.6801111, 0.6809111, 0.6828667, 0.67866665, 0.68137777, 0.6797111, 0.67991114, 0.67913336, 0.6791111, 0.68164444, 0.68042225, 0.68126667, 0.6821333, 0.6833111, 0.6835778, 0.67884445, 0.68593335, 0.6798, 0.67928886, 0.682, 0.6838667, 0.6833111, 0.68648887, 0.6845111, 0.6812889, 0.6846222, 0.6825778, 0.6810222, 0.68273336, 0.68315554, 0.6806667, 0.68648887, 0.68295556, 0.6824, 0.6821111, 0.681, 0.6835333, 0.68524444, 0.68455553, 0.6817333, 0.6833111, 0.6825333, 0.68675554, 0.6819111, 0.68475556, 0.6879333, 0.68473333, 0.68384445, 0.6862222, 0.6841111, 0.6841111, 0.68277776, 0.6884, 0.6818, 0.6853778, 0.6822444, 0.68637776, 0.6852889, 0.68615556, 0.6869556, 0.6840444, 0.6870667, 0.68564445, 0.68497777, 0.68531114, 0.6839111, 0.6844, 0.68924445, 0.68635553, 0.68484443, 0.6872, 0.6852889, 0.6884889, 0.68435556, 0.68475556, 0.6860667, 0.68664443, 0.6854889, 0.6857333, 0.68864447, 0.6874889, 0.6874, 0.6852889, 0.6850889, 0.6857778, 0.6856889, 0.6898444, 0.6896667, 0.6880222, 0.68762225, 0.68873334, 0.68815553, 0.6851111, 0.68813336, 0.6874667, 0.69233334, 0.6897111, 0.6887778, 0.68846667, 0.6905778, 0.6882222, 0.69188887, 0.6883111, 0.6878, 0.6901111, 0.6859556, 0.68902224, 0.69188887, 0.6915778, 0.69206667, 0.6874889, 0.6928, 0.689, 0.6896, 0.6896667, 0.6893111, 0.68997777, 0.6876, 0.6924667, 0.6876889, 0.6892222, 0.6910889, 0.6886, 0.6886889, 0.69391114, 0.6886889, 0.69284445, 0.69211113, 0.6900667, 0.6905556, 0.6885778, 0.6871333, 0.69188887, 0.69204444, 0.6908, 0.693, 0.69355553, 0.69211113, 0.6909556, 0.6921333, 0.6925333, 0.69126666, 0.69211113, 0.69277775, 0.6929111, 0.69075555, 0.69093335, 0.69075555, 0.6912, 0.68862224, 0.69346666, 0.6921778, 0.6904889, 0.69486666, 0.69166666, 0.6924, 0.69355553, 0.69373333, 0.6925111, 0.69295555, 0.69515556, 0.69184446, 0.69206667, 0.69537777, 0.6911111, 0.6930444, 0.69335556, 0.6888667, 0.69364446, 0.6946222, 0.6948444, 0.6927111, 0.6944444, 0.6907333, 0.69357777, 0.6952222, 0.69155556, 0.6915333, 0.69537777, 0.6924889, 0.69035554, 0.69366664, 0.6966, 0.6922, 0.6918667, 0.6926, 0.6960667, 0.6926, 0.69564444, 0.69328886, 0.6952889, 0.6944444, 0.69571114, 0.69546664, 0.694, 0.6939333, 0.6952889, 0.6956667]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 5 samples and 200 epochs, hs = 256 out.836806
################# cnn_gru_True Validation Accuracy =  [0.3074, 0.3502, 0.3972, 0.4236, 0.4458, 0.4612, 0.478, 0.4846, 0.4832, 0.494, 0.4936, 0.5028, 0.511, 0.5, 0.4942, 0.5186, 0.5216, 0.5274, 0.5356, 0.5306, 0.5296, 0.535, 0.5346, 0.5346, 0.5448, 0.534, 0.5384, 0.5442, 0.5434, 0.539, 0.5478, 0.552, 0.549, 0.5404, 0.5448, 0.5434, 0.5568, 0.5462, 0.5462, 0.5558, 0.5612, 0.5484, 0.5606, 0.5666, 0.5698, 0.5582, 0.5578, 0.5744, 0.56, 0.5466, 0.5554, 0.563, 0.5592, 0.5566, 0.5674, 0.5536, 0.5606, 0.5678, 0.5618, 0.559, 0.5676, 0.571, 0.563, 0.5646, 0.563, 0.5732, 0.565, 0.5738, 0.572, 0.5774, 0.5652, 0.5636, 0.5688, 0.5718, 0.5734, 0.558, 0.571, 0.577, 0.5674, 0.579, 0.5706, 0.5764, 0.567, 0.5772, 0.5738, 0.5688, 0.5706, 0.5712, 0.575, 0.5748, 0.5804, 0.5708, 0.566, 0.57, 0.5768, 0.5814, 0.569, 0.5796, 0.5776, 0.5702, 0.5806, 0.5834, 0.5708, 0.5748, 0.5794, 0.585, 0.5792, 0.5738, 0.5736, 0.5776, 0.5812, 0.5804, 0.5762, 0.5806, 0.5822, 0.5786, 0.5768, 0.5752, 0.5822, 0.5808, 0.5822, 0.5844, 0.5876, 0.589, 0.5872, 0.5764, 0.5808, 0.5738, 0.581, 0.5828, 0.5688, 0.577, 0.5798, 0.587, 0.5766, 0.5798, 0.5834, 0.5802, 0.5826, 0.578, 0.5786, 0.565, 0.5742, 0.5894, 0.5808, 0.5708, 0.5766, 0.5866, 0.5806, 0.577, 0.5794, 0.5802, 0.5776, 0.5824, 0.586, 0.574, 0.5804, 0.5834, 0.5834, 0.578, 0.5784, 0.571, 0.5668, 0.5798, 0.5792, 0.5748, 0.5824, 0.5628, 0.5814, 0.5796, 0.581, 0.575, 0.5802, 0.5786, 0.5802, 0.5852, 0.5818, 0.5826, 0.59, 0.5762, 0.59, 0.577, 0.5798, 0.5796, 0.581, 0.5806, 0.5774, 0.5772, 0.5798, 0.585, 0.588, 0.5856, 0.5836, 0.5858, 0.5842, 0.5826, 0.5818, 0.5764, 0.5814, 0.5812]
################# cnn_gru_True Training Accuracy =  [0.24148889, 0.35346666, 0.39324445, 0.4138, 0.4300222, 0.44151112, 0.45264444, 0.4583111, 0.46951112, 0.47684443, 0.48144445, 0.4867111, 0.4934, 0.49924445, 0.5006667, 0.5047333, 0.51008886, 0.5148889, 0.51364446, 0.5214889, 0.5223111, 0.5267556, 0.5283778, 0.52993333, 0.5365111, 0.5373333, 0.5393111, 0.5411556, 0.5418444, 0.5449778, 0.54704446, 0.55093336, 0.55095553, 0.5567778, 0.5597778, 0.5578, 0.55826664, 0.5587111, 0.56135553, 0.5613111, 0.5638222, 0.5689778, 0.5655111, 0.5698, 0.56924444, 0.57137775, 0.57251114, 0.57457775, 0.5736667, 0.578, 0.5786222, 0.5777556, 0.57964444, 0.5810889, 0.5809778, 0.5831556, 0.5817556, 0.584, 0.5824444, 0.5857111, 0.58357775, 0.58804446, 0.58624446, 0.5888444, 0.5892222, 0.59157777, 0.59275556, 0.5909333, 0.5932, 0.5918667, 0.59206665, 0.59437776, 0.5966, 0.5946889, 0.59984446, 0.59511113, 0.5969333, 0.6005333, 0.59893334, 0.5999333, 0.6010889, 0.60175556, 0.6009333, 0.6008, 0.60035557, 0.6005778, 0.6013778, 0.6052667, 0.6039111, 0.6061556, 0.60355556, 0.603, 0.60344446, 0.6076889, 0.6047556, 0.6068222, 0.60406667, 0.6079778, 0.60693336, 0.6074889, 0.6102889, 0.6061111, 0.61104447, 0.61002225, 0.6100444, 0.60866666, 0.6106, 0.61131114, 0.6118889, 0.61204445, 0.61377776, 0.61182225, 0.61311114, 0.61197776, 0.61635554, 0.6154889, 0.6140444, 0.61644447, 0.61704445, 0.61833334, 0.61795557, 0.6198222, 0.6174667, 0.6174, 0.61766666, 0.6165778, 0.6163778, 0.61793333, 0.61946666, 0.62144446, 0.6208444, 0.6163333, 0.61624444, 0.6175111, 0.62124443, 0.6211333, 0.6183778, 0.62288886, 0.6214667, 0.6212889, 0.6186889, 0.6230222, 0.62313336, 0.6221333, 0.6222, 0.62453336, 0.6224889, 0.6257333, 0.6224667, 0.6254445, 0.6226889, 0.62384444, 0.6247111, 0.6238889, 0.6228222, 0.6233778, 0.6265333, 0.6257333, 0.62604445, 0.6287111, 0.6253778, 0.6269111, 0.63024443, 0.6262889, 0.62766665, 0.62615556, 0.6257333, 0.6289778, 0.6282, 0.62615556, 0.62993336, 0.6257111, 0.6315111, 0.6270222, 0.6297333, 0.6268889, 0.6298222, 0.6300667, 0.6293333, 0.62995553, 0.6311333, 0.63037777, 0.6307333, 0.62993336, 0.6329111, 0.6297333, 0.63217777, 0.6298444, 0.6303333, 0.6312, 0.6305111, 0.6304, 0.6334444, 0.63204443, 0.63064444, 0.6292, 0.63317776, 0.63226664, 0.6315778, 0.6300667]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 5 samples and 500 epochs, hs = 256 out.848468

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 200 epochs, hs = 256 out.846686
################# cnn_gru_True Validation Accuracy =  [0.3584, 0.427, 0.4594, 0.4528, 0.4746, 0.4934, 0.5094, 0.5078, 0.5196, 0.5242, 0.5342, 0.5258, 0.5292, 0.533, 0.5444, 0.5422, 0.5572, 0.5486, 0.5644, 0.5618, 0.5692, 0.5666, 0.5764, 0.5676, 0.5674, 0.5466, 0.5744, 0.5802, 0.5782, 0.5784, 0.5742, 0.5786, 0.5762, 0.5692, 0.5916, 0.5654, 0.5772, 0.5744, 0.5854, 0.582, 0.5882, 0.5814, 0.595, 0.5838, 0.5866, 0.5888, 0.5876, 0.5888, 0.5866, 0.5782, 0.5958, 0.5926, 0.5914, 0.5778, 0.5944, 0.58, 0.5944, 0.5878, 0.5926, 0.5954, 0.595, 0.5844, 0.588, 0.5934, 0.5942, 0.598, 0.5974, 0.5944, 0.5924, 0.5944, 0.5908, 0.5952, 0.5966, 0.5966, 0.5992, 0.5966, 0.5956, 0.5836, 0.5956, 0.5832, 0.5938, 0.5992, 0.5976, 0.5952, 0.5904, 0.5906, 0.5924, 0.5878, 0.6094, 0.604, 0.5884, 0.5986, 0.5922, 0.5806, 0.5932, 0.5914, 0.603, 0.5888, 0.5892, 0.588, 0.5942, 0.6024, 0.5898, 0.5992, 0.6, 0.5928, 0.5958, 0.5824, 0.6004, 0.5842, 0.5914, 0.603, 0.5946, 0.5928, 0.5956, 0.5828, 0.608, 0.6058, 0.5928, 0.5934, 0.5938, 0.5958, 0.5952, 0.598, 0.5868, 0.6004, 0.5884, 0.593, 0.5936, 0.6094, 0.5996, 0.5984, 0.5976, 0.5984, 0.6084, 0.5964, 0.5886, 0.6, 0.6, 0.596, 0.5936, 0.6028, 0.5986, 0.5992, 0.5784, 0.5882, 0.5942, 0.598, 0.605, 0.5904, 0.6, 0.586, 0.5894, 0.5984, 0.5824, 0.5944, 0.5906, 0.5922, 0.588, 0.5952, 0.593, 0.5846, 0.5932, 0.5978, 0.5942, 0.5958, 0.5992, 0.5938, 0.5914, 0.5968, 0.5946, 0.5978, 0.6004, 0.588, 0.5982, 0.5992, 0.6012, 0.5976, 0.594, 0.5912, 0.5854, 0.5954, 0.5922, 0.5908, 0.5842, 0.6034, 0.5978, 0.6012, 0.5974, 0.5924, 0.5952, 0.6004, 0.5942, 0.6014, 0.5882, 0.5978, 0.5992, 0.5938, 0.5946, 0.6006]
################# cnn_gru_True Training Accuracy =  [0.25715557, 0.3822, 0.41933334, 0.4448889, 0.4602, 0.47442222, 0.48344445, 0.49475557, 0.5034222, 0.5122, 0.5181111, 0.5222, 0.5295778, 0.5335111, 0.54168886, 0.54411113, 0.54735553, 0.5506667, 0.5548667, 0.56093335, 0.5622444, 0.5642889, 0.56453335, 0.56953335, 0.57226664, 0.57644445, 0.57728887, 0.5796, 0.58304447, 0.58397776, 0.5872, 0.58673334, 0.58926666, 0.59195554, 0.59515554, 0.59691113, 0.59655553, 0.5989778, 0.60253334, 0.6033111, 0.60406667, 0.60415554, 0.6044889, 0.6035333, 0.6082444, 0.6112222, 0.60873336, 0.61075556, 0.61517775, 0.61646664, 0.61586666, 0.61855555, 0.6187111, 0.6170667, 0.62135553, 0.6203778, 0.6225111, 0.62142223, 0.62326664, 0.6216889, 0.62733334, 0.6271778, 0.6263555, 0.6276444, 0.62946665, 0.6291556, 0.63175553, 0.6302222, 0.63251114, 0.63193333, 0.63204443, 0.6330444, 0.63902223, 0.63384444, 0.6354222, 0.63735557, 0.63368887, 0.6359556, 0.63611114, 0.6389111, 0.63964444, 0.6369333, 0.6382667, 0.64206666, 0.64086664, 0.6418222, 0.64115554, 0.6411778, 0.6412, 0.6436, 0.64566666, 0.64433336, 0.6452444, 0.64735556, 0.64573336, 0.6467111, 0.6476, 0.64442223, 0.6466889, 0.64964443, 0.6488889, 0.64835554, 0.649, 0.6499556, 0.65151113, 0.65037775, 0.6474444, 0.64915556, 0.6519778, 0.6518222, 0.6531111, 0.6531778, 0.6557556, 0.65566665, 0.65246665, 0.6557556, 0.65124446, 0.6572222, 0.6570889, 0.6565111, 0.65326667, 0.6576889, 0.6542889, 0.656, 0.6550889, 0.6578444, 0.6576889, 0.65628886, 0.6586, 0.6575556, 0.6598667, 0.6606445, 0.6608, 0.6623778, 0.65937775, 0.6572222, 0.66206664, 0.6606445, 0.6616, 0.6620889, 0.6596, 0.6650222, 0.6609778, 0.66595554, 0.66095555, 0.6631111, 0.6647111, 0.66466665, 0.66433334, 0.6637333, 0.6649778, 0.6666222, 0.6659778, 0.6642889, 0.6621778, 0.6644222, 0.6658889, 0.66775554, 0.6658, 0.6669111, 0.6663778, 0.67017776, 0.67053336, 0.66724443, 0.6712889, 0.6671111, 0.668, 0.6692889, 0.66815555, 0.6710889, 0.6708, 0.6714, 0.66873336, 0.6704889, 0.66646665, 0.67095554, 0.67095554, 0.67053336, 0.6717333, 0.6691778, 0.6693778, 0.67091113, 0.6690889, 0.6716667, 0.6713333, 0.6724667, 0.67404443, 0.6733556, 0.67417777, 0.6732889, 0.6716667, 0.6734222, 0.6757333, 0.672, 0.6742, 0.67446667, 0.67435557, 0.6749111, 0.67593336, 0.6772889]

with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 500 epochs, hs = 256 out.848400
################# cnn_gru_True Validation Accuracy =  [0.3422, 0.4174, 0.4266, 0.4656, 0.4878, 0.4868, 0.5108, 0.4958, 0.5298, 0.5346, 0.5252, 0.5476, 0.5532, 0.5586, 0.5608, 0.56, 0.5424, 0.552, 0.565, 0.5654, 0.5628, 0.5552, 0.566, 0.5534, 0.5706, 0.57, 0.5748, 0.5714, 0.5522, 0.581, 0.5688, 0.5702, 0.5862, 0.5836, 0.5872, 0.5894, 0.5886, 0.5872, 0.5716, 0.5824, 0.5968, 0.5756, 0.5814, 0.5984, 0.6004, 0.588, 0.5806, 0.5666, 0.5892, 0.5862, 0.6026, 0.6034, 0.5834, 0.6026, 0.588, 0.5896, 0.589, 0.5998, 0.6068, 0.5786, 0.5922, 0.5984, 0.588, 0.5906, 0.6004, 0.5922, 0.5968, 0.5908, 0.5972, 0.5956, 0.6088, 0.5998, 0.5846, 0.609, 0.6006, 0.5986, 0.5984, 0.595, 0.6062, 0.5976, 0.6038, 0.5802, 0.6034, 0.593, 0.5772, 0.6036, 0.61, 0.599, 0.594, 0.6002, 0.6044, 0.592, 0.604, 0.6078, 0.591, 0.5972, 0.6098, 0.5998, 0.6018, 0.5908, 0.5952, 0.614, 0.6072, 0.603, 0.5918, 0.603, 0.6098, 0.6048, 0.606, 0.5926, 0.6008, 0.5958, 0.5998, 0.607, 0.6032, 0.6086, 0.5964, 0.608, 0.6158, 0.5996, 0.5914, 0.6034, 0.603, 0.6036, 0.6128, 0.5926, 0.613, 0.608, 0.6028, 0.602, 0.6024, 0.612, 0.604, 0.6016, 0.6036, 0.5968, 0.6098, 0.6142, 0.5884, 0.6148, 0.5884, 0.5962, 0.6038, 0.6088, 0.6098, 0.5998, 0.602, 0.6018, 0.6102, 0.6006, 0.6066, 0.6016, 0.609, 0.6046, 0.5858, 0.6038, 0.6022, 0.6066, 0.6052, 0.6014, 0.603, 0.5988, 0.598, 0.6032, 0.609, 0.6096, 0.6096, 0.5942, 0.6008, 0.5954, 0.5966, 0.6092, 0.6054, 0.5938, 0.6022, 0.6036, 0.6066, 0.5944, 0.5964, 0.6042, 0.6046, 0.5956, 0.6056, 0.6048, 0.6092, 0.6034, 0.6014, 0.6008, 0.5894, 0.5952, 0.6084, 0.6072, 0.608, 0.6064, 0.6062, 0.6026, 0.599, 0.595, 0.5918, 0.6014, 0.5986, 0.6024, 0.5964, 0.6014, 0.6036, 0.6006, 0.6052, 0.5994, 0.605, 0.6022, 0.6058, 0.6006, 0.6038, 0.5968, 0.6096, 0.598, 0.6094, 0.5934, 0.6022, 0.604, 0.6044, 0.5962, 0.5952, 0.6002, 0.607, 0.6152, 0.6024, 0.5966, 0.6064, 0.6066, 0.6078, 0.6096, 0.6076, 0.6092, 0.598, 0.6006, 0.604, 0.6048, 0.6094, 0.6078, 0.5972, 0.6056, 0.5918, 0.6028, 0.5942, 0.5938, 0.5986, 0.602, 0.5932, 0.6038, 0.6024, 0.6042, 0.5962, 0.5994, 0.6064, 0.6028, 0.6044, 0.6074, 0.606, 0.6006, 0.5976, 0.6048, 0.608, 0.6004, 0.598, 0.6062, 0.5986, 0.5984, 0.6084, 0.6106, 0.6048, 0.5988, 0.5934, 0.5998, 0.6094, 0.6014, 0.6024, 0.6076, 0.6012, 0.6098, 0.6066, 0.6018, 0.6056, 0.5964, 0.609, 0.6002, 0.5914, 0.6038, 0.5978, 0.6022, 0.598, 0.6034, 0.6032, 0.6058, 0.608, 0.6082, 0.6048, 0.608, 0.6088, 0.6108, 0.598, 0.6016, 0.6194, 0.6022, 0.6106, 0.616, 0.5984, 0.6086, 0.6124, 0.6126, 0.6032, 0.6102, 0.6154, 0.606, 0.6088, 0.6006, 0.601, 0.5996, 0.6024, 0.6094, 0.6088, 0.604, 0.5984, 0.6076, 0.606, 0.6062, 0.6068, 0.6022, 0.6122, 0.6036, 0.6082, 0.6, 0.608, 0.6104, 0.6032, 0.6082, 0.606, 0.6076, 0.6082, 0.6086, 0.6002, 0.5988, 0.5968, 0.6116, 0.5958, 0.6006, 0.5976, 0.5986, 0.606, 0.6088, 0.6, 0.6066, 0.606, 0.6048, 0.6128, 0.6148, 0.6074, 0.606, 0.6038, 0.6014, 0.6088, 0.591, 0.6028, 0.6108, 0.6042, 0.596, 0.6042, 0.6084, 0.6064, 0.6104, 0.5972, 0.604, 0.607, 0.6078, 0.6062, 0.6054, 0.6052, 0.6122, 0.6028, 0.6034, 0.6042, 0.6114, 0.6056, 0.6072, 0.6006, 0.6014, 0.5964, 0.6074, 0.5986, 0.61, 0.603, 0.601, 0.6156, 0.6092, 0.6018, 0.603, 0.6056, 0.613, 0.6078, 0.6044, 0.6134, 0.6088, 0.612, 0.607, 0.5956, 0.6046, 0.6078, 0.5996, 0.612, 0.6066, 0.6052, 0.6046, 0.607, 0.6124, 0.5974, 0.6032, 0.6022, 0.6074, 0.6016, 0.6124, 0.5958, 0.6084, 0.5974, 0.597, 0.5938, 0.603, 0.6044, 0.612, 0.6006, 0.6048, 0.605, 0.5996, 0.603, 0.6054, 0.605, 0.6014, 0.6058, 0.5986, 0.603, 0.603, 0.6018, 0.5996, 0.6074, 0.6138, 0.6052, 0.5958, 0.5992, 0.6008, 0.6004, 0.5978, 0.6022, 0.6096, 0.6016, 0.599, 0.604, 0.6032, 0.6, 0.6056, 0.6116, 0.6002, 0.6028, 0.6002, 0.6038, 0.6056, 0.6078, 0.5992, 0.6094, 0.6082, 0.6, 0.602, 0.6034, 0.6102, 0.6114, 0.6104, 0.6136, 0.6012, 0.6062, 0.609, 0.6106, 0.5994, 0.6104, 0.6082, 0.5986, 0.6128, 0.6068, 0.5956, 0.6094, 0.6056, 0.604, 0.6074, 0.6092, 0.6052, 0.609, 0.6018, 0.5988, 0.603, 0.6046, 0.6136, 0.601, 0.6096]
################# cnn_gru_True Training Accuracy =  [0.25786668, 0.3790222, 0.4166, 0.44213334, 0.45866665, 0.47633332, 0.48535556, 0.50006664, 0.50684446, 0.51404446, 0.52144444, 0.5278222, 0.5355333, 0.54028887, 0.5446, 0.54928887, 0.55237776, 0.55777776, 0.5623111, 0.5605556, 0.56864446, 0.57137775, 0.57482225, 0.5767556, 0.5815333, 0.58213335, 0.58206666, 0.5878222, 0.5881111, 0.58835554, 0.59375554, 0.5946222, 0.5930222, 0.59404445, 0.59864444, 0.6018222, 0.6023778, 0.60584444, 0.60555553, 0.60855556, 0.61115557, 0.60906667, 0.6112222, 0.6102, 0.6166667, 0.6162, 0.61568886, 0.6193333, 0.62104446, 0.61957777, 0.62513334, 0.6252667, 0.6237778, 0.6237556, 0.62633336, 0.62648886, 0.62726665, 0.6284889, 0.62704444, 0.6317111, 0.6308889, 0.6312, 0.6331111, 0.6336667, 0.63615555, 0.6372, 0.63375556, 0.63975555, 0.63442224, 0.6397111, 0.64255553, 0.64144444, 0.64077777, 0.6404222, 0.6431778, 0.6439111, 0.64435554, 0.6445111, 0.6450222, 0.64213336, 0.6482, 0.6462889, 0.64706665, 0.6511111, 0.6474222, 0.6480889, 0.6508222, 0.64915556, 0.65268886, 0.64933336, 0.6503111, 0.6513111, 0.6528889, 0.6526667, 0.65404445, 0.6509333, 0.6538, 0.6513778, 0.65573335, 0.65655553, 0.6541333, 0.65477777, 0.65444446, 0.6593111, 0.6591778, 0.6595778, 0.65826666, 0.66051114, 0.6603778, 0.66026664, 0.659, 0.6608667, 0.65797776, 0.6610889, 0.66084445, 0.6592444, 0.66, 0.6586889, 0.66231114, 0.66215557, 0.6639111, 0.6621111, 0.66371113, 0.6640222, 0.66415554, 0.6679556, 0.6629111, 0.6644, 0.6658222, 0.6660445, 0.6674889, 0.6696444, 0.6634222, 0.66653335, 0.6698, 0.66893333, 0.669, 0.6704222, 0.66926664, 0.6688222, 0.66642225, 0.6698667, 0.6676222, 0.6658889, 0.6681111, 0.66704446, 0.6712222, 0.67017776, 0.6698222, 0.6735111, 0.6719111, 0.6718, 0.6729111, 0.67315555, 0.6712667, 0.67226666, 0.67506665, 0.6686444, 0.6717333, 0.6743778, 0.67602223, 0.67553335, 0.6758889, 0.67446667, 0.67624444, 0.6772889, 0.6788667, 0.6779111, 0.6726889, 0.6772444, 0.6759111, 0.6738, 0.67546666, 0.6734222, 0.67833334, 0.6772889, 0.6770222, 0.6786, 0.6766222, 0.6764889, 0.6778889, 0.67606664, 0.6789111, 0.67928886, 0.6781778, 0.6788222, 0.68126667, 0.6812222, 0.67973334, 0.6762, 0.6797778, 0.68186665, 0.67995554, 0.6798, 0.6818445, 0.6811111, 0.6828222, 0.68024445, 0.6838889, 0.682, 0.68144447, 0.6811111, 0.68135554, 0.6801111, 0.6824, 0.68222225, 0.6816889, 0.67984444, 0.6815778, 0.68197775, 0.6831333, 0.68146664, 0.68053335, 0.6860222, 0.68604445, 0.68237776, 0.6853333, 0.6854, 0.6826444, 0.6863111, 0.68366665, 0.6824667, 0.6824889, 0.684, 0.68531114, 0.6867333, 0.6889778, 0.68464446, 0.6875111, 0.69002223, 0.6878, 0.68851113, 0.68542224, 0.6865778, 0.6861111, 0.6869111, 0.6848222, 0.6862222, 0.6854, 0.6863111, 0.68866664, 0.6878667, 0.6876, 0.68891114, 0.68546665, 0.68855554, 0.68815553, 0.6881111, 0.6870222, 0.6885333, 0.68806666, 0.68997777, 0.6918, 0.69086665, 0.6901778, 0.68635553, 0.6895555, 0.6906889, 0.6894, 0.68833333, 0.6897111, 0.68891114, 0.6886, 0.68795556, 0.6924, 0.6933778, 0.6904, 0.69211113, 0.6924, 0.6911333, 0.69093335, 0.68993336, 0.69042224, 0.6904889, 0.6910222, 0.6911778, 0.6888667, 0.6914, 0.6926444, 0.6955778, 0.69064444, 0.6924222, 0.69362223, 0.69233334, 0.69306666, 0.69122225, 0.6976, 0.6951333, 0.69173336, 0.69368887, 0.6961778, 0.6952, 0.69604445, 0.6980889, 0.6949111, 0.6916889, 0.6931111, 0.6956, 0.6932667, 0.69353336, 0.697, 0.6961333, 0.6938, 0.69346666, 0.69442225, 0.6922889, 0.69626665, 0.6917111, 0.6957333, 0.69722223, 0.6960889, 0.6982, 0.69773334, 0.69226664, 0.6975778, 0.69533336, 0.6971111, 0.69475555, 0.6984, 0.6978667, 0.69593334, 0.6959778, 0.6983111, 0.69575554, 0.6993778, 0.6959111, 0.6962, 0.69935554, 0.6978, 0.696, 0.69902223, 0.69673336, 0.6992889, 0.6993778, 0.6979111, 0.6999556, 0.6964222, 0.70004445, 0.6965333, 0.69884443, 0.6974889, 0.69713336, 0.7003111, 0.7003555, 0.7014, 0.69457775, 0.7014667, 0.69924444, 0.7006889, 0.6995111, 0.7011778, 0.7010222, 0.6969333, 0.70262223, 0.7001333, 0.7018667, 0.69795555, 0.6986, 0.7020444, 0.7001778, 0.7016444, 0.7002, 0.70111114, 0.69891113, 0.7023778, 0.70324445, 0.70346665, 0.70306665, 0.70228887, 0.7036222, 0.7012445, 0.6997111, 0.6986667, 0.70246667, 0.70431113, 0.70162225, 0.7001111, 0.7006889, 0.69895554, 0.7040667, 0.70306665, 0.7046889, 0.7016889, 0.70026666, 0.7020889, 0.70413333, 0.70615554, 0.7049556, 0.7029333, 0.7014889, 0.70184445, 0.70464444, 0.70408887, 0.7024, 0.70368886, 0.7046, 0.70493335, 0.7007333, 0.7032889, 0.70882225, 0.7028, 0.70486665, 0.70482224, 0.7062889, 0.70166665, 0.70786667, 0.704, 0.7037778, 0.7055111, 0.7028889, 0.70342225, 0.7040667, 0.70306665, 0.70435554, 0.7055778, 0.7054667, 0.7053111, 0.70566666, 0.7066444, 0.70442224, 0.70768887, 0.70593333, 0.70526665, 0.70604444, 0.7021111, 0.7046667, 0.7046, 0.70886666, 0.70624447, 0.7060889, 0.70622224, 0.7082667, 0.7096222, 0.7075111, 0.70575553, 0.7061778, 0.70728886, 0.7036667, 0.70233333, 0.7112, 0.7081556, 0.70831114, 0.70795554, 0.70633334, 0.7097333, 0.7103111, 0.70684445, 0.7074444, 0.7085111, 0.7087333, 0.7066444, 0.7101333, 0.7085111, 0.7079333, 0.7072222, 0.70857775, 0.7102444, 0.70644444, 0.7094667, 0.70773333, 0.70717776, 0.70966667, 0.71055555, 0.7103556, 0.70813334, 0.70915556, 0.7103556, 0.70926666, 0.7116445, 0.7065333, 0.7049111, 0.7116889, 0.7102444, 0.70795554, 0.7082222, 0.7115778, 0.70904446, 0.70948887, 0.7095111, 0.70964444, 0.7116, 0.70773333, 0.70982224, 0.7082, 0.7102, 0.70713335, 0.7127111, 0.7073333, 0.7090667, 0.7134445, 0.71062225, 0.7124, 0.7098, 0.7069111, 0.71, 0.70924443, 0.71117777, 0.7089555, 0.7138889, 0.7097333]
max = 61.94
Try with concat = False out.981209 (200 epochs)
max = 58.579
################# cnn_gru_0 Validation Accuracy =  [0.22579999268054962, 0.32420000433921814, 0.3287999927997589, 0.3783999979496002, 0.4081999957561493, 0.41200000047683716, 0.4246000051498413, 0.421999990940094, 0.4374000132083893, 0.42719998955726624, 0.4514000117778778, 0.45660001039505005, 0.45500001311302185, 0.4505999982357025, 0.46540001034736633, 0.4625999927520752, 0.4611999988555908, 0.45260000228881836, 0.47519999742507935, 0.48019999265670776, 0.4968000054359436, 0.47999998927116394, 0.4885999858379364, 0.4918000102043152, 0.4973999857902527, 0.5034000277519226, 0.49480000138282776, 0.48820000886917114, 0.48579999804496765, 0.5041999816894531, 0.49799999594688416, 0.503600001335144, 0.5109999775886536, 0.506600022315979, 0.5123999714851379, 0.5052000284194946, 0.5091999769210815, 0.5085999965667725, 0.5252000093460083, 0.5130000114440918, 0.5206000208854675, 0.5095999836921692, 0.5166000127792358, 0.531000018119812, 0.5184000134468079, 0.5356000065803528, 0.5180000066757202, 0.5303999781608582, 0.5281999707221985, 0.532800018787384, 0.5299999713897705, 0.5332000255584717, 0.5121999979019165, 0.5361999869346619, 0.5303999781608582, 0.5357999801635742, 0.5414000153541565, 0.5392000079154968, 0.5464000105857849, 0.5365999937057495, 0.5357999801635742, 0.5393999814987183, 0.5353999733924866, 0.5425999760627747, 0.5321999788284302, 0.5411999821662903, 0.5320000052452087, 0.5360000133514404, 0.5450000166893005, 0.5135999917984009, 0.5514000058174133, 0.5224000215530396, 0.5551999807357788, 0.5415999889373779, 0.5347999930381775, 0.5509999990463257, 0.5519999861717224, 0.5386000275611877, 0.5558000206947327, 0.5523999929428101, 0.5541999936103821, 0.5374000072479248, 0.5455999970436096, 0.5519999861717224, 0.5541999936103821, 0.5565999746322632, 0.5504000186920166, 0.5234000086784363, 0.5443999767303467, 0.5616000294685364, 0.5523999929428101, 0.5558000206947327, 0.5586000084877014, 0.550599992275238, 0.5529999732971191, 0.5490000247955322, 0.5577999949455261, 0.5504000186920166, 0.5533999800682068, 0.5600000023841858, 0.5616000294685364, 0.5396000146865845, 0.5532000064849854, 0.5522000193595886, 0.5636000037193298, 0.5577999949455261, 0.5523999929428101, 0.5335999727249146, 0.550599992275238, 0.5422000288963318, 0.550000011920929, 0.5631999969482422, 0.5645999908447266, 0.5379999876022339, 0.5573999881744385, 0.5626000165939331, 0.5655999779701233, 0.5641999840736389, 0.5562000274658203, 0.5641999840736389, 0.5491999983787537, 0.5447999835014343, 0.5636000037193298, 0.5546000003814697, 0.5684000253677368, 0.5685999989509583, 0.5651999711990356, 0.5616000294685364, 0.5663999915122986, 0.5681999921798706, 0.5558000206947327, 0.5616000294685364, 0.5709999799728394, 0.5604000091552734, 0.5676000118255615, 0.5577999949455261, 0.5605999827384949, 0.5734000205993652, 0.5662000179290771, 0.5681999921798706, 0.5637999773025513, 0.5623999834060669, 0.5622000098228455, 0.5681999921798706, 0.5645999908447266, 0.5529999732971191, 0.5541999936103821, 0.5681999921798706, 0.5669999718666077, 0.5490000247955322, 0.5496000051498413, 0.5577999949455261, 0.5609999895095825, 0.5717999935150146, 0.5690000057220459, 0.555400013923645, 0.5680000185966492, 0.5716000199317932, 0.5655999779701233, 0.5600000023841858, 0.5763999819755554, 0.5753999948501587, 0.5694000124931335, 0.5662000179290771, 0.5716000199317932, 0.5813999772071838, 0.5684000253677368, 0.5613999962806702, 0.555400013923645, 0.5649999976158142, 0.5723999738693237, 0.5631999969482422, 0.5659999847412109, 0.5813999772071838, 0.5712000131607056, 0.5626000165939331, 0.5509999990463257, 0.5640000104904175, 0.5649999976158142, 0.569599986076355, 0.5717999935150146, 0.5803999900817871, 0.5637999773025513, 0.5758000016212463, 0.5774000287055969, 0.5555999875068665, 0.5651999711990356, 0.5857999920845032, 0.5774000287055969, 0.5717999935150146, 0.5734000205993652, 0.5745999813079834, 0.5669999718666077, 0.5740000009536743, 0.5622000098228455, 0.5667999982833862, 0.5712000131607056, 0.5684000253677368, 0.5817999839782715, 0.5626000165939331]
################# cnn_gru_0 Training Accuracy =  [0.19939999282360077, 0.27006667852401733, 0.31695556640625, 0.3446222245693207, 0.36464443802833557, 0.3797111213207245, 0.38993334770202637, 0.4002888798713684, 0.4078444540500641, 0.41440001130104065, 0.420422226190567, 0.424311101436615, 0.431244432926178, 0.4356222152709961, 0.4380444586277008, 0.44404444098472595, 0.4456889033317566, 0.4509333372116089, 0.45471110939979553, 0.4583111107349396, 0.4600222110748291, 0.4658222198486328, 0.46933332085609436, 0.4724000096321106, 0.47813332080841064, 0.4840888977050781, 0.48500001430511475, 0.4867333471775055, 0.487888902425766, 0.49051111936569214, 0.4983111023902893, 0.49888888001441956, 0.5025110840797424, 0.5044000148773193, 0.5058888792991638, 0.5066888928413391, 0.5104222297668457, 0.5118222236633301, 0.5128222107887268, 0.513177752494812, 0.5137110948562622, 0.5186889171600342, 0.5210888981819153, 0.5189111232757568, 0.5212888717651367, 0.524911105632782, 0.5283555388450623, 0.5285999774932861, 0.5296444296836853, 0.5279333591461182, 0.5348222255706787, 0.5323333144187927, 0.5342444181442261, 0.5327110886573792, 0.5378888845443726, 0.5370444655418396, 0.5368000268936157, 0.5389999747276306, 0.5398444533348083, 0.540755569934845, 0.5434444546699524, 0.5434666872024536, 0.542555570602417, 0.5445555448532104, 0.5470444560050964, 0.5433777570724487, 0.5466889142990112, 0.5504666566848755, 0.5479555726051331, 0.5519555807113647, 0.5520666837692261, 0.5495111346244812, 0.5515555739402771, 0.5531777739524841, 0.5539555549621582, 0.5566444396972656, 0.5602444410324097, 0.5560222268104553, 0.5571555495262146, 0.5589110851287842, 0.560022234916687, 0.5600444674491882, 0.5619778037071228, 0.5645333528518677, 0.5624666810035706, 0.5614666938781738, 0.565155565738678, 0.5670222043991089, 0.5651333332061768, 0.5671333074569702, 0.5679555535316467, 0.5678222179412842, 0.5703999996185303, 0.5699333548545837, 0.5694666504859924, 0.5689555406570435, 0.5720000267028809, 0.5750444531440735, 0.5732444524765015, 0.5704444646835327, 0.5732444524765015, 0.5739333629608154, 0.5753999948501587, 0.5746444463729858, 0.5754222273826599, 0.5740666389465332, 0.5756666660308838, 0.5767999887466431, 0.5774666666984558, 0.579022228717804, 0.5767999887466431, 0.5757333040237427, 0.5807777643203735, 0.5778444409370422, 0.5782889127731323, 0.5836222171783447, 0.5840222239494324, 0.5828666687011719, 0.5834444165229797, 0.5846889019012451, 0.5827111005783081, 0.583466649055481, 0.5839333534240723, 0.5844444632530212, 0.5806666612625122, 0.5824221968650818, 0.5870444178581238, 0.5827999711036682, 0.5862666964530945, 0.5912222266197205, 0.587755560874939, 0.5888000130653381, 0.5889555811882019, 0.5885999798774719, 0.5866222381591797, 0.5886666774749756, 0.5890666842460632, 0.5849999785423279, 0.5930222272872925, 0.5926889181137085, 0.5915111303329468, 0.5928666591644287, 0.5909333229064941, 0.5920222401618958, 0.5926666855812073, 0.5923333168029785, 0.5913333296775818, 0.5930444598197937, 0.5943999886512756, 0.5952444672584534, 0.5947999954223633, 0.5927555561065674, 0.5936222076416016, 0.5965111255645752, 0.594955563545227, 0.5932888984680176, 0.5979777574539185, 0.5952666401863098, 0.5982666611671448, 0.59862220287323, 0.5991777777671814, 0.5964000225067139, 0.5924444198608398, 0.5962666869163513, 0.5973555445671082, 0.5979333519935608, 0.5993333458900452, 0.5977333188056946, 0.5998888611793518, 0.5979111194610596, 0.597955584526062, 0.5999777913093567, 0.6014222502708435, 0.6011555790901184, 0.6025111079216003, 0.6036221981048584, 0.6005333065986633, 0.6026666760444641, 0.6016444563865662, 0.6026444435119629, 0.6029333472251892, 0.6050000190734863, 0.6064888834953308, 0.6013555526733398, 0.6031777858734131, 0.6056666374206543, 0.603866696357727, 0.602911114692688, 0.6044222116470337, 0.6016222238540649, 0.6019555330276489, 0.6029333472251892, 0.6056444644927979, 0.6056888699531555, 0.603866696357727, 0.6045777797698975, 0.6063555479049683, 0.6097777485847473, 0.6065777540206909, 0.6092444658279419]


with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 200 epochs, hs = 256 out.848400
with kernel_regularizer keras.regularizers.l1_l2(l1=0.01, l2=0.01) out.437935 out.449019
################# cnn_gru_True Validation Accuracy =  [0.19740000367164612, 0.2881999909877777, 0.31299999356269836, 0.3434000015258789, 0.3653999865055084, 0.38100001215934753, 0.4153999984264374, 0.4300000071525574, 0.4244000017642975, 0.44679999351501465, 0.45260000228881836, 0.4553999900817871, 0.47699999809265137, 0.4758000075817108, 0.4812000095844269, 0.4984000027179718, 0.4909999966621399, 0.5026000142097473, 0.5034000277519226, 0.508400022983551, 0.5108000040054321, 0.527999997138977, 0.5139999985694885, 0.5325999855995178, 0.5311999917030334, 0.5338000059127808, 0.5404000282287598, 0.5429999828338623, 0.5249999761581421, 0.5307999849319458, 0.5429999828338623, 0.5450000166893005, 0.5526000261306763, 0.5428000092506409, 0.5406000018119812, 0.5527999997138977, 0.5523999929428101, 0.5460000038146973, 0.545799970626831, 0.5577999949455261, 0.5504000186920166, 0.555400013923645, 0.5455999970436096, 0.5514000058174133, 0.5522000193595886, 0.5551999807357788, 0.5618000030517578, 0.5509999990463257, 0.5564000010490417, 0.5658000111579895, 0.5594000220298767, 0.5577999949455261, 0.5672000050544739, 0.5551999807357788, 0.5672000050544739, 0.5591999888420105, 0.5655999779701233, 0.5622000098228455, 0.5699999928474426, 0.5669999718666077, 0.5640000104904175, 0.5672000050544739, 0.5627999901771545, 0.5631999969482422, 0.5623999834060669, 0.5648000240325928, 0.5598000288009644, 0.5604000091552734, 0.5663999915122986, 0.5709999799728394, 0.5598000288009644, 0.5654000043869019, 0.5676000118255615, 0.5636000037193298, 0.5727999806404114, 0.567799985408783, 0.5730000138282776, 0.5685999989509583, 0.5684000253677368, 0.5699999928474426, 0.5702000260353088, 0.5745999813079834, 0.5559999942779541, 0.5705999732017517, 0.5795999765396118, 0.5741999745368958, 0.5703999996185303, 0.5703999996185303, 0.5756000280380249, 0.5759999752044678, 0.5741999745368958, 0.5763999819755554, 0.5806000232696533, 0.573199987411499, 0.5722000002861023, 0.5756000280380249, 0.5741999745368958, 0.5788000226020813, 0.5727999806404114, 0.5735999941825867, 0.5785999894142151, 0.5745999813079834, 0.5788000226020813, 0.5676000118255615, 0.5730000138282776, 0.5684000253677368, 0.5691999793052673, 0.5776000022888184, 0.5776000022888184, 0.5748000144958496, 0.5758000016212463, 0.5716000199317932, 0.5763999819755554, 0.5684000253677368, 0.579200029373169, 0.5771999955177307, 0.578000009059906, 0.5752000212669373, 0.5831999778747559, 0.5795999765396118, 0.5777999758720398, 0.5726000070571899, 0.574999988079071, 0.5722000002861023, 0.5735999941825867, 0.5709999799728394, 0.5740000009536743, 0.5794000029563904, 0.5788000226020813, 0.5813999772071838, 0.5784000158309937, 0.5807999968528748, 0.5812000036239624, 0.5802000164985657, 0.5735999941825867, 0.5802000164985657, 0.5723999738693237, 0.5802000164985657, 0.5842000246047974, 0.5852000117301941, 0.5820000171661377, 0.5827999711036682, 0.5875999927520752, 0.578000009059906, 0.5759999752044678, 0.5843999981880188, 0.5831999778747559, 0.5789999961853027, 0.5827999711036682, 0.5691999793052673, 0.5812000036239624, 0.5799999833106995, 0.5758000016212463, 0.5849999785423279, 0.5825999975204468, 0.5781999826431274, 0.5831999778747559, 0.5838000178337097, 0.5758000016212463, 0.5726000070571899, 0.5834000110626221, 0.5842000246047974, 0.5884000062942505, 0.5863999724388123, 0.5799999833106995, 0.5848000049591064, 0.5825999975204468, 0.5794000029563904, 0.5830000042915344, 0.5789999961853027, 0.5860000252723694, 0.5806000232696533, 0.5784000158309937, 0.5881999731063843, 0.5789999961853027, 0.5881999731063843, 0.5821999907493591, 0.5785999894142151, 0.5860000252723694, 0.5839999914169312, 0.5776000022888184, 0.5812000036239624, 0.5763999819755554, 0.5834000110626221, 0.5720000267028809, 0.5824000239372253, 0.5835999846458435, 0.5825999975204468, 0.5774000287055969, 0.5843999981880188, 0.5860000252723694, 0.5917999744415283, 0.5821999907493591, 0.5852000117301941, 0.5934000015258789, 0.5896000266075134, 0.5884000062942505, 0.5748000144958496, 0.5838000178337097, 0.5861999988555908]
################# cnn_gru_True Training Accuracy =  [0.19660000503063202, 0.24744445085525513, 0.29660001397132874, 0.3197999894618988, 0.34042221307754517, 0.35946667194366455, 0.38271111249923706, 0.4038444459438324, 0.4194222092628479, 0.4294222295284271, 0.43666666746139526, 0.44555556774139404, 0.4554666578769684, 0.4596889019012451, 0.4680444300174713, 0.4723111093044281, 0.4806888997554779, 0.4835111200809479, 0.4874666631221771, 0.49408888816833496, 0.5022666454315186, 0.5037555694580078, 0.504111111164093, 0.5097777843475342, 0.5162222385406494, 0.5180888772010803, 0.5195333361625671, 0.5235777497291565, 0.5269333124160767, 0.5291110873222351, 0.5295777916908264, 0.5312444567680359, 0.535444438457489, 0.5351999998092651, 0.5364221930503845, 0.5388444662094116, 0.5406000018119812, 0.5428222417831421, 0.5442444682121277, 0.5446222424507141, 0.5503555536270142, 0.5470222234725952, 0.5522888898849487, 0.5533333420753479, 0.5532888770103455, 0.552911102771759, 0.5574222207069397, 0.558733344078064, 0.5594444274902344, 0.5628666877746582, 0.5593555569648743, 0.5623555779457092, 0.5642889142036438, 0.5643555521965027, 0.5698444247245789, 0.5675777792930603, 0.5713333487510681, 0.5699777603149414, 0.5699777603149414, 0.5728889107704163, 0.5720444321632385, 0.5725333094596863, 0.5763999819755554, 0.5739333629608154, 0.5762888789176941, 0.5751110911369324, 0.5798444151878357, 0.5796889066696167, 0.5815111398696899, 0.5797333121299744, 0.5790444612503052, 0.581933319568634, 0.584755539894104, 0.5832222104072571, 0.5863999724388123, 0.5874666571617126, 0.5854222178459167, 0.5855110883712769, 0.5855555534362793, 0.5879555344581604, 0.5888000130653381, 0.586222231388092, 0.5907999873161316, 0.5916666388511658, 0.5915777683258057, 0.5903555750846863, 0.5928221940994263, 0.5916222333908081, 0.5945777893066406, 0.5924888849258423, 0.5939333438873291, 0.5954889059066772, 0.5939777493476868, 0.5950666666030884, 0.5960000157356262, 0.5971333384513855, 0.5966444611549377, 0.6006444692611694, 0.5997111201286316, 0.5984444618225098, 0.5990222096443176, 0.6036888957023621, 0.6009555459022522, 0.5998666882514954, 0.6012444496154785, 0.6036444306373596, 0.599911093711853, 0.6018000245094299, 0.6055999994277954, 0.6050666570663452, 0.6059333086013794, 0.6061333417892456, 0.6032666563987732, 0.6064444184303284, 0.6061555743217468, 0.609000027179718, 0.6079555749893188, 0.6087777614593506, 0.6115777492523193, 0.6051111221313477, 0.6077333092689514, 0.6085110902786255, 0.6082888841629028, 0.6100000143051147, 0.6113777756690979, 0.61326664686203, 0.613111138343811, 0.6121777892112732, 0.6112666726112366, 0.6116889119148254, 0.615577757358551, 0.6137999892234802, 0.6133111119270325, 0.6153777837753296, 0.6159777641296387, 0.6172444224357605, 0.6125777959823608, 0.6107110977172852, 0.6137333512306213, 0.6190666556358337, 0.6146666407585144, 0.6165333390235901, 0.6161777973175049, 0.6160444617271423, 0.6154000163078308, 0.6169999837875366, 0.6182666420936584, 0.6179555654525757, 0.6194888949394226, 0.6154000163078308, 0.6197999715805054, 0.6198222041130066, 0.6195111274719238, 0.6213555335998535, 0.622355580329895, 0.6189555525779724, 0.6221110820770264, 0.6180889010429382, 0.6214888691902161, 0.6235555410385132, 0.621666669845581, 0.6259111166000366, 0.6236888766288757, 0.6235111355781555, 0.6221333146095276, 0.624822199344635, 0.6250444650650024, 0.625688910484314, 0.6254444718360901, 0.626466691493988, 0.6252889037132263, 0.6247555613517761, 0.6279555559158325, 0.625688910484314, 0.6261110901832581, 0.6280666589736938, 0.6285777688026428, 0.6279777884483337, 0.625511109828949, 0.6262444257736206, 0.628333330154419, 0.627133309841156, 0.6295999884605408, 0.6308888792991638, 0.6295777559280396, 0.6275110840797424, 0.6317999958992004, 0.6287333369255066, 0.6288444399833679, 0.6308000087738037, 0.629111111164093, 0.629622220993042, 0.6306222081184387, 0.6284000277519226, 0.6311777830123901, 0.6300222277641296, 0.6324666738510132, 0.6323778033256531, 0.6296889185905457, 0.6352221965789795]
Try with concat = False out.660437 (50 epochs)
################# cnn_gru_0 Validation Accuracy =  [0.31619998812675476, 0.3325999975204468, 0.3806000053882599, 0.4059999883174896, 0.4196000099182129, 0.423799991607666, 0.4357999861240387, 0.43779999017715454, 0.4535999894142151, 0.4641999900341034, 0.47380000352859497, 0.475600004196167, 0.48840001225471497, 0.4848000109195709, 0.48339998722076416, 0.49900001287460327, 0.49219998717308044, 0.5055999755859375, 0.5012000203132629, 0.5166000127792358, 0.5116000175476074, 0.506600022315979, 0.520799994468689, 0.5185999870300293, 0.5144000053405762, 0.5206000208854675, 0.5266000032424927, 0.522599995136261, 0.5375999808311462, 0.52920001745224, 0.5130000114440918, 0.5285999774932861, 0.5285999774932861, 0.5437999963760376, 0.5407999753952026, 0.5450000166893005, 0.5419999957084656, 0.5406000018119812, 0.5392000079154968, 0.5544000267982483, 0.5479999780654907, 0.5460000038146973, 0.5473999977111816, 0.5559999942779541, 0.5429999828338623, 0.5388000011444092, 0.5514000058174133, 0.5411999821662903, 0.5468000173568726, 0.5547999739646912]
################# cnn_gru_0 Training Accuracy =  [0.21320000290870667, 0.31262221932411194, 0.35028889775276184, 0.37102222442626953, 0.3886444568634033, 0.3989555537700653, 0.41306665539741516, 0.42100000381469727, 0.4274222254753113, 0.43479999899864197, 0.441777765750885, 0.4474000036716461, 0.455822229385376, 0.4593110978603363, 0.46577778458595276, 0.47244444489479065, 0.4786444306373596, 0.48251110315322876, 0.4856888949871063, 0.48768889904022217, 0.4945777654647827, 0.4945555627346039, 0.4999333322048187, 0.5040888786315918, 0.5044222474098206, 0.5098000168800354, 0.5132666826248169, 0.5106444358825684, 0.5141333341598511, 0.5174000263214111, 0.5239111185073853, 0.5222444534301758, 0.5272889137268066, 0.5264000296592712, 0.5284222364425659, 0.5353111028671265, 0.5317111015319824, 0.5315999984741211, 0.5336889028549194, 0.5348666906356812, 0.5392888784408569, 0.5394666790962219, 0.5410444736480713, 0.5435555577278137, 0.5426444411277771, 0.5475555658340454, 0.5474666953086853, 0.5461333394050598, 0.5516666769981384, 0.5508444309234619]

Add dense layer
with cnn_dropout = 0.4 and rnn dropout = 0.2 and lr = 5e-4 with res = 8 with 10 samples and 200 epochs, hs = 256 out.848400
out.9611

concat = False - out.9612



'''

from __future__ import division, print_function, absolute_import

print('Starting..................................')
import os
import sys
sys.path.insert(1, '/home/labs/ahissarlab/orra/imagewalker/')
import numpy as np
import cv2
import misc
import pandas as pd
import matplotlib.pyplot as plt
import pickle

from keras_utils import dataset_update, write_to_file, create_cifar_dataset
from misc import *

import tensorflow.keras as keras
import tensorflow as tf

from tensorflow.keras.datasets import cifar10

# load dataset
(trainX, trainy), (testX, testy) = cifar10.load_data()
images, labels = trainX, trainy

kernel_regularizer_list = [None, keras.regularizers.l1(),keras.regularizers.l2(),keras.regularizers.l1_l2()]
optimizer_list = [tf.keras.optimizers.Adam, tf.keras.optimizers.Nadam, tf.keras.optimizers.RMSprop]
if len(sys.argv) > 1:
    paramaters = {
    'epochs' : int(sys.argv[1]),
    
    'sample' : int(sys.argv[2]),
    
    'res' : int(sys.argv[3]),
    
    'hidden_size' : int(sys.argv[4]),
    
    'concat' : int(sys.argv[5]),
    
    'regularizer' : keras.regularizers.l1(),#kernel_regularizer_list[int(sys.argv[6])],
    
    'optimizer' : optimizer_list[int(sys.argv[7])],
    
    'cnn_dropout' : 0.4,

    'rnn_dropout' : 0.2,

    'lr' : 5e-4,
    
    'run_id' : np.random.randint(1000,9000)
    }
    
else:
    paramaters = {
    'epochs' : 1,
    
    'sample' : 5,
    
    'res' : 8,
    
    'hidden_size' : 128,
    
    'concat' : 1,
    
    'regularizer' : None,
    
    'optimizer' : optimizer_list[0],
    
    'cnn_dropout' : 0.4,

    'rnn_dropout' : 0.2,

    'lr' : 5e-4,
    
    'run_id' : np.random.randint(1000,9000)
    }
   
print(paramaters)
for key,val in paramaters.items():
    exec(key + '=val')
epochs = epochs
sample = sample 
res = res 
hidden_size =hidden_size
concat = concat
regularizer = regularizer
optimizer = optimizer
cnn_dropout = cnn_dropout
rnn_dropout = rnn_dropout
lr = lr
run_id = run_id
n_timesteps = sample
def split_dataset_xy(dataset):
    dataset_x1 = [uu[0] for uu in dataset]
    dataset_x2 = [uu[1] for uu in dataset]
    dataset_y = [uu[-1] for uu in dataset]
    return (np.array(dataset_x1),np.array(dataset_x2)[:,:n_timesteps,:]),np.array(dataset_y)

def cnn_gru(n_timesteps = 5, hidden_size = 128,input_size = 32, concat = True, 
            optimizer = tf.keras.optimizers.Adam, ):
    '''
    
    CNN RNN combination that extends the CNN to a network that achieves 
    ~80% accuracy on full res cifar.

    Parameters
    ----------
    n_timesteps : TYPE, optional
        DESCRIPTION. The default is 5.
    img_dim : TYPE, optional
        DESCRIPTION. The default is 32.
    hidden_size : TYPE, optional
        DESCRIPTION. The default is 128.
    input_size : TYPE, optional
        DESCRIPTION. The default is 32.

    Returns
    -------
    model : TYPE
        DESCRIPTION.

    '''
    inputA = keras.layers.Input(shape=(n_timesteps,input_size,input_size,3))
    inputB = keras.layers.Input(shape=(n_timesteps,2))

    # define CNN model

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same'))(inputA)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(32,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(64,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)

    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Conv2D(128,(3,3),activation='relu', padding = 'same'))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.MaxPooling2D(pool_size=(2, 2)))(x1)
    x1=keras.layers.TimeDistributed(keras.layers.Dropout(cnn_dropout))(x1)
    print(x1.shape)


    x1=keras.layers.TimeDistributed(keras.layers.Flatten())(x1)
    print(x1.shape)
    if concat:
        x = keras.layers.Concatenate()([x1,inputB])
    else:
        x = x1
    print(x.shape)

    # define LSTM model
    x = keras.layers.GRU(hidden_size,input_shape=(n_timesteps, None),
                         return_sequences=True,recurrent_dropout=rnn_dropout,
                         kernel_regularizer=regularizer)(x)
    
    x = keras.layers.Flatten()(x)
    #Add another dense layer (prior it reached 62%)
    x = keras.layers.Dense(512, activation="relu")(x)
    x = keras.layers.Dense(10,activation="softmax")(x)
    model = keras.models.Model(inputs=[inputA,inputB],outputs=x, name = 'cnn_gru_{}'.format(concat))
    opt=optimizer(lr=lr)

    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=["sparse_categorical_accuracy"],
    )
    return model

rnn_net = cnn_gru(n_timesteps = sample, hidden_size = hidden_size,input_size = res, concat = concat)
cnn_net = cnn_net = extended_cnn_one_img(n_timesteps = sample, input_size = res, dropout = cnn_dropout)


# hp = HP()
# hp.save_path = 'saved_runs'

# hp.description = "syclop cifar net search runs"
# hp.this_run_name = 'syclop_{}'.format(rnn_net.name)
# deploy_logs()

train_dataset, test_dataset = create_cifar_dataset(images, labels,res = res,
                                    sample = sample, return_datasets=True, 
                                    mixed_state = False, add_seed = 0,
                                    )
                                    #bad_res_func = bad_res101, up_sample = True)

train_dataset_x, train_dataset_y = split_dataset_xy(train_dataset)
test_dataset_x, test_dataset_y = split_dataset_xy(test_dataset)


print("##################### Fit {} and trajectories model on training data res = {} ##################".format(rnn_net.name,res))
rnn_history = rnn_net.fit(
    train_dataset_x,
    train_dataset_y,
    batch_size=64,
    epochs=epochs,
    # We pass some validation for
    # monitoring validation loss and metrics
    # at the end of each epoch
    validation_data=(test_dataset_x, test_dataset_y),
    verbose = 0)

# print('################# {} Validation Accuracy = '.format(cnn_net.name),cnn_history.history['val_sparse_categorical_accuracy'])
# print('################# {} Training Accuracy = '.format(cnn_net.name),rnn_history.history['sparse_categorical_accuracy'])

print('################# {} Validation Accuracy = '.format(rnn_net.name),rnn_history.history['val_sparse_categorical_accuracy'])
print('################# {} Training Accuracy = '.format(rnn_net.name),rnn_history.history['sparse_categorical_accuracy'])


plt.figure()
plt.plot(rnn_history.history['sparse_categorical_accuracy'], label = 'train')
plt.plot(rnn_history.history['val_sparse_categorical_accuracy'], label = 'val')
# plt.plot(cnn_history.history['sparse_categorical_accuracy'], label = 'cnn train')
# plt.plot(cnn_history.history['val_sparse_categorical_accuracy'], label = 'cnn val')
plt.legend()
plt.grid()
plt.ylim(0.5,0.63)
plt.title('{} on cifar res = {} hs = {} dropout = {}, num samples = {}'.format(rnn_net.name, res, hidden_size,cnn_dropout,sample))
plt.savefig('{} on Cifar res = {}, no upsample, val accur = {} hs = {} dropout = {}.png'.format(rnn_net.name,res,rnn_history.history['val_sparse_categorical_accuracy'][-1], hidden_size,cnn_dropout))

with open('/home/labs/ahissarlab/orra/imagewalker/cifar_net_search/{}'.format(run_id), 'wb') as file_pi:
    pickle.dump(rnn_history.history, file_pi)
    
# with open('/home/labs/ahissarlab/orra/imagewalker/cifar_net_search/{}HistoryDict'.format(cnn_net.name), 'wb') as file_pi:
#     pickle.dump(cnn_history.history, file_pi)


dataset_update(rnn_history, rnn_net,paramaters)    
write_to_file(rnn_history, rnn_net,paramaters)    
    
    